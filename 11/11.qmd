---
title: "11 – Lineare Regression (2)"
subtitle: "Statistische Datenanalyse mit R"
author: "Clemens Brunner"
date: 2023-01-19
format: html
engine: knitr
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
lang: de
author-title: "Autor"
published-title: "Veröffentlicht"
---

## Multiple lineare Regression

Die multiple lineare Regression ist eine Erweiterung der einfachen linearen Regression auf Situationen mit mehreren Prädiktoren. Das grundlegende Konzept bleibt aber unverändert, wir verwenden nach wie vor folgende allgemeine Modellgleichung (siehe vorige Einheit):

$$y_i = \hat{y}_i + \varepsilon_i$$

Das Modell mit $n$ Prädiktoren $x_{1}$, $x_{2}$, $\ldots$, $x_{n}$ wird nun wie folgt formuliert:

$$y_i = (b_0 + b_1 x_{1i} + b_2 x_{2i} + \ldots + b_n x_{ni}) + \varepsilon_i$$

Jeder Prädiktor erhält also ein eigenes Gewicht bzw. einen eigenen Regressionskoeffizienten. Die Koeffizienten werden mittels Least Squares wieder so berechnet, dass die entstehende Regressions(hyper)ebene den mittleren quadratischen Fehler minimiert.


### Quadratsummen und Model Fit

Die Quadratsummen SST, SSM und SSR werden analog wie bei der einfachen Regression berechnet. Wieder kann man $R^2$ berechnen, welches den Anteil der Varianz in der abhängigen Variable angibt, welche durch das Modell erklärt wird. Je höher dieser Wert ist, desto besser kann das Modell die Daten beschreiben. Im Gegensatz zur einfachen Regression kann man aus $R^2$ nicht die Pearson-Korrelation zwischen den einzelnen Variablen berechnen, sondern die Korrelation zwischen den vorhergesagten Werten und den tatsächlich beobachteten Werten.

Je mehr Prädiktoren man hinzufügt, desto größer wird $R^2$, d.h. man kann durch Erhöhen der Anzahl der Prädiktoren diesen Wert fast beliebig erhöhen. Dies liefert dann natürlich keine verlässliche Aussage mehr über die Modellqualität. Deswegen gibt es Kriterien, welche die Anzahl der Prädiktoren berücksichtigen, z.B. das [Akaike Information Criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) oder das [Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). Bei gegebenen Daten kann man so verschiedene Modelle vergleichen; jenes Modell mit dem kleinsten Wert von AIC (oder BIC) ist zu bevorzugen. Weiters kann anstelle von $R^2$ auch das [adjustierte Bestimmtheitsmaß](https://de.wikipedia.org/wiki/Bestimmtheitsma%C3%9F#Das_adjustierte_Bestimmtheitsma%C3%9F) verwendet werden, welches eine bessere Schätzung der vom Modell erklärten Varianz bei mehreren Prädiktoren liefert.


## Überprüfen der Datenpunkte

Ein gegebenes Modell sollte man immer darauf überprüfen, wie gut es die gemessenen Daten tatsächlich beschreibt. Lineare Modelle können sehr sensitiv auf einzelne Datenpunkte reagieren, welche nicht zum generellen Trend der Daten passen. Einen tatsächlichen Einfluss auf das Modell üben solche Ausreißer aber nur aus, wenn sie weit weg vom Mittelwert der Prädiktoren liegen – man bezeichnet diesen potentiellen Einfluss jedes Datenpunkts als *Leverage*.

Kritisch sind also jene Punkte, welche einen potentiellen hohen Einfluss haben (hohe Leverage) und gleichzeitig nicht zum generellen Trend der Daten passen, also Ausreißer sind. Die folgende Grafik veranschaulicht die drei möglichen Situationen. Dabei sind die Ausgangsdatenpunkte schwarz dargestellt und die zugehörige Regressionsgerade ist schwarz strichliert. Der zusätzliche fünfte Datenpunkt ist rot dargestellt, und die Regressionsgerade durch alle fünf Datenpunkte ist ebenfalls rot. Durch einen einzigen zusätzlichen Datenpunkt ändert sich also das ursprüngliche Modell mehr oder weniger stark (von schwarz strichliert auf rot durchgezogen).

```{r}
#| echo: false
x = c(5:8)
y = x + rnorm(length(x), 0, 0.1)
l = lm(y ~ x)

# low leverage outlier (= uninfluential)
x1 = 6.5
y1 = 3
l1 = lm(c(y, y1) ~ c(x, x1))

# high leverage non-outlier (= uninfluential)
x2 = 12
y2 = 11.8
l2 = lm(c(y, y2) ~ c(x, x2))

# high leverage outlier (= influential)
x3 = 14
y3 = 8
l3 = lm(c(y, y3) ~ c(x, x3))

par(mfrow=c(1, 3), mar=c(1, 1, 1, 1))
plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x1, y1, pch=19, col="red")
abline(l1, col="red")
title("low leverage outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x2, y2, pch=19, col="red")
abline(l2, col="red")
title("high leverage non-outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x3, y3, pch=19, col="red")
abline(l3, col="red")
title("high leverage outlier")
```

Je weniger Datenpunkte vorhanden sind, desto größer ist der Einfluss von Ausreißern auf das lineare Modell. Die folgende Grafik veranschaulicht die Auswirkung eines Ausreißers für drei verschiedene Stichprobengrößen. Man erkennt, dass ein Ausreißer das Modell nur wenig verändert wenn sehr viele Datenpunkte vorhanden sind. Wenn es aber nur wenig Datenpunkte gibt, kann ein einziger Ausreißer das Modell stark verändern.

```{r}
#| echo: false
outlier = function(n=100, alpha=0.5)
{
    x = runif(n, 5, 15)
    y = x + rnorm(length(x), 0, 0.2)
    l = lm(y ~ x)
    plot(x, y, pch=19, col=rgb(0, 0, 0, alpha), xlim=c(5, 20), ylim=c(5, 20), axes=FALSE, xlab="", ylab="")
    abline(l, lty=2)

    x_o = 20
    y_o = 5
    points(x_o, y_o, pch=19, col="red")
    l_o = lm(c(y, y_o) ~ c(x, x_o))
    abline(l_o, col="red")

    mtext(bquote(italic(n) == .(n)), side=1, family="serif")
}

par(mfrow=c(1, 3), mar=c(1, 3, 1, 3))
outlier(1000, alpha=0.15)
outlier(100)
outlier(10, alpha=0.8)
```


### Ausreißer

Ein Ausreißer ist ein Datenpunkt, welcher sich von den restlichen Daten unterscheidet. Im Falle eines linearen Modells hat ein Ausreißer bei gegebenen $x$-Werten einen ungewöhnlichen $y$-Wert. Ausreißer erkennt man z.B. an deren Residuen – diese sind typischerweise (aber nicht immer) sehr groß. Es ist allerdings sinnvoll, wenn man sich standardisierte Residuen zu jedem Datenpunkt ansieht (d.h. Residuen geteilt durch ihre Standardabweichung[^1]). Die Funktion `rstandard()` berechnet standardisierte Residuen.

[^1]: Manchmal werden die standardisierten Residuen auch als studentisierte Residuen bezeichnet, da die Standardabweichung der Population im Allgemeinen nicht bekannt ist und durch die Standardabweichung der Stichprobe geschätzt wird.

Zur Beurteilung des Modells anhand standardisierter Residuen gibt es folgende Daumenregeln:

* Standardisierte Residuen größer als 3 sind wahrscheinlich Ausreißer und sollten gesondert behandelt werden.
* Wenn mehr als 1% der Daten standardisierte Residuen größer als 2,5 haben ist das ein Hinweis darauf, dass das Modell die Daten schlecht beschreibt.
* Wenn mehr als 5% der Daten standardisierte Residuen größer als 2 haben ist das ebenfalls ein Hinweis darauf, dass das Modell die Daten schlecht beschreibt.

Alternativ zu den standardisierten Residuen kann man auch studentisierte Residuen mittels Leave-One-Out berechnen. Dies bedeutet, dass bei der Berechnung der Standardabweichung das Modell ohne den betreffenden Datenpunkt verwendet wird (dieser Datenpunkt wird ausgelassen). Diese studentisierten Residuen werden so für jeden Datenpunkt berechnet. In R verwendet man die Funktion `rstudent()` dafür.


### Datenpunkte mit hoher Leverage

Datenpunkte, die weit vom Mittelwert der Prädiktorvariablen entfernt sind, können ein lineares Modell potentiell stark beeinflussen. Ein Maß für den potentiellen Einfluss ist die sogenannte Leverage $h_i$ (wird auch als Hat-Wert bezeichnet). Die Leverage beschreibt den Einfluss eines Messpunktes $y_i$ auf alle vom Modell vorhergesagten Werte. Für $n$ Messpunkte gilt:

$$\frac{1}{n} \leq h_i \leq 1$$

Der durchschnittliche Wert für ein Modell mit $k$ Prädiktoren ist:
$$\frac{k + 1}{n}$$

In R kann man die Funktion `hatvalues()` benutzen, um die Leverage für jeden Datenpunkt zu berechnen.


### Datenpunkte mit großem Einfluss

Tatsächlichen Einfluss auf ein Modell haben Datenpunkte die gleichzeitig Ausreißer sind und hohe Leverage haben.

Man kann solche Datenpunkte erkennen, indem man einmal ein Modell mit allen Daten und ein zweites Modell mit allen Daten außer den zu untersuchenden Datenpunkt rechnet (Leave-One-Out). Wenn der untersuchte Datenpunkt keinen großen Einfluss auf das Modell ausübt, dann sind die beiden Modelle sehr ähnlich. Folgende konkrete Maße kann man hier für jeden Datenpunkt berechnen:

* DFBeta (`dfbeta()`) beschreibt den Unterschied zwischen den Regressionskoeffizienten, welche aus einem Modell mit allen Daten und einem Modell ohne den jeweiligen Messpunkt berechnet wurden. Der Nachteil ist, dass man hier nicht einen einzigen Wert bekommt, den man leicht beurteilen könnte, sondern einen Wert für jeden Regressionskoeffizienten.
* Cook's Distanz (`cooks.distance()`) berechnet aus den Unterschieden der einzelnen Koeffizienten einen Wert.
* Angepasste vorhergesagte Werte (`dffits()`) beschreiben den Unterschied zwischen den vorhergesagten Werten, welche durch die zwei verschiedenen Modelle berechnet wurden (dieses Maß ist auch als DFFit bekannt).


## Modellannahmen

Um mit einem linearen Regressionsmodell Vorhersagen auf ungesehene Daten machen zu können, müssen folgende Annahmen erfüllt sein:

* Die abhängige Variable muss intervallskaliert sein.
* Die unabhängigen Variablen (Prädiktoren) müssen intervallskaliert (oder nominalskaliert in zwei Kategorien) sein.
* Die Prädiktoren müssen Varianzen ungleich 0 haben.
* Es darf keine Multikollinearität bestehen, d.h. zwei oder mehrere Prädiktoren dürfen nicht linear voneinander abhängig sein. Dies kann z.B. mit der VIF-Statistik (Variance Inflation Factor) überprüft werden.
* Homoskedastizität, d.h. die Varianz der Residuen muss konstant über die Werte der Prädiktoren sein (Varianzhomogenität).
* Die Residuen müssen normalverteilt sein.
* Die Residuen müssen unabhängig voneinander sein (kann z.B. mit dem Durbin-Watson-Test überprüft werden).
* Die Beziehung zwischen unabhängigen Variablen und abhängiger Variable muss linear sein.


## Beispiel

Im folgenden Beispiel sehen wir uns wie in der letzten Einheit die Anzahl der Verkäufe von Musikalben in Abhängigkeit der Höhe des Werbebudgets an. Zusätzlich gibt es jetzt aber zwei weitere Prädiktoren, nämlich die Anzahl an Airplay-Stunden im größten nationalen Radiosender und die Attraktivität der Bandmitglieder. Wir beginnen mit dem Laden der Daten [`sales2.dat`](sales2.dat):

```{r}
#| message: false
library(readr)
album2 = read_tsv("sales2.dat")
```

Fitten wir nun ein lineares Regressionsmodell. Als Vergleichsmodell berechnen wir zuerst wieder eine einfache Regression mit dem Prädiktor Werbebudget:

```{r}
model1 = lm(sales ~ adverts, data=album2)
```

Zusätzliche Faktoren kann man nun in einem zweiten Modell einfach durch den `+`-Operator hinzufügen:

```{r}
model2 = lm(sales ~ adverts + airplay + attract, data=album2)
```

Anschließend können wir uns die zusammengefassten Ergebnisse der beiden Modelle anzeigen lassen:

```{r}
summary(model1)
summary(model2)
```

Da das erste Modell identisch mit jenem aus der vorigen Einheit ist, kennen wir die Ergebnisse bereits. Wenden wir uns daher dem zweiten Modell zu. $R^2$ ist hier 0.6647, das heißt das Modell kann nun 66% der Varianz erklären. Im Vergleich zum ersten Modell mit nur einem Prädiktor ist das eine Steigerung um 33%, d.h. die beiden Prädiktoren Airplay und Attraktivität können zusätzliche Varianz im Ausmaß von 33% erklären.

Die Regressionskoeffizienten werden ebenfalls in der Ausgabe dargestellt. Wir können daher das lineare Modell wie folgt schreiben:

$$y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 = -26.613 + 0.085 \cdot x_1 + 3.367 \cdot x_2 + 11.086 \cdot x_3$$

Hier stehen $b_0$ für den Intercept (d.h. jener Wert von $y$, wenn alle Prädiktoren 0 sind), $b_1$ für das Werbebudget `adverts`, $b_2$ für die Airplay-Stunden `airplay` und $b_3$ für die Attraktivität `attract`.

Die Regressionskoeffizienten geben Auskunft, um wie viel sich die abhängige Variable ändert, wenn man einen Prädiktor um eine Einheit erhöht und dabei alle anderen Prädiktoren konstant hält. Dies bedeutet im Beispiel:

* Wenn `adverts` um eine Einheit erhöht wird, dann erhöht sich `sales` um 0,085 Einheiten. D.h. wenn man 1000 EUR mehr für Werbung ausgibt, verkauft man um 85 Alben mehr.
* Wenn `airplay` um eine Einheit erhöht wird, dann erhöht sich `sales` um 3,37 Einheiten. D.h. wenn man das Album um eine Stunde mehr im Radio spielt, verkauft man um 3367 Alben mehr.
* Wenn `attract` um eine Einheit erhöht wird, dann erhöht sich `sales` um 11,086 Einheiten (d.h. 11086 zusätzliche Alben).

Für jeden Regressionskoeffizienten wird ein $t$-Test gerechnet, welcher angibt, ob sich der Koeffizient signifikant von 0 unterscheidet (d.h. ob er signifikant zum Modell beiträgt). Die Größe der $t$-Statistik lässt auf den Einfluss der Koeffizienten schließen, d.h. `adverts` und `airplay` haben einen ähnlich großen Einfluss auf das Modell, wo hingegen `attract` einen geringeren Einfluss hat.

Oft ist es hilfreich, nicht nur die Regressionskoeffizienten zu analysieren, sondern auch die standardisierten Regressionskoeffizienten. Diese kann man berechnen, in dem man zuerst alle Variablen standardisiert und danach das lineare Modell berechnet. Standardisierte Variablen haben einen Mittelwert von 0 und eine Standardabweichung von 1. Man könnte die Standardisierung daher relativ einfach selbst vornehmen, in dem man für jede Variable zuerst deren Mittelwert subtrahiert und danach durch deren Standardabweichung dividiert.

:::{.callout-tip}
Die Funktion `scale()` kann verwendet werden, um die Spalten eine Data Frames zu standardisieren. Diese liefert aber immer eine Matrix zurück, d.h. wenn man ein Data Frame bzw. Tibble standardisieren möchte, muss man danach noch `as.data.frame()` bzw. `as_tibble()` (aus dem Paket `tibble`) anwenden.
:::

Alternativ kann man dazu auch *nachträglich* die Funktion `lm.beta()` aus dem `lm.beta`-Paket benutzen:

```{r}
library(lm.beta)
lm.beta(model2)
```

Die standardisierten Regressionskoeffizienten werden üblicherweise mit $\beta_i$ bezeichnet. Da alle Variablen nun in Standardabweichungen gemessen werden, kann man diese direkt miteinander vergleichen. Man sieht im Beispiel also folgendes:

* Wenn `adverts` um eine Standardabweichung erhöht wird (485.655 EUR), dann erhöht sich `sales` um 0,511 Standardabweichungen (41.240 Alben).
* Wenn `airplay` um eine Standardabweichung erhöht wird (12,270), dann erhöht sich `sales` um 0,512 Standardabweichungen (41.320 Alben).
* Wenn `attract` um eine Standardabweichung erhöht wird (1,395), dann erhöht sich `sales` um 0,192 Standardabweichungen (15.490 Alben).

Konfidenzintervalle für die (nicht standardisierten) Regressionskoeffizienten erhält man mit der Funktion `confint()` (standardmäßig werden 95%-Intervalle berechnet):

```{r}
confint(model2)
```

Zwei (oder mehrere) Modelle können mit der $F$-Statistik verglichen werden. Der $F$-Wert, der bei der zusammenfassenden Beschreibung eines Modells angezeigt wird, vergleicht das Modell immer mit dem einfachsten Mittelwertmodell. Möchte man das Modell mit einem anderen Modell vergleichen, ist zu beachten, dass `model2` eine Erweiterung von `model1` sein muss, d.h. `model2` muss alle Terme von `model1` beinhalten plus eventuelle zusätzliche Faktoren. In R gibt man hier folgenden Befehl ein:

```{r}
anova(model1, model2)
```

Der $F$-Wert beträgt also 96,447 und ist signifikant, d.h. das zweite Modell ist signifikant besser als das erste.


### Datenpunkte mit großem Einfluss

Um die diversen Ausreißerstatistiken für jeden einzelnen Wert übersichtlich beurteilen zu können, kann man die Werte mit folgenden Funktionen berechnen:

* `resid()`: Residuen
* `rstandard()`: Standardisierte Residuen
* `rstudent()`: Studentisierte Residuen (berechnet mit Leave-One-Out)
* `hatvalues()`: Leverage
* `dfbeta()`: Unterschied der Regressionskoeffizienten mittels Leave-One-Out
* `cooks.distance()`: Cook's Distanz
* `dffits()`: Unterschied im vorhergesagtem Wert mittels Leave-One-Out

Am bequemsten ist die Funktion `influence.measures()`, welche mehrere Ausreißerstatistiken übersichtlich aufbereitet ausgibt.

```{r}
influence.measures(model2)
```

Um festzustellen, welche Punkte standardisierte Residuen (betragsmäßig) größer als 3 haben (also entweder größer als 3 oder kleiner als −3 sind), kann man folgenden Befehl verwenden:

```{r}
which(abs(rstandard(model2)) > 3)
```

Nur ein Datenpunkt (nämlich der 169.) ist hier also auffällig.


### Modellannahmen

Mulitkollinearität kann mit der VIF-Statistik beurteilt werden; in R kann man dazu die Funktion `vif()` aus dem `car`-Paket verwenden.

```{r}
#| message: false
library(car)
vif(model2)
```

Der größte VIF-Wert sollte nicht größer als 10 sein. Der durchschnittliche VIF sollte nicht wesentlich größer als 1 sein, was man wie folgt überprüfen kann:

```{r}
mean(vif(model2))
```

Wenn man den `plot()`-Befehl auf das Modell anwendet, werden vier diagnostische Plots erstellt.

```{r}
#| fig-width: 12
#| fig-height: 8
par(mfrow=c(2, 2), cex=0.75)
plot(model2)
```

Im Plot links oben sind die vorhergesagten Werte gegen die Residuen aufgetragen. Hier kann man die Linearitätsannahme (die rote Linie sollte immer ungefähr gleich Null sein) sowie die Homoskedastizitätsannahme (die Streuung der Datenpunkte sollte sich entlang der x-Achse nicht ändern) überprüfen. Der Plot links unten ist ähnlich, nur ist hier statt den (absoluten) Residuen die Wurzel aus dem Betrag der standardisierten Residuen aufgetragen. Auch hier lässt sich beurteilen, ob die Annahme der Varianzhomogenität erfüllt ist oder nicht. Im Plot rechts oben lässt sich die Normalverteilungsannahme der Residuen mit einem QQ-Plot überprüfen. Im Plot rechts unten sind Punkte mit großem Einfluss dargestellt (gemessen an der Leverage); Cook's Distanz ist ebenfalls im Plot ersichtlich.

Die Annahme über die Unabhängigkeit der Residuen kann mit dem Durbin-Watson-Test `dwt()` überprüft werden.

```{r}
dwt(model2)
```

In diesem Beispiel kann man davon ausgehen, dass die Residuen unabhängig sind, da aufgrund von $p\approx 0.7$ die Nullhypothese nicht verworfen werden kann.


## Übungen

### Übung 1

Laden Sie die Daten aus der Datei `sales2.dat` wie in den Unterlagen gezeigt. Standardisieren Sie danach alle Variablen und berechnen Sie dann ein lineares Regressionsmodell. Vergleichen Sie die Regressionskoeffizienten mit den Ergebnissen der Funktion `lm.beta()`, welche auf ein Modell mit nicht standardisierten Daten angewendet werden kann.


### Übung 2

Laden Sie den Datensatz [`aggression.dat`](aggression.dat), welcher (komplett erfundene) Daten über Aggressionen unter Kindern enthält. Es wurden 666 Kinder untersucht und folgende Variablen erhoben:

- Erziehungsstil (hoher Wert entspricht schlechtem Stil)
- Computerspielen (hoher Wert entspricht viel Computerspielen)
- Fernsehen (hoher Wert entspricht viel Fernsehen)
- Ernährung (hoher Wert entspricht gesunder Ernährung)
- Aggressionen der Geschwister (hoher Wert entspricht hoher Aggression)

Von früheren Studien weiß man, dass Erziehungsstil sowie Aggressionen der Geschwister signifikante Prädiktoren für das Aggressionslevel eines Kindes sind.

Stellen Sie zwei lineare Regressionsmodelle auf. Das erste soll nur die beiden Faktoren beinhalten, welche erwiesenermaßen einen Einfluss auf die Aggression haben. Das zweite Modell soll alle Faktoren beinhalten. Beantworten Sie anschließend folgende Punkte:

1. Bestimmen Sie für beide Modelle das Bestimmtheitsmaß $R^2$ und geben Sie die Tabelle der Regressionskoeffizienten aus.
2. Interpretieren Sie für beide Modelle getrennt die einzelnen Koeffizienten hinsichtlich Relevanz (hier sind standardisierte Koeffizienten hilfreich) und Signifikanz.
3. Vergleichen Sie beide Modelle miteinander. Ist das zweite Modell eine signifikante Verbesserung zum ersten?


### Übung 3

Überprüfen Sie für das zweite Modell (mit allen Prädiktoren) aus der vorigen Übung folgende Voraussetzungen:

- Sind die unabhängigen Variablen kollinear (VIF)?
- Sind die Residuen unabhängig (Durbin-Watson-Test)?
- Sind die Residuen normalverteilt (QQ-Plot)?
- Sind die Abhängigkeiten linear und ist die Varianz homogen (Plot Residuen vs. vorhergesagte Werte)?
- Gibt es Datenpunkte mit großem Einfluss auf das Modell (Plot Residuen vs. Leverage)?

*Hinweis:* Sehen Sie sich die Hilfe zur Funktion `plot.lm()` an (damit können Sie ein lineares Modell plotten und die Grafiken aus den letzten drei Fragen erstellen; mit dem Argument `which` können Sie sich die gewünschte Grafik herauspicken).


### Übung 4

Berechnen Sie für das zweite Modell aus dem vorigen Beispiel folgende diagnostische Statistiken für jeden Datenpunkt:

1. Cook's Distanz
2. Residuen
3. Standardisierte Residuen
4. Studentisierte Residuen
5. DFBeta
6. DFFit
7. Leverage

Finden Sie heraus, bei wie vielen Datenpunkten der Betrag der standardisierten Residuen größer als 3 ist (d.h. wie viele Datenpunkte haben standardisierte Residuen größer als 3 oder kleiner als −3). Wie viel Prozent der Daten haben standardisierte Residuen größer als 2 bzw. kleiner als −2?

*Hinweise:* Die Funktion `nrow()` gibt die Anzahl der Zeilen eines Data Frames zurück. Die Funktion `abs()` berechnet den Absolutbetrag einer Zahl.
