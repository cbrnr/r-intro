[
  {
    "objectID": "09/09.html",
    "href": "09/09.html",
    "title": "9 – Korrelation",
    "section": "",
    "text": "Oft ist es interessant zu fragen, ob zwei Variablen eine gegenseitige Abhängigkeit zeigen. Man möchte also wissen, ob sich die beiden Größen ähnlich verhalten oder nicht – wenn die eine Variable zunimmt, nimmt dann die andere auch zu? Oder ist es genau umgekehrt und sie nimmt dann immer ab? Die Korrelation ist ein einfaches und populäres Maß, um diese Fragestellungen zu beantworten.\nDabei ist es aber wichtig zu wissen, dass kein kausaler Zusammenhang zwischen den beiden Variablen bestehen muss. Folgende Beispiele von Spurious Correlations illustrieren die Gefahr der fehlerhaften Interpretation von Korrelationen. Im ersten Beispiel werden die Ausgaben für den Bereich Wissenschaft, Raumfahrt und Technik der USA im Zeitraum 1999–2009 dargestellt. Gleichzeitig werden die Anzahl der Selbstmorde durch Hängen, Strangulieren und Erstickung im selben Zeitraum gezeigt. Man sieht, dass beide Kurven einen sehr ähnlichen Verlauf haben, und der Korrelationskoeffizient (dazu später mehr) ist mit \\(r=0.998\\) extrem hoch:\n\nDas zweite Beispiel zeigt den Zusammenhang zwischen der Anzahl an Personen, die in einen Pool gefallen und ertrunken sind und der Anzahl an Filmen mit Nicolas Cage im Zeitraum 1999–2009. Auch hier kann man einen schönen Zusammenhang erkennen, welcher einen recht hohen Korrelationskoeffizienten von \\(r=0.666\\) aufweist:\n\nDiese Beispiele sollen verdeutlichen, dass Korrelation nicht automatisch einen kausalen Zusammenhang darstellt (“correlation is not causation”). Denn offensichtlich sind die gezeigten Beispiele rein zufällige Korrelationen, die in keinerlei kausalem Zusammenhang stehen."
  },
  {
    "objectID": "09/09.html#allgemeines",
    "href": "09/09.html#allgemeines",
    "title": "9 – Korrelation",
    "section": "",
    "text": "Oft ist es interessant zu fragen, ob zwei Variablen eine gegenseitige Abhängigkeit zeigen. Man möchte also wissen, ob sich die beiden Größen ähnlich verhalten oder nicht – wenn die eine Variable zunimmt, nimmt dann die andere auch zu? Oder ist es genau umgekehrt und sie nimmt dann immer ab? Die Korrelation ist ein einfaches und populäres Maß, um diese Fragestellungen zu beantworten.\nDabei ist es aber wichtig zu wissen, dass kein kausaler Zusammenhang zwischen den beiden Variablen bestehen muss. Folgende Beispiele von Spurious Correlations illustrieren die Gefahr der fehlerhaften Interpretation von Korrelationen. Im ersten Beispiel werden die Ausgaben für den Bereich Wissenschaft, Raumfahrt und Technik der USA im Zeitraum 1999–2009 dargestellt. Gleichzeitig werden die Anzahl der Selbstmorde durch Hängen, Strangulieren und Erstickung im selben Zeitraum gezeigt. Man sieht, dass beide Kurven einen sehr ähnlichen Verlauf haben, und der Korrelationskoeffizient (dazu später mehr) ist mit \\(r=0.998\\) extrem hoch:\n\nDas zweite Beispiel zeigt den Zusammenhang zwischen der Anzahl an Personen, die in einen Pool gefallen und ertrunken sind und der Anzahl an Filmen mit Nicolas Cage im Zeitraum 1999–2009. Auch hier kann man einen schönen Zusammenhang erkennen, welcher einen recht hohen Korrelationskoeffizienten von \\(r=0.666\\) aufweist:\n\nDiese Beispiele sollen verdeutlichen, dass Korrelation nicht automatisch einen kausalen Zusammenhang darstellt (“correlation is not causation”). Denn offensichtlich sind die gezeigten Beispiele rein zufällige Korrelationen, die in keinerlei kausalem Zusammenhang stehen."
  },
  {
    "objectID": "09/09.html#produkt-moment-korrelation-pearson-korrelation",
    "href": "09/09.html#produkt-moment-korrelation-pearson-korrelation",
    "title": "9 – Korrelation",
    "section": "Produkt-Moment-Korrelation (Pearson-Korrelation)",
    "text": "Produkt-Moment-Korrelation (Pearson-Korrelation)\nDie Produkt-Moment-Korrelation \\(r\\) (auch Pearson-Korrelation genannt) ist ein Maß für den Grad des linearen Zusammenhangs zweier intervallskalierter Variablen. Die Korrelation nimmt Werte zwischen −1 und 1 an und wird aus den Varianzen und der Kovarianz beider Variablen berechnet.\nDie Varianz einer Variablen \\(x\\) ist wie folgt definiert:\n\\[\\mathrm{Var}(x) = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})^2\\]\nHier ist \\(\\bar{x}\\) der Mittelwert über alle \\(N\\) Werte (welche als \\(x_i\\) bezeichnet werden), also\n\\[\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i.\\]\nAlternativ kann man die Varianz auch so schreiben (das Quadrat wird explizit als Multiplikation angeschrieben):\n\\[\\mathrm{Var}(x) = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})(x_i - \\bar{x})\\]\nDie Varianz beschreibt, wie stark die Datenpunkte um deren Mittelwert variieren (streuen). Dementsprechend ist die Kovarianz zwischen zwei Variablen \\(x\\) und \\(y\\) definiert als\n\\[\\mathrm{Cov}(x, y) = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y}).\\]\nDie Kovarianz beschreibt, wie stark die beiden Variablen gemeinsam um den jeweiligen Mittelwert variieren. Eine positive Kovarianz bedeutet, dass beide Variablen gleichsinnig variieren (d.h. wenn eine Variable größer als ihr Mittelwert ist, dann ist die andere auch größer). Umgekehrt bedeutet eine negative Kovarianz, dass beide Variablen gegensinnig variieren (ist eine Variable größer als ihr Mittelwert, dann ist die andere Variable kleiner).\nDie Kovarianz ist kein standardisiertes Maß, d.h. man kann nicht einfach zwei Kovarianzen aus unterschiedlichen Messreihen miteinander vergleichen, da sie von der Skalierung der beiden Variablen abhängt. Die Pearson-Korrelation standardisiert nun die Kovarianz mit den Varianzen der einzelnen Variablen und stellt somit sicher, dass die Korrelation im Wertebereich zwischen −1 und 1 liegt:\n\\[r = \\frac{\\mathrm{Cov}(x, y)}{\\sqrt{\\mathrm{Var}(x) \\mathrm{Var}(y)}}\\]\n\nRechenbeispiel\nVersuchen wir, die Korrelation mit den Formeln für zwei Beispielvektoren \\(x\\) und \\(y\\) zu berechnen:\n\\[\n\\begin{aligned}\nx &= (8, 1, -4, 5, 6, 10, 9) \\\\\ny &= (-2, -5, -6, 0, 3, 7, 10)\n\\end{aligned}\n\\]\nWir können diese sieben Datenpunkte in einem Scatterplot visualisieren:\n\n\n\n\n\n\n\n\n\nZunächst berechnen wir die beiden Mittelwerte:\n\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{7} \\cdot (8 + 1 - 4 + 5 + 6 + 10 + 9) = 5 \\\\\n\\bar{y} &= \\frac{1}{7} \\cdot (-2 - 5 - 6 + 0 + 3 + 7 + 10) = 1\n\\end{aligned}\n\\]\nDamit können wir die Varianzen berechnen:\n\\[\n\\begin{aligned}\n\\text{Var}(x) = \\frac{1}{6} \\cdot \\Bigl[ & (8 - 5)^2 + (1 - 5)^2 + (-4 - 5)^2 \\\\\n&+ (5 - 5)^2 + (6 - 5)^2 + (10 - 5)^2 + (9 - 5)^2 \\Bigr] = 24.\\dot{6} \\\\\n\\text{Var}(y) = \\frac{1}{6} \\cdot \\Bigl[ & (-2 - 1)^2 + (-5 - 1)^2 + (-6 - 1)^2 \\\\\n&+ (0 - 1)^2 + (3 - 1)^2 + (7 - 1)^2 + (10 - 1)^2 \\Bigr] = 36\n\\end{aligned}\n\\]\nSchließlich berechnen wir die Kovarianz:\n\\[\n\\begin{aligned}\n\\text{Cov}(x, y) = \\frac{1}{6} \\cdot \\Bigl[ & (8 - 5) \\cdot (-2 -1) + (1 - 5) \\cdot (-5 - 1) \\\\\n&+ (-4 - 5) \\cdot (-6 - 1) + (5 - 5) \\cdot (0 - 1) \\\\\n&+ (6 - 5) \\cdot (3 - 1) + (10 - 5) \\cdot (7 - 1) + (9 - 5) \\cdot (10 - 1) \\Bigr] = 24.\\dot{3}\n\\end{aligned}\n\\]\nUm die Pearson-Korrelation zu erhalten, müssen wir noch diese Größen laut Formel verarbeiten und erhalten somit:\n\\[r = \\frac{24.\\dot{3}}{\\sqrt{24.\\dot{6} \\cdot 36}} = 0.8165732\\]\n\n\nSignifikanz\nMeist wird nach der Berechnung der Korrelation ein Test durchgeführt, welcher prüft, ob die erhaltene Korrelation signifikant von der Nullhypothese (“es existiert keine Korrelation, d.h. die Korrelation ist 0”) abweicht. Da die Stichprobenverteilung der Korrelation keine Normalverteilung aufweist, kann man den Wert von \\(r\\) mit Hilfe der Fisher-Transformation in eine Normalverteilung mit Mittelwert\n\\[z_r = \\frac{1}{2} \\ln \\frac{1 + r}{1 - r} = \\text{arctanh}(r)\\]\numwandeln. Der Standardfehler von \\(z_r\\) beträgt dabei:\n\\[\\mathrm{SE}(z_r) = \\frac{1}{\\sqrt{N - 3}}\\]\nNun kann man den erhaltenen Wert von \\(z_r\\) durch den Standardfehler dividieren und das Ergebnis dann mit Werten aus einer Tabelle der Standardnormalverteilung vergleichen. So erhält man dann den \\(p\\)-Wert (siehe auch hier).\n\n\n\n\n\n\nHinweis\n\n\n\nDer \\(p\\)-Wert gibt hier die Wahrscheinlichkeit an, dass man eine Korrelation erhält, die mindestens so groß ist wie die beobachtete, unter der Annahme, dass die Nullhypothese zutrifft (also dass die Korrelation 0 ist).\nEin kleiner \\(p\\)-Wert bedeutet also, dass der beobachtete Wert unter der Nullhypothese sehr unwahrscheinlich ist. Daher entscheidet man sich in solchen Situationen, die Nullhypothese zu verwerfen und die Korrelation als signifikant (unterschiedlich von 0) zu betrachten.\n\n\nBetrachten wir dazu ein Beispiel zur Veranschaulichung der Berechnung des \\(p\\)-Wertes sowie des Konfidenzintervalls für eine gegebene Pearson-Korrelation.\n\n\nBeispiel\nGegeben sei eine Korrelation \\(r = 0.25\\) berechnet aus einer Stichprobe der Größe \\(N = 40\\):\n\nr = 0.25\nN = 40\n\nWeiters geben wir ein Signifikanzniveau von \\(\\alpha = 0.05\\) vor (das ist der Schwellwert für den \\(p\\)-Wert, ab dem wir eine Korrelation als signifikant betrachten wollen):\n\nalpha = 0.05\n\nWir möchten nun wissen, ob die Korrelation \\(r = 0.25\\) bei einem gegebenen Signifikanzniveau signifikant unterschiedlich von 0 ist. Dazu berechnen wir den \\(p\\)-Wert und das Konfidenzintervall. Um dies berechnen zu können, müssen wir zuerst die Fisher-Transformation von \\(r\\) berechnen:\n\n(z = atanh(r))\n\n[1] 0.2554128\n\n\nJetzt können wir den Standardfehler berechnen:\n\n(se_z = 1 / sqrt(N - 3))\n\n[1] 0.164399\n\n\nDen \\(p\\)-Wert erhält man nun, indem man die Wahrscheinlichkeit berechnet, dass man in einer Standardnormalverteilung Werte größer als \\(z \\, / \\, \\text{SE}(z) \\approx 1.55\\) erhält. Dazu sieht man entweder in einer Tabelle der Standardnormalverteilung nach oder man berechnet diesen Wert mit entsprechenden Funktionen in R (der Faktor 2 wird benötigt, da wir keine gerichtete Hypothese haben):\n\n(p = 2 * (1 - pnorm(z / se_z)))\n\n[1] 0.1202762\n\n\nDieser \\(p\\)-Wert ist größer als \\(\\alpha = 0.05\\), d.h. wir können die Nullhypothese (keine Korrelation) nicht verwerfen.\nDas Konfidenzintervall um \\(z\\) erhält man, indem man zum gegebenen Wert \\(z\\) das Produkt aus dem Signifikanzniveau entsprechenden Quantil (ca. 1.96 für \\(\\alpha = 0.05\\)) mit dem Standardfehler addiert bzw. subtrahiert:\n\ncl_z = z - qnorm(1 - alpha/2) * se_z\ncu_z = z + qnorm(1 - alpha/2) * se_z\nc(cl_z, z, cu_z)\n\n[1] -0.06680328  0.25541281  0.57762891\n\n\nMan beachte, dass es sich bei allen drei Werten um Fisher-transformierte Werte handelt. Möchte man ein Konfidenzintervall um die ursprüngliche Korrelation \\(r\\) angeben, so muss man diese drei Werte noch rücktransformieren (tanh ist die Umkehrfunktion von atanh):\n\ncl_r = tanh(cl_z)\ncu_r = tanh(cu_z)\nc(cl_r, r, cu_r)\n\n[1] -0.06670409  0.25000000  0.52093993\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDas Konfidenzintervall gibt an, in welchem Bereich der wahre Wert der Korrelation mit einer gegebenen Sicherheit (Konfidenz) liegt. Die Konfidenz ist nicht die Wahrscheinlichkeit, dass der wahre Wert in dem berechneten Intervall liegt – denn der wahre Wert ist entweder in dem Intervall oder nicht. Wenn man viele Stichproben ziehen würde und damit jedes Mal ein Konfidenzintervall berechnen würde, dann würde im Falle eines 95%-Konfidenzintervalls in 95% der berechneten Fälle der wahre Wert in dem Intervall liegen.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDie Tatsache, dass die Korrelation \\(r=0.25\\) bei einer Stichprobe von \\(N=40\\) nicht signifikant unterschiedlich von Null ist (\\(p=0.12\\)) kann man auch daran erkennen, dass das 95%-Konfidenzintervall den Wert 0 (also die Nullhypothese) enthält. Das bedeutet also, dass wir die Nullhypothese (keine Korrelation) nicht verwerfen können."
  },
  {
    "objectID": "09/09.html#nicht-parametrische-korrelationskoeffizienten",
    "href": "09/09.html#nicht-parametrische-korrelationskoeffizienten",
    "title": "9 – Korrelation",
    "section": "Nicht-parametrische Korrelationskoeffizienten",
    "text": "Nicht-parametrische Korrelationskoeffizienten\n\nSpearman Rangkorrelationskoeffizient\nIm Gegensatz zur Pearson-Korrelation misst der Spearman Rangkorrelationskoeffizient \\(\\rho\\) nicht nur lineare Zusammenhänge zwischen zwei Variablen, sondern der Zusammenhang kann eine beliebige monotone (nicht-lineare) Funktion sein. Die beiden Variablen müssen auch nicht intervallskaliert sein, d.h. so kann man auch ordinalskalierte Daten miteinander korrelieren.\nIm Prinzip berechnet man die Spearman-Korrelation, indem man die Daten \\(x\\) und \\(y\\) vorher in Ränge konvertiert und dann die Pearson-Korrelation berechnet. Zur Berechnung kann folgende vereinfachte Formel verwendet werden:\n\\[\\rho = 1 - \\frac{6 \\sum d_i^2}{N \\cdot (N^2 - 1)}\\] Hier ist \\(d_i\\) die Differenz der Ränge einer Beobachtung.\n\n\nKendall Rangkorrelationskoeffizient\nBei kleinen Stichproben und relativ großer Anzahl an gleichen Rängen liefert oft der Kendall Rangkorrelationskoeffizient \\(\\tau\\) bessere Ergebnisse. Hier werden nicht die Differenzen zwischen den Rängen betrachtet (also die Abstände der Ränge zwischen beiden Variablen), sondern ob es Unterschiede in den Rängen zwischen Datenpaaren gibt oder nicht."
  },
  {
    "objectID": "09/09.html#korrelationen-mit-r-berechnen",
    "href": "09/09.html#korrelationen-mit-r-berechnen",
    "title": "9 – Korrelation",
    "section": "Korrelationen mit R berechnen",
    "text": "Korrelationen mit R berechnen\nKorrelationskoeffizienten kann man mit den folgenden drei Funktionen berechnen: cor(), cor.test() und rcorr(). Die ersten beiden Funktionen sind Teil von R, die Funktion rcorr() muss mit dem Hmisc-Paket geladen werden.\n\nlibrary(Hmisc)\n\nDie drei Funktionen haben unterschiedliche Features, d.h. welche Funktion man benutzt ist abhängig von den benötigten Eigenschaften, welche in nachfolgender Tabelle zusammengefasst sind (“CI” steht für Konfidenzintervall, “Mehrfach” bedeutet, dass Korrelationen zwischen mehr als zwei Variablen paarweise berechnet werden können).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearson\nSpearman\nKendall\n\\(p\\)-Werte\nCI\nMehrfach\n\n\n\n\ncor\nx\nx\nx\n\n\nx\n\n\ncor.test\nx\nx\nx\nx\nx\n\n\n\nrcorr\nx\nx\n\nx\n\nx\n\n\n\n\nFunktion cor()\nDie Funktion cor() ruft man wie folgt auf:\ncor(x, y, method=\"pearson\")\nHier übergibt man zwei Vektoren und spezifiziert, welche Korrelation berechnet werden soll (standardmäßig wird die Pearson-Korrelation berechnet). Wenn x ein Data Frame mit mindestens zwei Spalten ist, kann man y weglassen – dann werden automatisch die Korrelationen zwischen allen Spaltenpaaren berechnet."
  },
  {
    "objectID": "09/09.html#funktion-cor.test",
    "href": "09/09.html#funktion-cor.test",
    "title": "9 – Korrelation",
    "section": "Funktion cor.test()",
    "text": "Funktion cor.test()\nDer Aufruf der Funktion cor.test() ist sehr ähnlich:\ncor.test(x, y, alternative=\"t\", method=\"pearson\", conf.level=0.95)\nHier kann man die Form der Alternativhypothese (\"two-sided\", \"greater\", \"less\") sowie das Konfidenzniveau angeben. Diese Funktion kann nur mit genau zwei Vektoren umgehen."
  },
  {
    "objectID": "09/09.html#funktion-rcorr",
    "href": "09/09.html#funktion-rcorr",
    "title": "9 – Korrelation",
    "section": "Funktion rcorr()",
    "text": "Funktion rcorr()\nDie Funktion rcorr() verwendet man wie folgt:\nrcorr(x, y, type=\"pearson\")\nWie bei den anderen Funktionen sind x und y Vektoren. Weiters ist es wie bei cor() möglich, nur das Argument x anzugeben, wenn dieses eine Matrix mit mindestens zwei Spalten ist."
  },
  {
    "objectID": "09/09.html#beispiel-1",
    "href": "09/09.html#beispiel-1",
    "title": "9 – Korrelation",
    "section": "Beispiel",
    "text": "Beispiel\nAm besten können die drei Funktionen anhand eines Beispiels veranschaulicht werden. Dazu laden wir einen (fiktiven) Datensatz exam.dat über Prüfungsangst:\n\nlibrary(readr)\n(exam = read_tsv(\"exam.dat\"))\n\n# A tibble: 103 × 5\n    Code Revise  Exam Anxiety Gender\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n 1     1      4    40    86.3 Male  \n 2     2     11    65    88.7 Female\n 3     3     27    80    70.2 Male  \n 4     4     53    80    61.3 Male  \n 5     5      4    40    89.5 Male  \n 6     6     22    70    60.5 Female\n 7     7     16    20    81.5 Female\n 8     8     21    55    75.8 Female\n 9     9     25    50    69.4 Female\n10    10     18    40    82.3 Female\n# ℹ 93 more rows\n\n\nEs ist hilfreich, die Daten zuerst einmal grafisch darzustellen. Für den Zusammenhang zwischen zwei Variablen bietet sich ein Scatterplot (inklusive Regressionsgeraden) an – so kann man die Korrelation direkt visualisieren. Wir können eine solche Grafik zunächst einmal separat für alle drei Kombinationen erzeugen:\n\nplot(exam$Revise, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))\nabline(lm(exam$Exam ~ exam$Revise), col=\"blue\")\n\n\n\n\n\n\n\nplot(exam$Anxiety, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))\nabline(lm(exam$Exam ~ exam$Anxiety), col=\"blue\")\n\n\n\n\n\n\n\nplot(exam$Revise, exam$Anxiety, pch=16, col=rgb(0, 0, 0, 0.5))\nabline(lm(exam$Anxiety ~ exam$Revise), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nBei mehreren Variablen (wie in diesem Beispiel) kann man die Funktion pairs() verwenden, um alle paarweisen Scatterplots gleichzeitig darzustellen:\n\npairs(exam[, 2:4])\n\n\n\n\n\n\n\n\nMöchte man auch Regressionsgeraden einzeichnen, kann man eine (anonyme) Funktion als panel-Argument übergeben:\n\npairs(\n    exam[2:4],\n    panel=function(x, y) {\n        points(x, y, pch=16, col=rgb(0, 0, 0, 0.5))\n        abline(lm(y ~ x), col=\"blue\")\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\nPearson-Korrelation\nNun berechnen wir die Pearson-Korrelationen zwischen den drei Variablen Exam, Anxiety und Revise:\n\ncor(exam[, c(\"Exam\", \"Anxiety\", \"Revise\")])\n\n              Exam    Anxiety     Revise\nExam     1.0000000 -0.4409934  0.3967207\nAnxiety -0.4409934  1.0000000 -0.7092493\nRevise   0.3967207 -0.7092493  1.0000000\n\n\nMan kann aus dieser Korrelationsmatrix direkt die einzelnen Koeffizienten für alle Variablenpaare ablesen. Die Diagonale beinhaltet die Korrelationen der Variablen mit sich selbst und besteht daher aus lauter Werten, die exakt gleich 1 sind. Außerdem ist es egal, ob man die Korrelationen in dem Dreieck unter der Diagonale oder über der Diagonale abliest, da die Korrelationsmatrix symmetrisch ist (die Korrelation zwischen “Exam” und “Revise” ist dieselbe wie zwischen “Revise” und “Exam” – die Korrelation misst ja keine kausalen Zusammenhänge).\nMöchte man jedoch auch \\(p\\)-Werte, muss man die Funktion rcorr() verwenden. Diese Funktion erwartet die Daten jedoch nicht als Data Frame, sondern als Matrix. Daher müssen die Daten beim Aufruf der Funktion in eine Matrix umgewandelt werden:\n\nrcorr(as.matrix(exam[, c(\"Exam\", \"Anxiety\", \"Revise\")]))\n\n         Exam Anxiety Revise\nExam     1.00   -0.44   0.40\nAnxiety -0.44    1.00  -0.71\nRevise   0.40   -0.71   1.00\n\nn= 103 \n\n\nP\n        Exam Anxiety Revise\nExam          0       0    \nAnxiety  0            0    \nRevise   0    0            \n\n\nZusätzlich zur Korrelationsmatrix bekommt man auch die \\(p\\)-Werte geliefert. In diesem Beispiel sind alle Korrelationen signifikant, da die \\(p\\)-Werte sehr klein sind (gerundet Null).\nWenn man auch Konfidenzintervalle benötigt, muss man die Funktion cor.test() verwenden. Diese Funktion unterstützt aber nur zwei Variablen, d.h. bei mehreren Variablen muss man die Funktion mehrmals aufrufen, um alle paarweisen Korrelationen zu erhalten.\n\ncor.test(exam$Anxiety, exam$Exam)\n\n\n    Pearson's product-moment correlation\n\ndata:  exam$Anxiety and exam$Exam\nt = -4.938, df = 101, p-value = 3.128e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5846244 -0.2705591\nsample estimates:\n       cor \n-0.4409934 \n\n\n\n\nBestimmtheitsmaß \\(R^2\\)\nWenn man den Korrelationskoeffizienten \\(r\\) quadriert, erhält man das Bestimmtheitsmaß \\(R^2\\). Es gibt an, wie viel Varianz in einer Variablen von der zweiten erklärt werden kann (wobei auch hier wieder gilt, dass dies keine Aussage über Kausalität ist). Es ist üblich, die Korrelation mit dem Kleinbuchstaben \\(r\\) zu bezeichnen, für das Bestimmtheitsmaß verwendet man aber einen Großbuchstaben \\(R^2\\).\n\\[R^2 = r^2\\]\n\ncor(exam[, c(\"Exam\", \"Anxiety\", \"Revise\")])^2\n\n             Exam   Anxiety    Revise\nExam    1.0000000 0.1944752 0.1573873\nAnxiety 0.1944752 1.0000000 0.5030345\nRevise  0.1573873 0.5030345 1.0000000\n\n\n\n\nSpearman Rangkorrelationskoeffizient\nFür die Beispieldaten können wir analog auch die Spearman-Korrelation bestimmen:\n\ncor(exam[, c(\"Exam\", \"Anxiety\", \"Revise\")], method=\"spearman\")\n\n              Exam    Anxiety     Revise\nExam     1.0000000 -0.4046141  0.3498948\nAnxiety -0.4046141  1.0000000 -0.6219694\nRevise   0.3498948 -0.6219694  1.0000000\n\nrcorr(as.matrix(exam[, c(\"Exam\", \"Anxiety\", \"Revise\")]), type=\"spearman\")\n\n         Exam Anxiety Revise\nExam     1.00   -0.40   0.35\nAnxiety -0.40    1.00  -0.62\nRevise   0.35   -0.62   1.00\n\nn= 103 \n\n\nP\n        Exam  Anxiety Revise\nExam          0e+00   3e-04 \nAnxiety 0e+00         0e+00 \nRevise  3e-04 0e+00         \n\ncor.test(exam$Revise, exam$Exam, method=\"spearman\")\n\nWarning in cor.test.default(exam$Revise, exam$Exam, method = \"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  exam$Revise and exam$Exam\nS = 118387, p-value = 0.0002913\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3498948 \n\n\n\n\nKendall Rangkorrelationskoeffizient\nDie Funktion cor.test() gibt im vorigen Beispiel eine Warnung aus, dass der berechnete \\(p\\)-Wert nicht exakt ist, da die Daten gleiche Ränge beinhalten. In solchen Fällen ist daher der Kendall-Korrelationskoeffizient die bessere Wahl:\n\ncor.test(exam$Revise, exam$Exam, method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  exam$Revise and exam$Exam\nz = 3.8034, p-value = 0.0001427\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2633259"
  },
  {
    "objectID": "09/09.html#übungen",
    "href": "09/09.html#übungen",
    "title": "9 – Korrelation",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nIm theoretischen Teil der Übung haben wir in einem Beispiel die Signifikanz einer Korrelation \\(r = 0.25\\) bei einer Stichprobengröße von \\(N = 40\\) berechnet. Diese war mit \\(p = 0.12\\) nicht signifikant.\n\nWelche Stichprobengröße \\(N\\) müssten Sie mindestens wählen, um ein mit \\(\\alpha = 0.05\\) signifikantes Ergebnis zu erhalten?\nAb welcher Stichprobengröße wird sogar eine sehr kleine Korrelation von \\(r = 0.05\\) signifikant?\n\nSie können dieses Beispiel durch Probieren lösen, indem Sie die Stichprobengröße schrittweise verändern (vergrößern) und sich den zugehörigen \\(p\\)-Wert ausrechnen.\n\n\nÜbung 2\nIn dieser Einheit haben wir die Korrelation zwischen \\(x = (8, 1, -4, 5, 6, 10, 9)^T\\) und \\(y = (-2, -5, -6, 0, 3, 7, 10)^T\\) händisch berechnet. Berechnen Sie nun mit Hilfe von R die Pearson-Korrelation zwischen \\(x\\) und \\(y\\). Geben Sie außerdem das 99%-Konfidenzintervall sowie den \\(p\\)-Wert an! Ist diese Korrelation signifikant bei einem Signifikanzniveau von \\(\\alpha = 0.01\\)?\n\n\nÜbung 3\nVerwenden Sie die Beispieldaten mtcars und analysieren Sie den Zusammenhang zwischen den Variablen mpg, disp und hp. Stellen Sie den Zusammenhang zwischen den Variablenpaaren grafisch dar, und berechnen Sie danach Pearson- und Spearman-Korrelationen!\n\n\nÜbung 4\nIn der Datei pm10.csv finden Sie die monatlichen Feinstaubwerte PM10 von zwei Messstationen in Graz im Zeitraum Februar 2006 bis Mai 2016. Führen Sie folgende Analysen durch:\n\nErstellen Sie eine Grafik, in der Sie den Verlauf der PM10-Konzentration von beiden Messstationen über die Zeit darstellen.\nErstellen Sie eine Grafik, in der Sie den Zusammenhang zwischen den beiden Messstationen darstellen.\n\nBerechnen Sie abschließend die Pearson-Korrelation zwischen den Daten beider Messstationen inklusive Konfidenzinterval sowie \\(p\\)-Wert. Wie viel Varianz der einen Variable kann durch die andere Variable erklärt werden?\n\n\n\n\n\n\nHinweis\n\n\n\nWandeln Sie die Spalte Datum nach dem Einlesen in einen Datums-Typ um. Für die gleichzeitige Darstellung der beiden Zeitverläufe können Sie zuerst eine Liniengrafik einer Variable erzeugen und dann mit der Funktion lines() den zweiten Zeitverlauf mit einer anderen Farbe hinzufügen.\nWenn Sie die Funktion cor() zur Berechnung der Korrelation zwischen den beiden Variablen verwenden möchten, können Sie fehlende Werte in jeder der beiden Variablen mit dem Argument use=\"complete.obs\" ausschließen.\n\n\n\n\nÜbung 5\nBerechnen Sie die Korrelation der Schnabellänge (bill_length_mm) mit der Schnabeltiefe (bill_depth_mm) im Datensatz penguins (aus dem Paket palmerpenguins). Berechnen Sie außerdem diese Korrelation für jede Spezies separat! Was stellen Sie fest?"
  },
  {
    "objectID": "02/02-solutions.html",
    "href": "02/02-solutions.html",
    "title": "2 – Lösungen",
    "section": "",
    "text": "Pakete können mit der Funktion install.packages() installiert werden, für die drei genannten Pakete führt man also folgende drei Befehle aus:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"psych\")\nDie Versionsnummern sind in der “Packages”-Ansicht in RStudio ersichtlich; ich habe folgende Versionen installiert:\n\ntidyverse: 2.0.0\nHmisc: 5.1-3\npsych: 2.4.6.26\n\nInstallierte Pakete können mit der Funktion library() aktiviert werden, also für die drei Pakete:\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(psych)"
  },
  {
    "objectID": "02/02-solutions.html#übung-1",
    "href": "02/02-solutions.html#übung-1",
    "title": "2 – Lösungen",
    "section": "",
    "text": "Pakete können mit der Funktion install.packages() installiert werden, für die drei genannten Pakete führt man also folgende drei Befehle aus:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"psych\")\nDie Versionsnummern sind in der “Packages”-Ansicht in RStudio ersichtlich; ich habe folgende Versionen installiert:\n\ntidyverse: 2.0.0\nHmisc: 5.1-3\npsych: 2.4.6.26\n\nInstallierte Pakete können mit der Funktion library() aktiviert werden, also für die drei Pakete:\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(psych)"
  },
  {
    "objectID": "02/02-solutions.html#übung-2",
    "href": "02/02-solutions.html#übung-2",
    "title": "2 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n?help\nhelp(help)"
  },
  {
    "objectID": "02/02-solutions.html#übung-3",
    "href": "02/02-solutions.html#übung-3",
    "title": "2 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\nEin Funktionsaufruf besteht aus dem Funktionsnamen gefolgt von (). Innerhalb dieser Klammern können der Funktion eventuelle Argumente übergeben werden."
  },
  {
    "objectID": "02/02-solutions.html#übung-4",
    "href": "02/02-solutions.html#übung-4",
    "title": "2 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\nFolgender Inhalt wird in der Textdatei my_first_script.R gespeichert:\n# Übung 4\nlibrary(Hmisc)\n(45 + 66 + 37 + 54 + 7 + 22) / 6\nDieses Script lässt sich fehlerfrei ausführen (z.B. durch Klicken auf “Source” in RStudio)."
  },
  {
    "objectID": "02/02-solutions.html#übung-5",
    "href": "02/02-solutions.html#übung-5",
    "title": "2 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\nDie Hilfe wird mit ?sum angezeigt. Daraus ist zu entnehmen, dass die Funktion auch mit keinem Argument aufzurufen ist (das Ergebnis von sum() ist 0). Das Argument ... hat nämlich eine spezielle Bedeutung, es steht für “beliebig viele Argumente”, also keines, eines, zwei, drei, usw. Es gibt insgesamt also gewissermaßen zwei optionale Argumente (denn na.rm hat den Standardwert FALSE).\nLaut Hilfe ist die Summe von “nichts” – also sum() – gleich Null. Dies funktioniert bei der Funktion mean() nicht, da man hier ein Argument übergeben muss (einen Vektor mit Zahlen).\nDas erste Argument der Funktion sum() ist ..., was wie bereits erwähnt bedeutet, dass beliebig viele Werte (Zahlen) übergeben werden können. Die Funktion mean() erwartet hingegen alle zu mittelnden Zahlen im ersten Argument x. Wenn man also mean(1, 2, 3) aufruft, dann ist das erste Argument x gleich 1 (es soll also nur die Zahl 1 gemittelt werden). Die weiteren Zahlen 2 und 3 werden dann dem zweiten bzw. dritten Argument zugewiesen (also trim und na.rm).\nDer Befehl sum ruft die Funktion nicht auf. Es werden lediglich Informationen über das Objekt sum ausgegeben. In den allermeisten Fällen ist das nicht gewünscht, sondern man möchte eine Funktion mit sum() aufrufen."
  },
  {
    "objectID": "12/12.html",
    "href": "12/12.html",
    "title": "12 – Mittelwertvergleich",
    "section": "",
    "text": "Neben der Betrachtung von Zusammenhängen zwischen Variablen (zum Beispiel mittels Korrelation oder Regression) möchten wir häufig wissen, ob sich zwei (oder mehr) Gruppen in ihren Mittelwerten unterscheiden. In korrelativen Studien (Beobachtungsstudien) erheben wir Daten, ohne etwas zu manipulieren. Experimentelle Studien (Interventionsstudien) manipulieren hingegen gezielt eine oder mehrere Variablen (z.B. eine Gruppe erhält ein Medikament und eine andere ein Placebo). Dadurch können wir Unterschiede im Ergebnis der Manipulation zuschreiben und kausale Schlussfolgerungen ziehen, was bei korrelativen Studien nicht möglich ist.\nInnerhalb experimenteller Studien unterscheiden wir zwischen unabhängigen und abhängigen Designs. In einem unabhängigen Design bestehen die Gruppen aus unterschiedlichen Personen, sodass die Beobachtungen zwischen den Gruppen nicht zusammenhängen. In einem abhängigen Design (auch Messwiederholungs-Design genannt) nehmen dieselben Personen an mehreren Bedingungen teil, wodurch eine Abhängigkeit zwischen den Gruppen entsteht. Da jede Person als ihre eigene Kontrollbedingung dient, rechnen abhängige Designs automatisch individuelle Unterschiede heraus, die für die Behandlung irrelevant sind. Dadurch hat die Analyse mehr statistische Power als bei einem unabhängigen Design (d.h. kleinere Unterschiede können besser erkannt werden). Dieser Vorteil wird im folgenden Beispiel deutlich."
  },
  {
    "objectID": "12/12.html#versuchsdesigns",
    "href": "12/12.html#versuchsdesigns",
    "title": "12 – Mittelwertvergleich",
    "section": "",
    "text": "Neben der Betrachtung von Zusammenhängen zwischen Variablen (zum Beispiel mittels Korrelation oder Regression) möchten wir häufig wissen, ob sich zwei (oder mehr) Gruppen in ihren Mittelwerten unterscheiden. In korrelativen Studien (Beobachtungsstudien) erheben wir Daten, ohne etwas zu manipulieren. Experimentelle Studien (Interventionsstudien) manipulieren hingegen gezielt eine oder mehrere Variablen (z.B. eine Gruppe erhält ein Medikament und eine andere ein Placebo). Dadurch können wir Unterschiede im Ergebnis der Manipulation zuschreiben und kausale Schlussfolgerungen ziehen, was bei korrelativen Studien nicht möglich ist.\nInnerhalb experimenteller Studien unterscheiden wir zwischen unabhängigen und abhängigen Designs. In einem unabhängigen Design bestehen die Gruppen aus unterschiedlichen Personen, sodass die Beobachtungen zwischen den Gruppen nicht zusammenhängen. In einem abhängigen Design (auch Messwiederholungs-Design genannt) nehmen dieselben Personen an mehreren Bedingungen teil, wodurch eine Abhängigkeit zwischen den Gruppen entsteht. Da jede Person als ihre eigene Kontrollbedingung dient, rechnen abhängige Designs automatisch individuelle Unterschiede heraus, die für die Behandlung irrelevant sind. Dadurch hat die Analyse mehr statistische Power als bei einem unabhängigen Design (d.h. kleinere Unterschiede können besser erkannt werden). Dieser Vorteil wird im folgenden Beispiel deutlich."
  },
  {
    "objectID": "12/12.html#beispiel",
    "href": "12/12.html#beispiel",
    "title": "12 – Mittelwertvergleich",
    "section": "Beispiel",
    "text": "Beispiel\nIn einer fiktiven Studie wurde die Angst vor Spinnen auf einer Skala von 0 bis 100 gemessen (größere Werte stehen für höhere Angst). Es wurden 24 Personen in zwei Gruppen untersucht (also 12 Personen pro Gruppe). Einer Gruppe wurden Fotos von Spinnen gezeigt, der anderen Gruppe wurden echte Spinnen gezeigt.\nWir beginnen mit dem Importieren der Daten aus der Datei spider.dat:\n\nlibrary(readr)\n(spider = read_tsv(\"spider.dat\"))\n\n# A tibble: 24 × 2\n   Group   Anxiety\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 picture      30\n 2 picture      35\n 3 picture      45\n 4 picture      40\n 5 picture      50\n 6 picture      35\n 7 picture      55\n 8 picture      25\n 9 picture      30\n10 picture      45\n# ℹ 14 more rows\n\n\nDie mittleren Angstwerte für die beiden Gruppen betragen:\n\nby(spider$Anxiety, spider$Group, mean)\n\nspider$Group: picture\n[1] 40\n------------------------------------------------------------------------------------------ \nspider$Group: real\n[1] 47\n\n\nIm Durchschnitt ist die Angst also in der Gruppe, in der echte Spinnen präsentiert wurden, um 7 größer.\n\nUnabhängige Stichproben\nDie Daten in spider enthalten eine Person pro Zeile (ein Format, das sich gut für unabhängige Designs eignet). Die folgende Abbildung zeigt die Angstwerte für jede Person in beiden Gruppen (schwarze Punkte stellen die Gruppenmittelwerte dar, rote Punkte die individuellen Angstwerte, und die Fehlerbalken zeigen 95%-Konfidenzintervalle):\n\n\n\n\n\n\n\n\n\nMan erkennt, dass die Konfidenzintervalle stark überlappen, d.h. der Unterschied zwischen den beiden Mittelwerten ist wahrscheinlich nicht signifikant.\n\n\nAbhängige Stichproben\nStellen wir uns nun vor, dass insgesamt nur 12 Personen untersucht wurden, die aber jeweils in beiden Gruppen getestet wurden. Wir konvertieren also die Daten am besten ins Wide-Format, indem wir für jede Person zwei Spalten mit den Angstwerten erstellen (eine für jede Gruppe):\n\nlibrary(tidyr)\nspider_w = pivot_wider(\n    cbind(id=rep(1:12, 2), spider),\n    names_from=Group,\n    values_from=Anxiety\n)\nspider_w$id = NULL\nspider_w\n\n# A tibble: 12 × 2\n   picture  real\n     &lt;dbl&gt; &lt;dbl&gt;\n 1      30    40\n 2      35    35\n 3      45    50\n 4      40    55\n 5      50    65\n 6      35    55\n 7      55    50\n 8      25    35\n 9      30    30\n10      45    50\n11      40    60\n12      50    39\n\n\nAnschließend berechnen wir angepasste Werte, welche die individuellen Unterschiede der Personen berücksichtigen. Dazu fügen wir zunächst eine Spalte mit der mittleren Angst jeder Person hinzu.\n\nspider_w$mean = rowMeans(spider_w)\nspider_w\n\n# A tibble: 12 × 3\n   picture  real  mean\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      30    40  35  \n 2      35    35  35  \n 3      45    50  47.5\n 4      40    55  47.5\n 5      50    65  57.5\n 6      35    55  45  \n 7      55    50  52.5\n 8      25    35  30  \n 9      30    30  30  \n10      45    50  47.5\n11      40    60  50  \n12      50    39  44.5\n\n\nNun berechnen wir die Differenz der Personenmittelwerte zum Gesamtmittelwert aller Datenpunkte – dies ist der Korrekturfaktor für Designs mit Messwiederholung.\n\nspider_w$adj = mean(c(spider_w$picture, spider_w$real)) - spider_w$mean\nspider_w\n\n# A tibble: 12 × 4\n   picture  real  mean   adj\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      30    40  35     8.5\n 2      35    35  35     8.5\n 3      45    50  47.5  -4  \n 4      40    55  47.5  -4  \n 5      50    65  57.5 -14  \n 6      35    55  45    -1.5\n 7      55    50  52.5  -9  \n 8      25    35  30    13.5\n 9      30    30  30    13.5\n10      45    50  47.5  -4  \n11      40    60  50    -6.5\n12      50    39  44.5  -1  \n\n\nDamit können wir die einzelnen Angstwerte pro Person korrigieren.\n\nspider_w$picture_adj = spider_w$picture + spider_w$adj\nspider_w$real_adj = spider_w$real + spider_w$adj\n\nDadurch haben wir erreicht, dass die korrigierten Werte nun für alle Personen denselben Mittelwert ergeben, d.h. personenspezifische Unterschiede werden herausgerechnet und nur der Unterschied zwischen den Gruppen wird untersucht:\n\nrowMeans(spider_w[, c(\"picture_adj\", \"real_adj\")])\n\n [1] 43.5 43.5 43.5 43.5 43.5 43.5 43.5 43.5 43.5 43.5 43.5 43.5\n\n\nDie Daten sehen nun so aus:\n\nspider_w\n\n# A tibble: 12 × 6\n   picture  real  mean   adj picture_adj real_adj\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1      30    40  35     8.5        38.5     48.5\n 2      35    35  35     8.5        43.5     43.5\n 3      45    50  47.5  -4          41       46  \n 4      40    55  47.5  -4          36       51  \n 5      50    65  57.5 -14          36       51  \n 6      35    55  45    -1.5        33.5     53.5\n 7      55    50  52.5  -9          46       41  \n 8      25    35  30    13.5        38.5     48.5\n 9      30    30  30    13.5        43.5     43.5\n10      45    50  47.5  -4          41       46  \n11      40    60  50    -6.5        33.5     53.5\n12      50    39  44.5  -1          49       38  \n\n\nDie angepassten Werte können wir wieder in einer Grafik darstellen:\n\n\n\n\n\n\n\n\n\nAus der Grafik ist ersichtlich, dass die Gruppenmittelwerte (schwarze Punkte) gleich wie im unabhängigen Design sind. Auch die Rohdaten (rote Punkte) haben sich nicht geändert. Die Konfidenzintervalle sind jedoch aufgrund des abhängigen Designs wesentlich kleiner geworden. In diesem Beispiel überlappen sie jetzt nicht mehr, was auf einen signifikanten Unterschied zwischen den Gruppenmittelwerten schließen lässt."
  },
  {
    "objectID": "12/12.html#der-t-test",
    "href": "12/12.html#der-t-test",
    "title": "12 – Mittelwertvergleich",
    "section": "Der t-Test",
    "text": "Der t-Test\nNach der grafischen Darstellung von Gruppenunterschieden wenden wir uns jetzt der statistischen Analyse zu. Man verwendet den t-Test, um die Mittelwerte zweier Gruppen miteinander zu vergleichen. Hier gibt es zwei Varianten, nämlich einen t-Test für abhängige Gruppen und einen für unabhängige Gruppen. Ersteren nennt man auch gepaarten oder abhängigen t-Test, letzteren nennt man unabhängigen t-Test.\nDie t-Statistik ist wie viele Statistiken aufgebaut. Sie setzt die Varianz, die vom Modell erklärt werden kann, in Beziehung zur Varianz, die nicht vom Modell erklärt werden kann (oder anders formuliert, Effekt geteilt durch Fehler). Im Falle des t-Tests ist das Modell der Unterschied der beiden Mittelwerte minus der erwarteten Differenz, und der Fehler wird durch den Standardfehler der Mittelwertsdifferenz geschätzt:\n\\[t = \\frac{\\text{Beobachtete Differenz} - \\text{Erwartete Differenz}}{\\text{Standardfehler der Differenz}}\\]\nDie beobachtete Differenz bezieht sich auf die beobachtete Differenz zwischen den Mittelwerten. Die erwartete Differenz (unter der Annahme der Nullhypothese) ist in den meisten Fällen gleich Null (wir erwarten keine Gruppenunterschiede). Die Vorgehensweise beim Testen ist also wie folgt:\n\nZwei Stichproben werden erhoben und deren Mittelwerte berechnet.\nWenn beide Stichproben aus derselben Population stammen, sollten die Mittelwerte ungefähr gleich sein. Dies ist die Annahme der Nullhypothese (es gibt keinen Unterschied). Große Differenzen können in seltenen Fällen aber zufällig auftreten.\nWir vergleichen den beobachteten Unterschied mit dem erwarteten Unterschied, und wir verwenden den Standardfehler als Maß für die Variabilität der Stichprobenmittelwerte.\nWenn der beobachtete Unterschied größer als der erwartete ist, kann das zwei Gründe haben:\n\nEs gibt keinen Unterschied und der beobachtete Unterschied ist zufällig aufgetreten, d.h. zumindest eine Stichprobe ist nicht repräsentativ für die Population.\nDie beiden Stichproben kommen aus unterschiedlichen Populationen, d.h. beide Stichproben sind repräsentativ für ihre Population. Das bedeutet, dass es also tatsächlich einen Unterschied gibt!\n\n\nJe größer der Unterschied bzw. die t-Statistik, desto mehr spricht für den zweiten Fall (d.h. für einen tatsächlichen Effekt)."
  },
  {
    "objectID": "12/12.html#der-t-test-als-lineares-modell",
    "href": "12/12.html#der-t-test-als-lineares-modell",
    "title": "12 – Mittelwertvergleich",
    "section": "Der t-Test als lineares Modell",
    "text": "Der t-Test als lineares Modell\nWir haben den t-Test bereits bei der linearen Regression kennengelernt. Hier wird er verwendet, um zu überprüfen, ob Regressionskoeffizienten signifikant unterschiedlich von 0 sind. Wie wir außerdem soeben erfahren haben, wird der t-Test auch angewendet, um Mittelwerte zu vergleichen. Tatsächlich ist es so, dass man Gruppenmittelwerte als lineares Modell anschreiben kann:\n\\[y_i = \\hat{y}_i + \\varepsilon_i\\]\nDie abhängige Variable \\(y\\) ist die Variable, deren Gruppenmittelwerte man vergleichen möchte (z.B. Angst vor Spinnen). Die unabhängige Variable \\(x\\) codiert die Gruppen. Man verwendet dann für \\(\\hat{y}\\) das lineare Modell:\n\\[\\hat{y}_i = b_0 + b_1 \\cdot x_i\\]\nAm besten illustriert man die Funktionsweise mit einem Beispiel. Dazu verwenden wir wieder die Daten über die Angst vor Spinnen. Erstellen wir nun ein lineares Regressionsmodell, welches Anxiety durch Group vorhersagt:\n\nmodel = lm(Anxiety ~ Group, data=spider)\nsummary(model)\n\n\nCall:\nlm(formula = Anxiety ~ Group, data = spider)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -17.0   -8.5    1.5    8.0   18.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   40.000      2.944  13.587 3.53e-12 ***\nGroupreal      7.000      4.163   1.681    0.107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.2 on 22 degrees of freedom\nMultiple R-squared:  0.1139,    Adjusted R-squared:  0.07359 \nF-statistic: 2.827 on 1 and 22 DF,  p-value: 0.1068\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nHier ist anzumerken, dass die kategorische Variable Group von R automatisch mit 0 und 1 codiert wird (und zwar alphabetisch, d.h. die Stufe picture entspricht 0 und die Stufe real entspricht 1). Man spricht hier von Dummy-Coding.\n\n\nWenn man sich nun die Regressionskoeffizienten ansieht, erkennt man, dass der Intercept \\(b_0 = 40\\) dem Mittelwert der Gruppe 0 (picture) entspricht. Die Steigung \\(b_1 = 7\\) entspricht dem Unterschied der Mittelwerte zwischen den beiden Gruppen (\\(47 - 40 = 7\\)). Der t-Test für diesen Koeffizienten testet, ob die Steigung signifikant von Null verschieden ist. Er testet somit also auch automatisch, ob der Unterschied zwischen den Mittelwerten signifikant von Null verschieden ist. Man sieht, dass dieser Test mit \\(p=0.107\\) nicht signifikant ist, d.h. man kann daraus schließen, dass sich die Mittelwerte nicht signifikant voneinander unterscheiden. Dieses Ergebnis bestätigt also die Erkenntnis aus unserer grafischen Darstellung (stark überlappende Konfidenzintervalle).\nGrafisch kann man die Situation wie folgt darstellen:\n\n\n\n\n\n\n\n\n\nDie Regressionsgerade verbindet beide Gruppenmittelwerte und die Steigung beträgt 7; diese ist statistisch nicht signifikant von Null verschieden.\nWir können dies durch Einsetzen der Gruppenmittelwerte in die Gleichung des linearen Modells selbst nachrechnen. Wir beginnen mit der Gruppe 0, also picture. Wir wissen, dass der Mittelwert dieser Gruppe gleich 40 ist.\n\nmean(spider$Anxiety[spider$Group==\"picture\"])\n\n[1] 40\n\n\nWir setzen in die Modellgleichung ein: \\[\\hat{y}_i = b_0 + b_1 \\cdot x_i\\] Für \\(\\hat{y}_i\\) verwenden wir den Gruppenmittelwert der picture-Gruppe, und das zugehörige \\(x_i\\) ist also \\(x_{\\text{Picture}}\\) (codiert mit 0). \\[\\bar{y}_{\\text{picture}} = b_0 + b_1 \\cdot x_{\\text{picture}}\\] \\[40 = b_0 + b_1 \\cdot 0\\] \\[b_0 = 40\\]\nMan sieht also, dass der Intercept \\(b_0\\) dem Gruppenmittelwert der ersten Gruppe (mit Codierung 0) entspricht.\nSetzen wir jetzt die Werte für die Gruppe real ein. Hier beträgt der Gruppenmittelwert 47:\n\nmean(spider$Anxiety[spider$Group==\"real\"])\n\n[1] 47\n\n\n\\[\\bar{y}_{\\text{real}} = b_0 + b_1 \\cdot x_{\\text{real}}\\] \\[47 = 40 + b_1 \\cdot 1\\] \\[b_1 = 7\\]\nDie Steigung der Geraden \\(b_1\\) entspricht also tatsächlich genau dem Unterschied der beiden Mittelwerte."
  },
  {
    "objectID": "12/12.html#annahmen",
    "href": "12/12.html#annahmen",
    "title": "12 – Mittelwertvergleich",
    "section": "Annahmen",
    "text": "Annahmen\nDa der t-Test auf einer lineare Regression basiert, setzt er auch dieselben Annahmen wie diese voraus:\n\nDie Residuen sind normalverteilt (beim abhängigen t-Test sind die Residuen der Differenzen gemeint).\nDie Daten sind intervallskaliert.\nBeim unabhängigen t-Test müssen die Daten in den Gruppen voneinander unabhängig sein.\nBeim unabhängigen t-Test muss Varianzhomogenität gegeben sein. Praktisch ist diese Voraussetzung aber nicht notwendig, da standardmäßig ein verbessertes Verfahren angewendet wird, welches Verletzungen dieser Annahme automatisch korrigiert (Welch t-Test)."
  },
  {
    "objectID": "12/12.html#unabhängiger-t-test-in-r",
    "href": "12/12.html#unabhängiger-t-test-in-r",
    "title": "12 – Mittelwertvergleich",
    "section": "Unabhängiger t-Test in R",
    "text": "Unabhängiger t-Test in R\nDie t-Statistik berechnet sich also durch das Verhältnis erklärter Varianz zu nicht erklärter Varianz. In der Ausgabe von summary(model) (wobei model das mit lm() gerechnete lineare Regressionsmodell ist) kann man diese Größen in den Spalten Estimate bzw. Std. Error ablesen. Beim unabhängigen t-Test vergleicht man so die Mittelwerte beider Bedingungen:\n\\[t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\text{Standardfehler}}\\]\nDie Nullhypothese besagt dass \\(\\mu_1 = \\mu_2\\), daher vereinfacht sich die Gleichung zu:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\text{Standardfehler}}\\]\nDer Standardfehler der Differenz beider Gruppen ist bei gleicher Gruppengröße wie folgt definiert:\n\\[\\text{SE} = \\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}\\]\nDaher lautet die Gleichung für den t-Test:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}}\\]\nWenn die beiden Gruppen unterschiedlich viele Personen enthalten, muss man den Standardfehler über die gepoolte Varianz berechnen:\n\\[s_p^2 = \\frac{(N_1 - 1) s_1^2 + (N_2 - 1) s_2^2}{N_1 + N_2 - 2}\\]\nDaraus ergibt sich dann für den t-Test mit \\(N_1 + N_2 - 2\\) Freiheitsgraden:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_p^2}{N_1} + \\frac{s_p^2}{N_2}}}\\]\nIn R kann man t-Tests mit der Funktion lm() durchführen. Alternativ gibt es aber auch die Funktion t.test(), welche im Hintergrund ebenfalls lm() verwendet, aber eine vertrautere Darstellung der Ergebnisse bietet:\n\n(model = t.test(Anxiety ~ Group, data=spider))\n\n\n    Welch Two Sample t-test\n\ndata:  Anxiety by Group\nt = -1.6813, df = 21.385, p-value = 0.1072\nalternative hypothesis: true difference in means between group picture and group real is not equal to 0\n95 percent confidence interval:\n -15.648641   1.648641\nsample estimates:\nmean in group picture    mean in group real \n                   40                    47 \n\n\nDas erste Argument ist also eine Formel, deren linke Seite die Datenpunkte angibt (die Spalte Anxiety in diesem Beispiel). Die rechte Seite der Formel gibt die Gruppierungsspalte an (also hier Group). Damit man die Spaltennamen direkt verwenden kann, spezifiziert man noch data=spider, damit klar ist, dass diese Spalten im Data Frame spider zu finden sind.\nAusgegeben wird der Wert der t-Statistik, die Freiheitsgrade (standardmäßig korrigiert nach Welch, was die Voraussetzung der Varianzhomogenität überflüssig macht), sowie der p-Wert. Weiters gibt es noch das 95%-Konfidenzintervall für die t-Statistik sowie die Gruppenmittelwerte.\nDie zweite Möglichkeit, die Funktion aufzurufen, verwendet zwei Argumente; hier übergibt man also die Daten der beiden Gruppen in zwei separaten Vektoren:\n\n(model = t.test(spider_w$picture, spider_w$real))\n\n\n    Welch Two Sample t-test\n\ndata:  spider_w$picture and spider_w$real\nt = -1.6813, df = 21.385, p-value = 0.1072\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.648641   1.648641\nsample estimates:\nmean of x mean of y \n       40        47 \n\n\nDas Ergebnis ist aber natürlich dasselbe.\nDie Effektgröße kann man aus dem Wert von \\(t\\) in eine Korrelation \\(r\\) umrechnen:\n\\[r = \\sqrt{\\frac{t^2}{t^2 + \\text{df}}}\\]\nIm Beispiel ist die Effektgröße \\(r\\) also:\n\nt = model$statistic[[1]]\ndf = model$parameter[[1]]\nr = sqrt(t^2 / (t^2 + df))\nround(r, 3)\n\n[1] 0.342"
  },
  {
    "objectID": "12/12.html#abhängiger-t-test-in-r",
    "href": "12/12.html#abhängiger-t-test-in-r",
    "title": "12 – Mittelwertvergleich",
    "section": "Abhängiger t-Test in R",
    "text": "Abhängiger t-Test in R\nDer abhängige (oder gepaarte) t-Test funktioniert ähnlich, verwendet aber die Mittelwerte der einzelnen Differenzen anstelle der Differenz der Mittelwerte, um die individuellen Unterschiede herauszurechnen:\n\\[t = \\frac{\\bar{D} - \\mu_D}{s_D / \\sqrt{N}}\\]\nUnter der Nullhypothese ist \\(\\mu_D = 0\\): \\[t = \\frac{\\bar{D}}{s_D / \\sqrt{N}}\\]\nFür den abhängigen t-Test verwendet man wieder die Funktion t.test() und setzt das Argument paired=TRUE. Hier sollte man die Variante mit zwei Argumenten verwenden (d.h. die Daten sollten im Wide-Format vorliegen):\n\n(model = t.test(spider_w$picture, spider_w$real, paired=TRUE))\n\n\n    Paired t-test\n\ndata:  spider_w$picture and spider_w$real\nt = -2.4725, df = 11, p-value = 0.03098\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -13.2312185  -0.7687815\nsample estimates:\nmean difference \n             -7 \n\n\nDas Ergebnis ist diesmal signifikant mit \\(p=0.03098\\). Dies entspricht unseren Überlegungen mit den unterschiedlich großen Konfidenzintervallen von abhängigen und unabhängigen Versuchsdesigns. Abhängige Designs können also kleinere Unterschiede detektieren.\nDie Effektgröße kann man wieder aus \\(t\\) mit der Formel oben in \\(r\\) umwandeln:\n\nt = model$statistic[[1]]\ndf = model$parameter[[1]]\nr = sqrt(t^2 / (t^2 + df))\nround(r, 3)\n\n[1] 0.598\n\n\nEs handelt sich also in diesem Fall um einen großen Effekt."
  },
  {
    "objectID": "12/12.html#ausblick",
    "href": "12/12.html#ausblick",
    "title": "12 – Mittelwertvergleich",
    "section": "Ausblick",
    "text": "Ausblick\nZusammenfassend haben wir gesehen, dass der Mittelwertsvergleich über den t-Test auf ein lineares Modell zurückgeführt werden kann. Möchte man mehr als zwei Mittelwerte vergleichen, kann man das lineare Modell mit mehreren Prädiktoren verwenden (Dummy-Coding). In der klassischen Varianzanalyse betrachtet man die F-Statistik, welche aber auch Teil der linearen Regression ist (sie misst die Güte des Modells) und dementsprechend auch in der Ausgabe aufscheint. Man kann also auch eine ANOVA als Spezialfall eines linearen Modells sehen. Um die vertrauten ANOVA-Tabellen dennoch auch in R zu erhalten, kann man beispielsweise das Paket afex benutzen.\nBei abhängigen Messungen kann man keine linearen Modelle rechnen, da eine der wichtigsten Voraussetzungen die Unabhängigkeit der Messpunkte ist. Sogenannte lineare gemischte Modelle (engl. linear mixed models) können aber mit diesen Abhängigkeiten umgehen und werden immer häufiger statt den klassischen Messwiederholungs-ANOVAs eingesetzt. Gemischte Modelle sind Verallgemeinerungen von linearen Modellen, oder umgekehrt sind lineare Modelle Spezialfälle von linearen gemischten Modellen. Das Paket lme4 hat sich in R als Standard zur Berechnung dieser Modelle durchgesetzt (bzw. die darauf aufbauenden Pakete lmerTest und afex)."
  },
  {
    "objectID": "12/12.html#übungen",
    "href": "12/12.html#übungen",
    "title": "12 – Mittelwertvergleich",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nDas Paket dplyr beinhaltet den Datensatz starwars (aktivieren Sie das Paket um auf diesen Datensatz zugreifen zu können). Verwenden Sie diese Daten um herauszufinden, ob die weiblichen Charaktere signifikant kleiner sind als die männlichen (Spalte height). Verwenden Sie die Spalte gender um zwischen den Geschlechtern \"feminine\" und \"masculine\" zu unterscheiden. Verwenden Sie für Ihre Analyse ein Signifikanzniveau von 5% und geben Sie die Mittelwerte beider Gruppen an.\nFühren Sie auch einen Vergleich des Gewichtes (Spalte mass) durch (überprüfen Sie die Hypothese, dass männliche Charaktere mehr wiegen als weibliche). Geben Sie auch hier die Mittelwerte beider Gruppen an.\n\n\n\n\n\n\nHinweis\n\n\n\nVerwenden Sie das Argument alternative von t.test(), um einen einseitigen Test durchzuführen.\n\n\n\n\nÜbung 2\nVergleichen Sie im penguins-Datensatz (aus dem Paket palmerpenguins), ob sich die Merkmale bill_length_mm bzw. bill_depth_mm zwischen den Spezies unterscheiden (führen Sie paarweise Vergleiche für jedes Merkmal durch). Berichten Sie die relevanten Statistiken, Signifikanzen und Effektgrößen (Korrelationskoeffizient r) für jeden t-Test.\nDie Funktion pairwise.t.test() rechnet mehrere paarweise Vergleiche. Verwenden Sie diese Funktion (zusätzlich zu den einzeln gerechneten t-Tests von zuvor), um die Vergleiche durchzuführen. Mit dem Argument p.adjust.method können Sie außerdem die p-Werte korrigieren, weil mehrere Tests die Wahrscheinlichkeit eines falsch positiven Ergebnisses (der t-Test sagt “es gibt einen Unterschied” obwohl tatsächlich kein Unterschied besteht) erhöhen.\n\n\nÜbung 3\nLaden Sie den Datensatz sleep (standardmäßig bei R dabei). Unterscheiden sich die beiden Gruppen (group) in der Anzahl an zusätzlichen Schlafstunden (extra)? Wenn ja, wie groß ist dieser Unterschied im Mittel? Geben Sie auch ein 95%-Konfidenzintervall an.\n\n\n\n\n\n\nHinweis\n\n\n\nEntnehmen Sie dem Hilfetext, ob Sie einen abhängigen oder unabhängigen t-Test rechnen müssen."
  },
  {
    "objectID": "04/04.html",
    "href": "04/04.html",
    "title": "4 – Tabellarische Daten",
    "section": "",
    "text": "Vektoren werden in R verwendet, um eindimensionale Daten abzubilden. Häufig sind Daten aber zweidimensional strukturiert, also in Form einer Tabelle. Für tabellarische Daten gibt es in R zwei gängige Datentypen, nämlich Matrizen (Einzahl Matrix) und sogenannte Data Frames. Matrizen können genau wie Vektoren nur Elemente eines einzigen Datentyps enthalten (sie sind also homogene Datentypen). Im Gegensatz dazu können Data Frames Spalten mit unterschiedlichen Datentypen enthalten."
  },
  {
    "objectID": "04/04.html#allgemeines",
    "href": "04/04.html#allgemeines",
    "title": "4 – Tabellarische Daten",
    "section": "",
    "text": "Vektoren werden in R verwendet, um eindimensionale Daten abzubilden. Häufig sind Daten aber zweidimensional strukturiert, also in Form einer Tabelle. Für tabellarische Daten gibt es in R zwei gängige Datentypen, nämlich Matrizen (Einzahl Matrix) und sogenannte Data Frames. Matrizen können genau wie Vektoren nur Elemente eines einzigen Datentyps enthalten (sie sind also homogene Datentypen). Im Gegensatz dazu können Data Frames Spalten mit unterschiedlichen Datentypen enthalten."
  },
  {
    "objectID": "04/04.html#matrizen",
    "href": "04/04.html#matrizen",
    "title": "4 – Tabellarische Daten",
    "section": "Matrizen",
    "text": "Matrizen\n\nZusammenhang mit Vektoren\nMatrizen sind Vektoren mit einem speziellen Dimensionsattribut, welches man mit der Funktion dim() abfragen und setzen kann:\n\n(v = 1:20)  # Vektor\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\ndim(v)\n\nNULL\n\nlength(v)\n\n[1] 20\n\n\nEin “normaler” Vektor hat kein Dimensionsattribut, daher wird hier NULL angezeigt.\nNun kann man für einen Vektor das Dimensionsattribut auf die gewünschte Anzahl an Zeilen und Spalten setzen (das Produkt von Zeilen und Spalten muss mit der Gesamtanzahl an Elementen im Vektor übereinstimmen):\n\ndim(v) = c(4, 5)  # 4 Zeilen, 5 Spalten\n\nNun besitzt v also ein Dimensionsattribut:\n\ndim(v)\n\n[1] 4 5\n\n\nTatsächlich wird v ab jetzt auch als Matrix mit 4 Zeilen und 5 Spalten dargestellt:\n\nv\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\n\nUnd der Typ von v lautet jetzt:\n\nclass(v)\n\n[1] \"matrix\" \"array\" \n\n\nDieses Beispiel soll zeigen, dass sich die zugrundeliegenden Daten (welche nach wie vor als Vektor vorliegen) nicht ändern – sie werden durch das Dimensionsattribut lediglich anders dargestellt bzw. interpretiert.\n\n\n\n\n\n\nHinweis\n\n\n\nMit dem Dimensionsattribut kann man beliebig viele Dimensionen erstellen, nicht nur Matrizen mit zwei Dimensionen. Man spricht dann von Arrays, und eine Matrix ist ein spezielles Array mit zwei Dimensionen. Daher lautet die Ausgabe von class(v) im vorigen Beispiel auch \"matrix\" \"array\".\n\n\n\n\nErstellen von Matrizen\nEine Matrix kann nicht nur aus einem bereits vorhandenen Vektor, sondern auch direkt mit der Funktion matrix() erzeugt werden:\n\n(m = matrix(1:20, nrow=4, ncol=5))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\n\nDas erste Argument sind die Daten (ein Vektor), die die Matrix enthalten soll. Das zweite Argument nrow ist die Anzahl der Zeilen, und das dritte Argument ncol ist die Anzahl der Spalten der Matrix. Die ursprünglichen Daten werden standardmäßig spaltenweise in der Matrix angeordnet. Möchte man die Daten zeilenweise anordnen, kann man das Argument byrow=TRUE setzen:\n\nmatrix(1:20, nrow=4, ncol=5, byrow=TRUE)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\n\n\n\nBenennen von Zeilen und Spalten\nSo, wie man einen Vektor mit benannten Elementen erstellen kann, kann man in einer Matrix Zeilen- und Spaltennamen vergeben:\n\nrownames(m) = c(\"one\", \"two\", \"three\", \"four\")\ncolnames(m) = c(\"A\", \"B\", \"C\", \"D\", \"E\")\nm\n\n      A B  C  D  E\none   1 5  9 13 17\ntwo   2 6 10 14 18\nthree 3 7 11 15 19\nfour  4 8 12 16 20\n\n\n\n\nIndizieren\nDas Herausgreifen einzelner Elemente funktioniert im Prinzip gleich wie bei Vektoren: mit eckigen Klammern. Der Hauptunterschied ist, dass man bei Matrizen zwei Indizes (getrennt mit einem Komma) angibt: der erste Index entspricht den Zeilen und der zweite entspricht den Spalten. Wenn der erste Index weggelassen wird, werden alle Zeilen herausgegriffen. Wenn der zweite Index weggelassen wird, werden alle Spalten herausgegriffen.\nDie folgenden Beispiele illustrieren das Indizieren einer Matrix:\n\nm\n\n      A B  C  D  E\none   1 5  9 13 17\ntwo   2 6 10 14 18\nthree 3 7 11 15 19\nfour  4 8 12 16 20\n\nm[1, 4]  # 1. Zeile, 4. Spalte\n\n[1] 13\n\nm[, 3]  # 3. Spalte\n\n  one   two three  four \n    9    10    11    12 \n\nm[3,]  # 3. Zeile\n\n A  B  C  D  E \n 3  7 11 15 19 \n\nm[c(2, 4),]  # 2. und 4. Zeile\n\n     A B  C  D  E\ntwo  2 6 10 14 18\nfour 4 8 12 16 20\n\nm[c(1, 3), c(1, 2, 5)]  # Zeilen 1 und 3, Spalten 1, 2 und 5\n\n      A B  E\none   1 5 17\nthree 3 7 19\n\nm[, \"C\"]  # Spalte C\n\n  one   two three  four \n    9    10    11    12 \n\nm[\"two\",]  # Zeile \"two\"\n\n A  B  C  D  E \n 2  6 10 14 18 \n\nm[m[, \"A\"] &gt; 2,]  # Zeilen, in denen die Spalte A &gt; 2 ist\n\n      A B  C  D  E\nthree 3 7 11 15 19\nfour  4 8 12 16 20\n\n\nDas letzte Beispiel ist in der Praxis sehr wichtig, da man so Zeilen aus einer Matrix filtern kann. Stellen Sie daher sicher, dass Sie diese Art der Indizierung verstehen – wir werden sie noch öfter brauchen!\n\n\nZwang (Coercion)\nWenn man einer numerischen Matrix z.B. eine neue Spalte vom Typ character hinzufügen möchte, dann funktioniert das nicht wie gewünscht, denn die numerischen Elemente werden automatisch in Zeichenketten umgewandelt (es handelt sich bei einer Matrix ja um einen homogenen Vektor). Wir werden sehen, dass Data Frames besser geeignet sind, wenn man unterschiedliche Spaltentypen benötigt.\n\nsubjects = c(\"Hans\", \"Birgit\", \"Ferdinand\", \"Johanna\")\ncbind(subjects, m)\n\n      subjects    A   B   C    D    E   \none   \"Hans\"      \"1\" \"5\" \"9\"  \"13\" \"17\"\ntwo   \"Birgit\"    \"2\" \"6\" \"10\" \"14\" \"18\"\nthree \"Ferdinand\" \"3\" \"7\" \"11\" \"15\" \"19\"\nfour  \"Johanna\"   \"4\" \"8\" \"12\" \"16\" \"20\"\n\n\nAnhand dieses Beispiels sieht man auch eine weitere Möglichkeit, wie man Matrizen erstellen bzw. erweitern kann. Die Funktion cbind() hängt Vektoren (oder Matrizen) spaltenweise zusammen, während analog dazu die Funktion rbind() Objekte zeilenweise zusammenfügt.\n\n\nRechnen mit Matrizen\nGenau wie bei Vektoren werden Rechenoperationen mit Matrizen elementweise durchgeführt. Zusätzlich gibt es noch praktische Funktionen, mit denen man die Zeilen- bzw. Spaltensummen einer Matrix berechnen kann, nämlich rowSums() und colSums():\n\nrowSums(m)\n\n  one   two three  four \n   45    50    55    60 \n\ncolSums(m)\n\n A  B  C  D  E \n10 26 42 58 74 \n\n\nAnalog dazu gibt es auch noch rowMeans() bzw. colMeans() zur Berechnung der Zeilen- bzw. Spaltenmittelwerte.\nEine Matrix ist, wie man anhand der obigen Beispiele erkennt, eigentlich nur für rein numerische Daten geeignet. Oft will man aber auch nicht-numerische Spalten wie z.B. Namen oder Gruppenzugehörigkeiten hinzufügen – dies funktioniert wie oben gezeigt mit Matrizen praktisch nicht, weil dann alle Elemente in den Typ character konvertiert werden."
  },
  {
    "objectID": "04/04.html#data-frames",
    "href": "04/04.html#data-frames",
    "title": "4 – Tabellarische Daten",
    "section": "Data Frames",
    "text": "Data Frames\nData Frames sind ebenso wie Matrizen zweidimensionale Datenstrukturen (sie bestehen aus Zeilen und Spalten). Im Gegensatz zu Matrizen können Spalten aber unterschiedliche Datentypen haben (z.B. kann eine Spalte numerisch sein, eine andere Spalte kann Zeichenketten beinhalten, und so weiter). Innerhalb einer Spalte müssen aber alle Werte homogen sein. Man kann sich die einzelnen Spalten in einem Data Frame daher als Vektoren vorstellen.\n\n\n\n\n\n\nHinweis\n\n\n\nTatsächlich ist ein Data Frame eine Liste aus Vektoren gleicher Länge, welche den einzelnen Spalten entsprechen (den allgemeinen Datentyp list, welcher hier im Hintergrund verwendet wird, werden wir allerdings nicht genauer kennenlernen).\n\n\n\nErstellen von Data Frames\nMit der Funktion data.frame() kann man ein Data Frame spaltenweise aus einzelnen Vektoren erzeugen, indem man die einzelnen Spalten als (benannte) Argumente übergibt:\n\ndata.frame(x=1:5, id=c(\"X\", \"c1\", \"V\", \"RR\", \"7G\"), value=c(12, 18, 19, 3, 8))\n\n  x id value\n1 1  X    12\n2 2 c1    18\n3 3  V    19\n4 4 RR     3\n5 5 7G     8\n\n\nFür die Spaltennamen werden automatisch die jeweiligen Argumentnamen verwendet, so entstehen die Spaltennamen x, id und value.\nAlternativ kann man die Funktion data.frame() ähnlich wie cbind() verwenden, um Vektoren und/oder zweidimensionale Objekte (wie Matrizen) spaltenweise aneinanderzuhängen:\n\n(df = data.frame(subjects, m))\n\n       subjects A B  C  D  E\none        Hans 1 5  9 13 17\ntwo      Birgit 2 6 10 14 18\nthree Ferdinand 3 7 11 15 19\nfour    Johanna 4 8 12 16 20\n\n\nWie bei Matrizen kann man mit der Funktion colnames() die Spaltennamen lesen bzw. setzen:\n\ncolnames(df)\n\n[1] \"subjects\" \"A\"        \"B\"        \"C\"        \"D\"        \"E\"       \n\ncolnames(df) = c(\"patient\", \"age\", \"weight\", \"bp\", \"rating\", \"test\")\ndf\n\n        patient age weight bp rating test\none        Hans   1      5  9     13   17\ntwo      Birgit   2      6 10     14   18\nthree Ferdinand   3      7 11     15   19\nfour    Johanna   4      8 12     16   20\n\n\nAuch rownames() funktioniert, aber in der Praxis werden fast immer nur Spaltennamen gesetzt. Die Zeilennamen verändert man am besten nicht, diese sind dann beginnend mit 1 aufsteigend durchnummeriert.\n\n\nAnzeigen von Data Frames\nErzeugen wir für die folgenden Beispiele nochmals ein kleines Data Frame namens df (beachten Sie, dass wir Zeilenumbrüche zwecks besserer Lesbarkeit einfügen können):\n\ndf = data.frame(\n    patient=c(\"Hans\", \"Birgit\", \"Ferdinand\", \"Johanna\"),\n    age=c(34, 17, 26, 44),\n    weight=c(77, 60, 83, 64),\n    height=c(175, 169, 185, 170)\n)\n\nEine schnelle Übersicht über ein Data Frame bekommt man mit den Funktionen str(), head() und tail().\nDie Funktion str() stellt die Struktur eines Objektes knapp zusammengefasst dar:\n\nstr(df)\n\n'data.frame':   4 obs. of  4 variables:\n $ patient: chr  \"Hans\" \"Birgit\" \"Ferdinand\" \"Johanna\"\n $ age    : num  34 17 26 44\n $ weight : num  77 60 83 64\n $ height : num  175 169 185 170\n\n\nDie Funktion head() gibt die ersten sechs Zeilen am Bildschirm aus, während tail() die letzten sechs Zeilen ausgibt. Es gibt mit dem Argument n auch die Möglichkeit, die Anzahl der angezeigten Zeilen anzupassen.\n\nl = data.frame(a=rnorm(5000), b=rpois(5000, 2), x=rep(letters, length.out=5000))\ndim(l)\n\n[1] 5000    3\n\nhead(l)\n\n           a b x\n1 -0.4894597 2 a\n2  1.2635565 2 b\n3  0.9322108 1 c\n4  0.8847193 5 d\n5 -2.4860697 1 e\n6  0.3363164 2 f\n\ntail(l, n=4)\n\n                a b x\n4997  0.517448620 2 e\n4998 -0.871455228 5 f\n4999 -0.008058434 2 g\n5000 -0.556363855 4 h\n\n\n\n\nIndizieren\nAuf einzelne Spalten eines Data Frames kann man mit $ gefolgt vom Spaltennamen zugreifen (das Ergebnis ist ein Vektor):\n\ndf$patient\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\ndf$height\n\n[1] 175 169 185 170\n\n\nDiese Schreibweise kann man auch anwenden, wenn man dem Data Frame eine neue Spalte hinzufügen will. Dazu gibt man einen Spaltennamen an, der noch nicht vorhanden ist, und weist diesem einen Vektor mit der entsprechenden Länge zu:\n\ndf$value = c(\"yes\", \"no\", \"no\", \"yes\")\ndf\n\n    patient age weight height value\n1      Hans  34     77    175   yes\n2    Birgit  17     60    169    no\n3 Ferdinand  26     83    185    no\n4   Johanna  44     64    170   yes\n\n\nAlternativ kann man wie bei Matrizen auch rbind() bzw. cbind() zum Hinzufügen neuer Zeilen bzw. Spalten benutzen.\nEine Spalte kann man aus einem Data Frame entfernen, indem man dieser den Wert NULL zuweist:\n\ndf$value = NULL\ndf\n\n    patient age weight height\n1      Hans  34     77    175\n2    Birgit  17     60    169\n3 Ferdinand  26     83    185\n4   Johanna  44     64    170\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDa ein Data Frame eigentlich eine Liste ist, kann man einzelne Elemente der Liste (welche den Spalten entsprechen), auch mit folgender Syntax herausgreifen:\n\ndf[[\"patient\"]]\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\ndf[[\"height\"]]\n\n[1] 175 169 185 170\n\n\nDies ist also eine Alternative zur $-Schreibweise. Diese hat die gefährliche Eigenschaft, dass man Spaltennamen nicht komplett angeben muss, es reichen die ersten paar Buchstaben:\n\ndf$pat  # wird automatisch auf df$patient ergänzt\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\n\nBei den doppelten eckigen Klammern ist das nicht der Fall:\n\ndf[[\"pat\"]]\n\nNULL\n\n\nUm Fehler zu vermeiden, sollte man daher generell die Schreibweise mit den doppelten eckigen Klammern bevorzugen.\n\n\nZeilen und Spalten kann man auch per “normaler” Indizierung mit eckigen Klammern herausgreifen. Dies funktioniert wie bei Matrizen, man benötigt zwei Indizes, welche mit einem Komma voneinander getrennt sind. Die erste Zahl in den eckigen Klammern steht für die Zeile, und die zweite Zahl steht für die Spalte. Wenn ein Index weggelassen wird, werden alle Zeilen bzw. Spalten herausgegriffen:\n\ndf[1,]\n\n  patient age weight height\n1    Hans  34     77    175\n\ndf[2:3,]\n\n    patient age weight height\n2    Birgit  17     60    169\n3 Ferdinand  26     83    185\n\n\nSpalten können somit mit dem zweiten Index indiziert werden:\n\ndf[, 1]\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\ndf[, 4]\n\n[1] 175 169 185 170\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nDas Komma darf nicht weggelassen werden, auch wenn man einen Index weglässt!\n\n\nSpalten (oder auch Zeilen) kann man anstelle ihrer Indizes auch mit ihren Namen ansprechen:\n\ndf[, \"patient\"]\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\ndf[, \"height\"]\n\n[1] 175 169 185 170\n\n\n\n\n\n\n\n\nTipp\n\n\n\nSpalten kann man, wie bereits erwähnt, auch mit den doppelten eckigen Klammern herausgreifen. Hier kann man nicht nur den Spaltennamen, sondern auch die Spaltenposition verwenden:\n\ndf[[\"patient\"]]\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\ndf[[1]]\n\n[1] \"Hans\"      \"Birgit\"    \"Ferdinand\" \"Johanna\"  \n\n\n\n\nAußerdem kann auch gezielt ein bestimmter Bereich herausgegriffen werden, wenn man sowohl Zeilen- als auch Spaltenbereiche angibt:\n\ndf[1:2, c(1, 3:4)]\n\n  patient weight height\n1    Hans     77    175\n2  Birgit     60    169"
  },
  {
    "objectID": "04/04.html#tibbles",
    "href": "04/04.html#tibbles",
    "title": "4 – Tabellarische Daten",
    "section": "Tibbles",
    "text": "Tibbles\nMit den Bordmitteln von R (d.h. mit den eingebauten Datentypen wie Data Frames sowie dazugehörigen Funktionen) kann man hervorragend arbeiten. Manchmal sind diese Konstrukte aber etwas umständlich zu verwenden, und daher bietet sich die populäre Paketsammlung namens Tidyverse an, diverse Dinge zu modernisieren bzw. zu vereinfachen. Man installiert alle notwendigen Pakete aus dem Tidyverse mit dem Paket tidyverse. Darin enthalten ist das Paket tibble, welches eine moderne Alternative zu Data Frames darstellt. Insbesondere ist die Konvertierung, Erstellung und Indizierung von Tibbles nachvollziehbarer, und die Darstellung von Tibbles am Bildschirm ist wesentlich übersichtlicher. Überall wo man Data Frames verwenden kann, kann man auch Tibbles benutzen.\nBevor man Tibbles verwenden kann, muss man entweder tibble oder tidyverse aktivieren. Letzteres aktiviert gleich alle Pakete aus dem Tidyverse.\n\nlibrary(tibble)\n\nMit der Funktion tibble() kann man nun ein neues Tibble erstellen, ganz analog zur Funktion data.frame():\n\n(t = tibble(\n    subjects=c(\"Hans\", \"Birgit\", \"Ferdinand\", \"Johanna\"),\n    A=1:4,\n    B=5:8,\n    C=9:12,\n    D=13:16,\n    E=17:20\n))\n\n# A tibble: 4 × 6\n  subjects      A     B     C     D     E\n  &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Hans          1     5     9    13    17\n2 Birgit        2     6    10    14    18\n3 Ferdinand     3     7    11    15    19\n4 Johanna       4     8    12    16    20\n\n\nWenn man ein Tibble am Bildschirm ausgibt, werden für jede Spalte automatisch die Datentypen angeführt.\n\n\n\n\n\n\nWichtig\n\n\n\nIn diesem Beispiel wird für die Spalten A bis E der Typ int angezeigt, der für integer (Ganzzahlen) steht. Dezimalzahlen erscheinen mit dem Typ dbl (für double), während herkömmliche Data Frames dafür die Bezeichnung num verwenden. Entscheidend für uns ist lediglich, dass die Typen int, dbl und num allesamt numerische Datentypen darstellen.\n\n\nBei längeren Tabellen zeigt sich ein weiterer Vorteil von Tibbles: gibt man lange Tibbles am Bildschirm aus, wird die Darstellung automatisch so angepasst, dass nicht alle Zeilen/Spalten ausgegeben werden, sondern nur so viel wie möglich, um noch einen guten Überblick über die Daten zu gewährleisten. Dies sieht man beispielsweise am Datensatz CO2, welcher als Data Frame in R vorhanden ist:\n\nCO2\n\n   Plant        Type  Treatment conc uptake\n1    Qn1      Quebec nonchilled   95   16.0\n2    Qn1      Quebec nonchilled  175   30.4\n3    Qn1      Quebec nonchilled  250   34.8\n4    Qn1      Quebec nonchilled  350   37.2\n5    Qn1      Quebec nonchilled  500   35.3\n6    Qn1      Quebec nonchilled  675   39.2\n7    Qn1      Quebec nonchilled 1000   39.7\n8    Qn2      Quebec nonchilled   95   13.6\n9    Qn2      Quebec nonchilled  175   27.3\n10   Qn2      Quebec nonchilled  250   37.1\n11   Qn2      Quebec nonchilled  350   41.8\n12   Qn2      Quebec nonchilled  500   40.6\n13   Qn2      Quebec nonchilled  675   41.4\n14   Qn2      Quebec nonchilled 1000   44.3\n15   Qn3      Quebec nonchilled   95   16.2\n16   Qn3      Quebec nonchilled  175   32.4\n17   Qn3      Quebec nonchilled  250   40.3\n18   Qn3      Quebec nonchilled  350   42.1\n19   Qn3      Quebec nonchilled  500   42.9\n20   Qn3      Quebec nonchilled  675   43.9\n21   Qn3      Quebec nonchilled 1000   45.5\n22   Qc1      Quebec    chilled   95   14.2\n23   Qc1      Quebec    chilled  175   24.1\n24   Qc1      Quebec    chilled  250   30.3\n25   Qc1      Quebec    chilled  350   34.6\n26   Qc1      Quebec    chilled  500   32.5\n27   Qc1      Quebec    chilled  675   35.4\n28   Qc1      Quebec    chilled 1000   38.7\n29   Qc2      Quebec    chilled   95    9.3\n30   Qc2      Quebec    chilled  175   27.3\n31   Qc2      Quebec    chilled  250   35.0\n32   Qc2      Quebec    chilled  350   38.8\n33   Qc2      Quebec    chilled  500   38.6\n34   Qc2      Quebec    chilled  675   37.5\n35   Qc2      Quebec    chilled 1000   42.4\n36   Qc3      Quebec    chilled   95   15.1\n37   Qc3      Quebec    chilled  175   21.0\n38   Qc3      Quebec    chilled  250   38.1\n39   Qc3      Quebec    chilled  350   34.0\n40   Qc3      Quebec    chilled  500   38.9\n41   Qc3      Quebec    chilled  675   39.6\n42   Qc3      Quebec    chilled 1000   41.4\n43   Mn1 Mississippi nonchilled   95   10.6\n44   Mn1 Mississippi nonchilled  175   19.2\n45   Mn1 Mississippi nonchilled  250   26.2\n46   Mn1 Mississippi nonchilled  350   30.0\n47   Mn1 Mississippi nonchilled  500   30.9\n48   Mn1 Mississippi nonchilled  675   32.4\n49   Mn1 Mississippi nonchilled 1000   35.5\n50   Mn2 Mississippi nonchilled   95   12.0\n51   Mn2 Mississippi nonchilled  175   22.0\n52   Mn2 Mississippi nonchilled  250   30.6\n53   Mn2 Mississippi nonchilled  350   31.8\n54   Mn2 Mississippi nonchilled  500   32.4\n55   Mn2 Mississippi nonchilled  675   31.1\n56   Mn2 Mississippi nonchilled 1000   31.5\n57   Mn3 Mississippi nonchilled   95   11.3\n58   Mn3 Mississippi nonchilled  175   19.4\n59   Mn3 Mississippi nonchilled  250   25.8\n60   Mn3 Mississippi nonchilled  350   27.9\n61   Mn3 Mississippi nonchilled  500   28.5\n62   Mn3 Mississippi nonchilled  675   28.1\n63   Mn3 Mississippi nonchilled 1000   27.8\n64   Mc1 Mississippi    chilled   95   10.5\n65   Mc1 Mississippi    chilled  175   14.9\n66   Mc1 Mississippi    chilled  250   18.1\n67   Mc1 Mississippi    chilled  350   18.9\n68   Mc1 Mississippi    chilled  500   19.5\n69   Mc1 Mississippi    chilled  675   22.2\n70   Mc1 Mississippi    chilled 1000   21.9\n71   Mc2 Mississippi    chilled   95    7.7\n72   Mc2 Mississippi    chilled  175   11.4\n73   Mc2 Mississippi    chilled  250   12.3\n74   Mc2 Mississippi    chilled  350   13.0\n75   Mc2 Mississippi    chilled  500   12.5\n76   Mc2 Mississippi    chilled  675   13.7\n77   Mc2 Mississippi    chilled 1000   14.4\n78   Mc3 Mississippi    chilled   95   10.6\n79   Mc3 Mississippi    chilled  175   18.0\n80   Mc3 Mississippi    chilled  250   17.9\n81   Mc3 Mississippi    chilled  350   17.9\n82   Mc3 Mississippi    chilled  500   17.9\n83   Mc3 Mississippi    chilled  675   18.9\n84   Mc3 Mississippi    chilled 1000   19.9\n\n\nDie Darstellung von allen 84 Zeilen ist wenig übersichtlich, deswegen lässt man sich hier am besten eine Zusammenfassung mittels str(), head() oder tail() ausgeben. Bei Tibbles ist das nicht notwendig, wie wir gleich sehen werden. Die Funktion as_tibble() konvertiert ein existierendes Data Frame in ein Tibble:\n\n(co2_tibble = as_tibble(CO2))\n\n# A tibble: 84 × 5\n   Plant Type   Treatment   conc uptake\n   &lt;ord&gt; &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 Qn1   Quebec nonchilled    95   16  \n 2 Qn1   Quebec nonchilled   175   30.4\n 3 Qn1   Quebec nonchilled   250   34.8\n 4 Qn1   Quebec nonchilled   350   37.2\n 5 Qn1   Quebec nonchilled   500   35.3\n 6 Qn1   Quebec nonchilled   675   39.2\n 7 Qn1   Quebec nonchilled  1000   39.7\n 8 Qn2   Quebec nonchilled    95   13.6\n 9 Qn2   Quebec nonchilled   175   27.3\n10 Qn2   Quebec nonchilled   250   37.1\n# ℹ 74 more rows\n\n\nMan erkennt, dass bei einem Tibble nur die ersten 10 Zeilen dargestellt werden, was wesentlich übersichtlicher ist. Möchte man dennoch alle Zeilen sehen, kann man entweder print() oder View() verwenden (welche auch mit Data Frames funktionieren):\nprint(co2_tibble, n=Inf)  # n=Inf zeigt alle Zeilen an\nView(co2_tibble)  # öffnet eine Tabellenansicht in RStudio\n\n\n\n\n\n\nTipp\n\n\n\nDie Funktion str() liefert für Tibbles oft eine sehr unübersichtliche Zusammenfassung. Stattdessen kann man glimpse() aus dem Paket tibble verwenden (dies funktioniert sowohl für Data Frames als auch Tibbles):\n\nglimpse(CO2)\n\nRows: 84\nColumns: 5\n$ Plant     &lt;ord&gt; Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn2, Qn2, Qn2, Qn2, Qn2, Qn2, Qn2, Qn3, Qn3, Qn3, Qn3, Qn3, Qn3, …\n$ Type      &lt;fct&gt; Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Queb…\n$ Treatment &lt;fct&gt; nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, nonc…\n$ conc      &lt;dbl&gt; 95, 175, 250, 350, 500, 675, 1000, 95, 175, 250, 350, 500, 675, 1000, 95, 175, 250, 350, 500, 675, 1…\n$ uptake    &lt;dbl&gt; 16.0, 30.4, 34.8, 37.2, 35.3, 39.2, 39.7, 13.6, 27.3, 37.1, 41.8, 40.6, 41.4, 44.3, 16.2, 32.4, 40.3…"
  },
  {
    "objectID": "04/04.html#übungen",
    "href": "04/04.html#übungen",
    "title": "4 – Tabellarische Daten",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nErstellen Sie einen Vektor u mit den geraden Zahlen von 98 bis 50 (in absteigender Reihenfolge) und einen Vektor v mit den geraden Zahlen von 0 bis 48. Wandeln Sie dann beide Vektoren in Matrizen mit jeweils 5 Zeilen um (verwenden Sie dazu dim()). Fügen Sie anschließend u und v (in dieser Reihenfolge) spaltenweise zusammen und speichern Sie das Ergebnis in der Variablen r ab. Geben Sie dann r am Bildschirm aus. Welchen Typ hat das Objekt r?\n\n\nÜbung 2\nBeantworten Sie folgende Fragen zur Matrix r aus Übung 1 mit Hilfe von geeigneten R-Befehlen:\n\nWie viele Zeilen und Spalten hat r?\nWie viele Elemente beinhaltet r insgesamt?\nWie lautet das Element in der 4. Zeile und 6. Spalte?\nWie lauten die Zeilenmittelwerte bzw. die Spaltenmittelwerte?\nWie lautet der Mittelwert der Elemente in den Zeilen 3–5 und Spalten 1–2 (also der Mittelwert von diesen sechs Werten)?\n\n\n\nÜbung 3\nErstellen Sie ein Data Frame (oder Tibble) df mit 10 Zeilen und 3 Spalten wie folgt:\n\nDie erste Spalte soll name heißen und die Werte Ben, Emma, Luis, Mia, Paul, Hanna, Lukas, Sophia, Jonas und Emilia beinhalten.\nDie zweite Spalte gender soll das Geschlecht der Personen beinhalten, d.h. entweder den Buchstaben m oder f.\nDie dritte Spalte value soll 10 beliebige Zahlen zwischen 1 und 100 beinhalten.\n\nGeben Sie abschließend df am Bildschirm aus. Welche Datentypen haben die drei Spalten?\n\n\nÜbung 4\nErstellen Sie ein neues Data Frame df_f, welches die Zeilen aller weiblichen Personen aus df (aus Übung 3) enthält, und geben Sie dieses am Bildschirm aus. Verwenden Sie für die Lösung einen logischen Indexvektor, damit dies auch für beliebig lange Daten (bei denen sich männliche und weibliche Personen nicht zeilenweise abwechseln) funktioniert!\nGeben Sie außerdem mindestens vier Möglichkeiten an, wie man auf die erste Spalte zugreifen kann!\n\n\nÜbung 5\nErstellen Sie aus dem in R vorhandenen Data Frame mtcars ein neues Data Frame namens mtcars1, welches nur aus jenen Zeilen besteht in denen die Spalte mpg Werte größer als 25 aufweist. Aus wie vielen Zeilen bzw. Spalten bestehen mtcars bzw. mtcars1?\n\n\nÜbung 6\nEs gibt einen wesentlichen Unterschied zwischen Data Frames und Tibbles beim Indizieren mit normalen eckigen Klammern. Versuchen Sie, diesen Unterschied zu finden, in dem Sie sich den in R verfügbaren Datensatz airquality ansehen.\nErstellen Sie zunächst ein Tibble namens air aus dem Data Frame airquality. Greifen Sie dann aus beiden Objekten die erste Spalte \"Ozone\" heraus, und zwar mit folgenden Varianten:\n\n[, 1]\n[, \"Ozone\"]\n$Ozone\n[[\"Ozone\"]]\n\nKönnen Sie aus den Ergebnissen eine Regel ableiten, wie das Indizieren bei Data Frames bzw. Tibbles funktioniert und wo die Unterschiede bestehen? Welche Variante ist konsistenter?"
  },
  {
    "objectID": "06/06.html",
    "href": "06/06.html",
    "title": "6 – Deskriptive Statistiken",
    "section": "",
    "text": "Nachdem man eine Datei korrekt importiert hat (d.h. wenn die Daten in einem Data Frame bzw. Tibble vorhanden sind und alle Spalten den passenden Datentyp haben), kann man mit der statistischen Analyse beginnen. Der erste Schritt dabei ist meist, sich mit deskriptiven Statistiken eine Beschreibung der vorhandenen Daten zu erzeugen.\nAls Beispiel dazu importieren wir die Datei lecturer.dat aus der letzten Übung:\n\nlibrary(readr)\n(df = read_tsv(\"lecturer.dat\"))\n\n# A tibble: 10 × 7\n   name   birth_date   job friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977       1       5      10  20000       10\n 2 Martin 5/24/1969      1       2      15  40000       17\n 3 Andy   6/21/1973      1       0      20  35000       14\n 4 Paul   7/16/1970      1       4       5  22000       13\n 5 Graham 10/10/1949     1       1      30  50000       21\n 6 Carina 11/5/1983      2      10      25   5000        7\n 7 Karina 10/8/1987      2      12      20    100       13\n 8 Doug   1/23/1989      2      15      16   3000        9\n 9 Mark   5/20/1973      2      12      17  10000       14\n10 Zoe    11/12/1984     2      17      18     10       13\n\n\nDanach führen wir wieder die Konvertierungen für die Spalten birth_date und job durch:\n\ndf$birth_date = as.Date(df$birth_date, format=\"%m/%d/%Y\")\ndf$job = factor(df$job, levels=c(1, 2), labels=c(\"Lecturer\", \"Student\"))\ndf\n\n# A tibble: 10 × 7\n   name   birth_date job      friends alcohol income neurotic\n   &lt;chr&gt;  &lt;date&gt;     &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    1977-07-03 Lecturer       5      10  20000       10\n 2 Martin 1969-05-24 Lecturer       2      15  40000       17\n 3 Andy   1973-06-21 Lecturer       0      20  35000       14\n 4 Paul   1970-07-16 Lecturer       4       5  22000       13\n 5 Graham 1949-10-10 Lecturer       1      30  50000       21\n 6 Carina 1983-11-05 Student       10      25   5000        7\n 7 Karina 1987-10-08 Student       12      20    100       13\n 8 Doug   1989-01-23 Student       15      16   3000        9\n 9 Mark   1973-05-20 Student       12      17  10000       14\n10 Zoe    1984-11-12 Student       17      18     10       13\n\n\nDie name-Spalte brauchen wir für unsere nachfolgenden Betrachtungen nicht mehr, deswegen entfernen wir sie:\n\ndf$name = NULL\n\nEs gibt in R eine Reihe an Funktionen, welche zusammenfassende Statistiken einer Variablen (bzw. eines Vektors) berechnen. Nützliche Funktionen sind z.B. mean(), sd(), var(), min(), max(), median(), range() und quantile(). Den Mittelwert eines Vektors (und damit auch einer Spalte von df) kann man also wie folgt berechnen:\n\nmean(df$friends)\n\n[1] 7.8\n\n\nVor allem bei Faktoren ist es interessant zu wissen, wie viele Stufen (unterschiedliche Werte) diese beinhalten. Auch bei numerischen oder Text-Vektoren mit mehrfach vorkommenden Werten ist es manchmal praktisch, die unterschiedlichen Werte herauszufinden. Dafür gibt es die Funktion unique(), welche die einzigartigen Elemente eines Vektors zurückgibt:\n\nunique(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2))\n\n[1] 1 2\n\n\n\nunique(df$friends)\n\n[1]  5  2  0  4  1 10 12 15 17\n\nunique(df$job)\n\n[1] Lecturer Student \nLevels: Lecturer Student\n\n\nAll diese Berechnungen müsste man nun für jede interessierende Spalte wiederholen (weil diese nur mit Vektoren funktionieren), was relativ mühsam wäre. Deswegen gibt es die Funktion sapply(), welche eine Funktion auf jede Spalte eines Data Frames einzeln anwendet. Möchte man also den Mittelwert für jede numerische Spalte von df berechnen, kann man dies so tun:\n\nsapply(df[, -c(1, 2)], mean)\n\n friends  alcohol   income neurotic \n     7.8     17.6  18511.0     13.1 \n\n\n\n\n\n\n\n\nTipp\n\n\n\nFür die Berechnung der Spaltenmittelwerte kann man auch die uns bereits bekannte Funktion colMeans() verwenden.\n\n\nBeachten Sie, dass df[, -c(1, 2)] alle Spalten aus df ausgenommen der ersten und zweiten bedeutet. So kann man jede beliebige Funktion auf einzelne Spalten anwenden, z.B. die Standardabweichung:\n\nsapply(df[, -c(1, 2)], sd)\n\n     friends      alcohol       income     neurotic \n    6.142746     7.042727 18001.354548     3.984693 \n\n\nEs gibt aber auch spezielle Funktionen, welche direkt mehrere statistische Kenngrößen für alle Spalten eines Data Frames berechnen. Im Folgenden gehen wir näher auf drei dieser Funktionen ein, nämlich summary(), describe() und stat.desc(). Nur summary() ist standardmäßig bei R dabei, die anderen zwei Funktionen erfordern die Installation von zusätzlichen Paketen.\n\n\nDie Funktion summary() liefert eine geeignete Zusammenfassung für jede Spalte eines Data Frames (Tibbles). Numerische Spalten sowie Datumsspalten werden mit sechs Werten beschrieben:\n\nMinimum\nErstes Quartil (25%-Perzentil)\nMedian (50%-Perzentil)\nDrittes Quartil (75%-Perzentil)\nMaximum\nArithmetischer Mittelwert\n\nFür Faktoren werden die Stufen sowie die Anzahl an Fällen pro Stufe aufgelistet.\n\nsummary(df)\n\n   birth_date               job       friends        alcohol          income         neurotic    \n Min.   :1949-10-10   Lecturer:5   Min.   : 0.0   Min.   : 5.00   Min.   :   10   Min.   : 7.00  \n 1st Qu.:1971-04-01   Student :5   1st Qu.: 2.5   1st Qu.:15.25   1st Qu.: 3500   1st Qu.:10.75  \n Median :1975-06-27                Median : 7.5   Median :17.50   Median :15000   Median :13.00  \n Mean   :1975-12-17                Mean   : 7.8   Mean   :17.60   Mean   :18511   Mean   :13.10  \n 3rd Qu.:1984-08-10                3rd Qu.:12.0   3rd Qu.:20.00   3rd Qu.:31750   3rd Qu.:14.00  \n Max.   :1989-01-23                Max.   :17.0   Max.   :30.00   Max.   :50000   Max.   :21.00  \n\n\nAuch bei anderen Objekten als Data Frames liefert summary() oft eine sinnvolle kurze Beschreibung.\n\n\n\nEine weitere Möglichkeit, noch mehr statistische Kenngrößen für numerische Spalten auszugeben, bietet die Funktion describe() aus dem psych-Paket. Nicht-numerische Spalten werden hier nicht vernünftig zusammengefasst, deshalb sollte man der Funktion nur numerische Spalten übergeben.\n\nlibrary(psych)\ndescribe(df[, 3:6])\n\n         vars  n    mean       sd  median  trimmed      mad min   max range  skew kurtosis      se\nfriends     1 10     7.8     6.14     7.5     7.62     7.41   0    17    17  0.11    -1.75    1.94\nalcohol     2 10    17.6     7.04    17.5    17.62     3.71   5    30    25 -0.03    -0.75    2.23\nincome      3 10 18511.0 18001.35 15000.0 16887.50 19940.97  10 50000 49990  0.45    -1.48 5692.53\nneurotic    4 10    13.1     3.98    13.0    12.88     2.97   7    21    14  0.36    -0.66    1.26\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nEigentlich kann man Funktionen aus Paketen auch ohne Aktivierung verwenden. Dazu muss man den Paketnamen gefolgt von :: voranstellen. Im vorigen Beispiel könnte man also auf library(psych) verzichten und die Funktion mit psych::describe() trotzdem verwenden.\n\n\nMan kann diese Funktion auch auf einzelne Gruppen separat anwenden. Im Beispiel könnte man dies getrennt für jede Stufe von df$job tun. Dazu verwendet man die verwandte Funktion describeBy():\n\ndescribeBy(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], df$job)\n\n\n Descriptive statistics by group \ngroup: Lecturer\n         vars n    mean       sd median trimmed      mad   min   max range skew kurtosis      se\nfriends     1 5     2.4     2.07      2     2.4     2.97     0     5     5 0.11    -2.03    0.93\nalcohol     2 5    16.0     9.62     15    16.0     7.41     5    30    25 0.28    -1.72    4.30\nincome      3 5 33400.0 12561.85  35000 33400.0 19273.80 20000 50000 30000 0.10    -1.98 5617.83\nneurotic    4 5    15.0     4.18     14    15.0     4.45    10    21    11 0.25    -1.72    1.87\n------------------------------------------------------------------------------------------ \ngroup: Student\n         vars n   mean      sd median trimmed     mad min   max range  skew kurtosis      se\nfriends     1 5   13.2    2.77     12    13.2    2.97  10    17     7  0.23    -1.89    1.24\nalcohol     2 5   19.2    3.56     18    19.2    2.97  16    25     9  0.66    -1.43    1.59\nincome      3 5 3622.0 4135.69   3000  3622.0 4299.54  10 10000  9990  0.48    -1.64 1849.54\nneurotic    4 5   11.2    3.03     13    11.2    1.48   7    14     7 -0.37    -2.01    1.36\n\n\nDas erste Argument ist das zu beschreibende Data Frame, und das zweite Argument ist die Spalte, nach der gruppiert werden soll (üblicherweise ein Faktor).\n\n\n\nDas Paket pastecs beinhaltet die Funktion stat.desc() zur Beschreibung von Daten. Mit der Funktion round() sollte man hier außerdem einstellen, wie viele Kommastellen ausgegeben werden sollen, da die Ausgabe der Funktion sonst relativ unübersichtlich ist. Wenn das Argument norm=TRUE gesetzt wird, werden für alle Spalten Tests auf Normalverteilung durchgeführt.\n\nlibrary(pastecs)\nround(\n    stat.desc(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], norm=TRUE),\n    digits=2\n)\n\n             friends alcohol       income neurotic\nnbr.val        10.00   10.00        10.00    10.00\nnbr.null        1.00    0.00         0.00     0.00\nnbr.na          0.00    0.00         0.00     0.00\nmin             0.00    5.00        10.00     7.00\nmax            17.00   30.00     50000.00    21.00\nrange          17.00   25.00     49990.00    14.00\nsum            78.00  176.00    185110.00   131.00\nmedian          7.50   17.50     15000.00    13.00\nmean            7.80   17.60     18511.00    13.10\nSE.mean         1.94    2.23      5692.53     1.26\nCI.mean.0.95    4.39    5.04     12877.39     2.85\nvar            37.73   49.60 324048765.56    15.88\nstd.dev         6.14    7.04     18001.35     3.98\ncoef.var        0.79    0.40         0.97     0.30\nskewness        0.11   -0.03         0.45     0.36\nskew.2SE        0.08   -0.03         0.33     0.26\nkurtosis       -1.75   -0.75        -1.48    -0.66\nkurt.2SE       -0.66   -0.28        -0.55    -0.25\nnormtest.W      0.92    0.98         0.90     0.95\nnormtest.p      0.37    0.94         0.20     0.65\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nIn R kann man Dezimalzahlen auch in der sogenannten wissenschaftlichen Notation anschreiben. Hier verwendet man eine Darstellung mit Zehnerpotenzen, die man mit e eingeben kann – e kann man als “mal zehn hoch” lesen.\n\n1e0  # 1 mal 10 hoch 0\n\n[1] 1\n\n\n\n-4e0  # -4 mal 10 hoch 0\n\n[1] -4\n\n\n\n1e1  # 1 mal 10 hoch 1\n\n[1] 10\n\n\n\n3.5e2  # 3.5 mal 10 hoch 2\n\n[1] 350\n\n\n\n1e-2  # 1 mal 10 hoch -2\n\n[1] 0.01\n\n\n\n1e-15  # 1 mal 10 hoch -15 = 0.000000000000001\n\n[1] 1e-15\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nMöchte man die Ausgabe von Zahlen in der wissenschaftlichen Notation weitgehend vermeiden, kann man zu Beginn einer Sitzung folgende Option setzen:\noptions(scipen=100)\nEine andere Möglichkeit ist es, Zahlen mit format() als normale Dezimalzahlen darzustellen. Dies ergibt aber immer einen Character-Vektor, erfordert dafür aber nicht das Ändern einer Option:\n\nformat(1e-15, scientific=FALSE)\n\n[1] \"0.000000000000001\"\n\n\n\n\n\n\n\nFür die Funktion stat.desc() gibt es keine direkte Variante für gruppierte Daten (so wie describeBy() für describe()). Es gibt aber in R die generische Funktion by(), welche beliebige Funktionen auf gruppierte Daten anwendet. Das erste Argument ist hier wie üblich der Datensatz, das zweite Argument ist die Gruppierungsspalte, und das dritte Argument ist die Funktion, die auf die gruppierten Daten angewendet werden soll.\n\nby(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], df$job, describe)\n\ndf$job: Lecturer\n         vars n    mean       sd median trimmed      mad   min   max range skew kurtosis      se\nfriends     1 5     2.4     2.07      2     2.4     2.97     0     5     5 0.11    -2.03    0.93\nalcohol     2 5    16.0     9.62     15    16.0     7.41     5    30    25 0.28    -1.72    4.30\nincome      3 5 33400.0 12561.85  35000 33400.0 19273.80 20000 50000 30000 0.10    -1.98 5617.83\nneurotic    4 5    15.0     4.18     14    15.0     4.45    10    21    11 0.25    -1.72    1.87\n------------------------------------------------------------------------------------------ \ndf$job: Student\n         vars n   mean      sd median trimmed     mad min   max range  skew kurtosis      se\nfriends     1 5   13.2    2.77     12    13.2    2.97  10    17     7  0.23    -1.89    1.24\nalcohol     2 5   19.2    3.56     18    19.2    2.97  16    25     9  0.66    -1.43    1.59\nincome      3 5 3622.0 4135.69   3000  3622.0 4299.54  10 10000  9990  0.48    -1.64 1849.54\nneurotic    4 5   11.2    3.03     13    11.2    1.48   7    14     7 -0.37    -2.01    1.36\n\n\nMöchte man der Funktion im dritten Argument selbst Argumente übergeben (z.B. norm=TRUE für die Funktion stat.desc()), kann man dies mit weiteren Argumenten ganz am Ende tun:\n\nby(\n    df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")],\n    df$job,\n    stat.desc,\n    norm=TRUE\n)\n\ndf$job: Lecturer\n                 friends    alcohol        income   neurotic\nnbr.val       5.00000000  5.0000000  5.000000e+00  5.0000000\nnbr.null      1.00000000  0.0000000  0.000000e+00  0.0000000\nnbr.na        0.00000000  0.0000000  0.000000e+00  0.0000000\nmin           0.00000000  5.0000000  2.000000e+04 10.0000000\nmax           5.00000000 30.0000000  5.000000e+04 21.0000000\nrange         5.00000000 25.0000000  3.000000e+04 11.0000000\nsum          12.00000000 80.0000000  1.670000e+05 75.0000000\nmedian        2.00000000 15.0000000  3.500000e+04 14.0000000\nmean          2.40000000 16.0000000  3.340000e+04 15.0000000\nSE.mean       0.92736185  4.3011626  5.617829e+03  1.8708287\nCI.mean.0.95  2.57476927 11.9419419  1.559759e+04  5.1942532\nvar           4.30000000 92.5000000  1.578000e+08 17.5000000\nstd.dev       2.07364414  9.6176920  1.256185e+04  4.1833001\ncoef.var      0.86401839  0.6011058  3.761032e-01  0.2788867\nskewness      0.11304669  0.2832618  9.869949e-02  0.2458756\nskew.2SE      0.06191822  0.1551489  5.405994e-02  0.1346716\nkurtosis     -2.03411574 -1.7235062 -1.980205e+00 -1.7239184\nkurt.2SE     -0.50852893 -0.4308766 -4.950513e-01 -0.4309796\nnormtest.W    0.95235149  0.9787162  9.342460e-01  0.9785712\nnormtest.p    0.75397300  0.9276364  6.255965e-01  0.9268345\n------------------------------------------------------------------------------------------ \ndf$job: Student\n                friends    alcohol        income   neurotic\nnbr.val       5.0000000  5.0000000  5.000000e+00  5.0000000\nnbr.null      0.0000000  0.0000000  0.000000e+00  0.0000000\nnbr.na        0.0000000  0.0000000  0.000000e+00  0.0000000\nmin          10.0000000 16.0000000  1.000000e+01  7.0000000\nmax          17.0000000 25.0000000  1.000000e+04 14.0000000\nrange         7.0000000  9.0000000  9.990000e+03  7.0000000\nsum          66.0000000 96.0000000  1.811000e+04 56.0000000\nmedian       12.0000000 18.0000000  3.000000e+03 13.0000000\nmean         13.2000000 19.2000000  3.622000e+03 11.2000000\nSE.mean       1.2409674  1.5937377  1.849536e+03  1.3564660\nCI.mean.0.95  3.4454778  4.4249254  5.135136e+03  3.7661534\nvar           7.7000000 12.7000000  1.710392e+07  9.2000000\nstd.dev       2.7748874  3.5637059  4.135689e+03  3.0331502\ncoef.var      0.2102187  0.1856097  1.141825e+00  0.2708170\nskewness      0.2291423  0.6649718  4.835220e-01 -0.3663862\nskew.2SE      0.1255064  0.3642200  2.648359e-01 -0.2006780\nkurtosis     -1.8935200 -1.4346010 -1.644573e+00 -2.0145180\nkurt.2SE     -0.4733800 -0.3586503 -4.111433e-01 -0.5036295\nnormtest.W    0.9385501  0.8852008  8.943871e-01  0.8576350\nnormtest.p    0.6557061  0.3335463  3.796449e-01  0.2198809\n\nby(df[, -c(1, 2)], df$job, summary)\n\ndf$job: Lecturer\n    friends       alcohol       income         neurotic \n Min.   :0.0   Min.   : 5   Min.   :20000   Min.   :10  \n 1st Qu.:1.0   1st Qu.:10   1st Qu.:22000   1st Qu.:13  \n Median :2.0   Median :15   Median :35000   Median :14  \n Mean   :2.4   Mean   :16   Mean   :33400   Mean   :15  \n 3rd Qu.:4.0   3rd Qu.:20   3rd Qu.:40000   3rd Qu.:17  \n Max.   :5.0   Max.   :30   Max.   :50000   Max.   :21  \n------------------------------------------------------------------------------------------ \ndf$job: Student\n    friends        alcohol         income         neurotic   \n Min.   :10.0   Min.   :16.0   Min.   :   10   Min.   : 7.0  \n 1st Qu.:12.0   1st Qu.:17.0   1st Qu.:  100   1st Qu.: 9.0  \n Median :12.0   Median :18.0   Median : 3000   Median :13.0  \n Mean   :13.2   Mean   :19.2   Mean   : 3622   Mean   :11.2  \n 3rd Qu.:15.0   3rd Qu.:20.0   3rd Qu.: 5000   3rd Qu.:13.0  \n Max.   :17.0   Max.   :25.0   Max.   :10000   Max.   :14.0  \n\nby(df$friends, df$job, mean)\n\ndf$job: Lecturer\n[1] 2.4\n------------------------------------------------------------------------------------------ \ndf$job: Student\n[1] 13.2"
  },
  {
    "objectID": "06/06.html#allgemeines",
    "href": "06/06.html#allgemeines",
    "title": "6 – Deskriptive Statistiken",
    "section": "",
    "text": "Nachdem man eine Datei korrekt importiert hat (d.h. wenn die Daten in einem Data Frame bzw. Tibble vorhanden sind und alle Spalten den passenden Datentyp haben), kann man mit der statistischen Analyse beginnen. Der erste Schritt dabei ist meist, sich mit deskriptiven Statistiken eine Beschreibung der vorhandenen Daten zu erzeugen.\nAls Beispiel dazu importieren wir die Datei lecturer.dat aus der letzten Übung:\n\nlibrary(readr)\n(df = read_tsv(\"lecturer.dat\"))\n\n# A tibble: 10 × 7\n   name   birth_date   job friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977       1       5      10  20000       10\n 2 Martin 5/24/1969      1       2      15  40000       17\n 3 Andy   6/21/1973      1       0      20  35000       14\n 4 Paul   7/16/1970      1       4       5  22000       13\n 5 Graham 10/10/1949     1       1      30  50000       21\n 6 Carina 11/5/1983      2      10      25   5000        7\n 7 Karina 10/8/1987      2      12      20    100       13\n 8 Doug   1/23/1989      2      15      16   3000        9\n 9 Mark   5/20/1973      2      12      17  10000       14\n10 Zoe    11/12/1984     2      17      18     10       13\n\n\nDanach führen wir wieder die Konvertierungen für die Spalten birth_date und job durch:\n\ndf$birth_date = as.Date(df$birth_date, format=\"%m/%d/%Y\")\ndf$job = factor(df$job, levels=c(1, 2), labels=c(\"Lecturer\", \"Student\"))\ndf\n\n# A tibble: 10 × 7\n   name   birth_date job      friends alcohol income neurotic\n   &lt;chr&gt;  &lt;date&gt;     &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    1977-07-03 Lecturer       5      10  20000       10\n 2 Martin 1969-05-24 Lecturer       2      15  40000       17\n 3 Andy   1973-06-21 Lecturer       0      20  35000       14\n 4 Paul   1970-07-16 Lecturer       4       5  22000       13\n 5 Graham 1949-10-10 Lecturer       1      30  50000       21\n 6 Carina 1983-11-05 Student       10      25   5000        7\n 7 Karina 1987-10-08 Student       12      20    100       13\n 8 Doug   1989-01-23 Student       15      16   3000        9\n 9 Mark   1973-05-20 Student       12      17  10000       14\n10 Zoe    1984-11-12 Student       17      18     10       13\n\n\nDie name-Spalte brauchen wir für unsere nachfolgenden Betrachtungen nicht mehr, deswegen entfernen wir sie:\n\ndf$name = NULL\n\nEs gibt in R eine Reihe an Funktionen, welche zusammenfassende Statistiken einer Variablen (bzw. eines Vektors) berechnen. Nützliche Funktionen sind z.B. mean(), sd(), var(), min(), max(), median(), range() und quantile(). Den Mittelwert eines Vektors (und damit auch einer Spalte von df) kann man also wie folgt berechnen:\n\nmean(df$friends)\n\n[1] 7.8\n\n\nVor allem bei Faktoren ist es interessant zu wissen, wie viele Stufen (unterschiedliche Werte) diese beinhalten. Auch bei numerischen oder Text-Vektoren mit mehrfach vorkommenden Werten ist es manchmal praktisch, die unterschiedlichen Werte herauszufinden. Dafür gibt es die Funktion unique(), welche die einzigartigen Elemente eines Vektors zurückgibt:\n\nunique(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2))\n\n[1] 1 2\n\n\n\nunique(df$friends)\n\n[1]  5  2  0  4  1 10 12 15 17\n\nunique(df$job)\n\n[1] Lecturer Student \nLevels: Lecturer Student\n\n\nAll diese Berechnungen müsste man nun für jede interessierende Spalte wiederholen (weil diese nur mit Vektoren funktionieren), was relativ mühsam wäre. Deswegen gibt es die Funktion sapply(), welche eine Funktion auf jede Spalte eines Data Frames einzeln anwendet. Möchte man also den Mittelwert für jede numerische Spalte von df berechnen, kann man dies so tun:\n\nsapply(df[, -c(1, 2)], mean)\n\n friends  alcohol   income neurotic \n     7.8     17.6  18511.0     13.1 \n\n\n\n\n\n\n\n\nTipp\n\n\n\nFür die Berechnung der Spaltenmittelwerte kann man auch die uns bereits bekannte Funktion colMeans() verwenden.\n\n\nBeachten Sie, dass df[, -c(1, 2)] alle Spalten aus df ausgenommen der ersten und zweiten bedeutet. So kann man jede beliebige Funktion auf einzelne Spalten anwenden, z.B. die Standardabweichung:\n\nsapply(df[, -c(1, 2)], sd)\n\n     friends      alcohol       income     neurotic \n    6.142746     7.042727 18001.354548     3.984693 \n\n\nEs gibt aber auch spezielle Funktionen, welche direkt mehrere statistische Kenngrößen für alle Spalten eines Data Frames berechnen. Im Folgenden gehen wir näher auf drei dieser Funktionen ein, nämlich summary(), describe() und stat.desc(). Nur summary() ist standardmäßig bei R dabei, die anderen zwei Funktionen erfordern die Installation von zusätzlichen Paketen.\n\n\nDie Funktion summary() liefert eine geeignete Zusammenfassung für jede Spalte eines Data Frames (Tibbles). Numerische Spalten sowie Datumsspalten werden mit sechs Werten beschrieben:\n\nMinimum\nErstes Quartil (25%-Perzentil)\nMedian (50%-Perzentil)\nDrittes Quartil (75%-Perzentil)\nMaximum\nArithmetischer Mittelwert\n\nFür Faktoren werden die Stufen sowie die Anzahl an Fällen pro Stufe aufgelistet.\n\nsummary(df)\n\n   birth_date               job       friends        alcohol          income         neurotic    \n Min.   :1949-10-10   Lecturer:5   Min.   : 0.0   Min.   : 5.00   Min.   :   10   Min.   : 7.00  \n 1st Qu.:1971-04-01   Student :5   1st Qu.: 2.5   1st Qu.:15.25   1st Qu.: 3500   1st Qu.:10.75  \n Median :1975-06-27                Median : 7.5   Median :17.50   Median :15000   Median :13.00  \n Mean   :1975-12-17                Mean   : 7.8   Mean   :17.60   Mean   :18511   Mean   :13.10  \n 3rd Qu.:1984-08-10                3rd Qu.:12.0   3rd Qu.:20.00   3rd Qu.:31750   3rd Qu.:14.00  \n Max.   :1989-01-23                Max.   :17.0   Max.   :30.00   Max.   :50000   Max.   :21.00  \n\n\nAuch bei anderen Objekten als Data Frames liefert summary() oft eine sinnvolle kurze Beschreibung.\n\n\n\nEine weitere Möglichkeit, noch mehr statistische Kenngrößen für numerische Spalten auszugeben, bietet die Funktion describe() aus dem psych-Paket. Nicht-numerische Spalten werden hier nicht vernünftig zusammengefasst, deshalb sollte man der Funktion nur numerische Spalten übergeben.\n\nlibrary(psych)\ndescribe(df[, 3:6])\n\n         vars  n    mean       sd  median  trimmed      mad min   max range  skew kurtosis      se\nfriends     1 10     7.8     6.14     7.5     7.62     7.41   0    17    17  0.11    -1.75    1.94\nalcohol     2 10    17.6     7.04    17.5    17.62     3.71   5    30    25 -0.03    -0.75    2.23\nincome      3 10 18511.0 18001.35 15000.0 16887.50 19940.97  10 50000 49990  0.45    -1.48 5692.53\nneurotic    4 10    13.1     3.98    13.0    12.88     2.97   7    21    14  0.36    -0.66    1.26\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nEigentlich kann man Funktionen aus Paketen auch ohne Aktivierung verwenden. Dazu muss man den Paketnamen gefolgt von :: voranstellen. Im vorigen Beispiel könnte man also auf library(psych) verzichten und die Funktion mit psych::describe() trotzdem verwenden.\n\n\nMan kann diese Funktion auch auf einzelne Gruppen separat anwenden. Im Beispiel könnte man dies getrennt für jede Stufe von df$job tun. Dazu verwendet man die verwandte Funktion describeBy():\n\ndescribeBy(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], df$job)\n\n\n Descriptive statistics by group \ngroup: Lecturer\n         vars n    mean       sd median trimmed      mad   min   max range skew kurtosis      se\nfriends     1 5     2.4     2.07      2     2.4     2.97     0     5     5 0.11    -2.03    0.93\nalcohol     2 5    16.0     9.62     15    16.0     7.41     5    30    25 0.28    -1.72    4.30\nincome      3 5 33400.0 12561.85  35000 33400.0 19273.80 20000 50000 30000 0.10    -1.98 5617.83\nneurotic    4 5    15.0     4.18     14    15.0     4.45    10    21    11 0.25    -1.72    1.87\n------------------------------------------------------------------------------------------ \ngroup: Student\n         vars n   mean      sd median trimmed     mad min   max range  skew kurtosis      se\nfriends     1 5   13.2    2.77     12    13.2    2.97  10    17     7  0.23    -1.89    1.24\nalcohol     2 5   19.2    3.56     18    19.2    2.97  16    25     9  0.66    -1.43    1.59\nincome      3 5 3622.0 4135.69   3000  3622.0 4299.54  10 10000  9990  0.48    -1.64 1849.54\nneurotic    4 5   11.2    3.03     13    11.2    1.48   7    14     7 -0.37    -2.01    1.36\n\n\nDas erste Argument ist das zu beschreibende Data Frame, und das zweite Argument ist die Spalte, nach der gruppiert werden soll (üblicherweise ein Faktor).\n\n\n\nDas Paket pastecs beinhaltet die Funktion stat.desc() zur Beschreibung von Daten. Mit der Funktion round() sollte man hier außerdem einstellen, wie viele Kommastellen ausgegeben werden sollen, da die Ausgabe der Funktion sonst relativ unübersichtlich ist. Wenn das Argument norm=TRUE gesetzt wird, werden für alle Spalten Tests auf Normalverteilung durchgeführt.\n\nlibrary(pastecs)\nround(\n    stat.desc(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], norm=TRUE),\n    digits=2\n)\n\n             friends alcohol       income neurotic\nnbr.val        10.00   10.00        10.00    10.00\nnbr.null        1.00    0.00         0.00     0.00\nnbr.na          0.00    0.00         0.00     0.00\nmin             0.00    5.00        10.00     7.00\nmax            17.00   30.00     50000.00    21.00\nrange          17.00   25.00     49990.00    14.00\nsum            78.00  176.00    185110.00   131.00\nmedian          7.50   17.50     15000.00    13.00\nmean            7.80   17.60     18511.00    13.10\nSE.mean         1.94    2.23      5692.53     1.26\nCI.mean.0.95    4.39    5.04     12877.39     2.85\nvar            37.73   49.60 324048765.56    15.88\nstd.dev         6.14    7.04     18001.35     3.98\ncoef.var        0.79    0.40         0.97     0.30\nskewness        0.11   -0.03         0.45     0.36\nskew.2SE        0.08   -0.03         0.33     0.26\nkurtosis       -1.75   -0.75        -1.48    -0.66\nkurt.2SE       -0.66   -0.28        -0.55    -0.25\nnormtest.W      0.92    0.98         0.90     0.95\nnormtest.p      0.37    0.94         0.20     0.65\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nIn R kann man Dezimalzahlen auch in der sogenannten wissenschaftlichen Notation anschreiben. Hier verwendet man eine Darstellung mit Zehnerpotenzen, die man mit e eingeben kann – e kann man als “mal zehn hoch” lesen.\n\n1e0  # 1 mal 10 hoch 0\n\n[1] 1\n\n\n\n-4e0  # -4 mal 10 hoch 0\n\n[1] -4\n\n\n\n1e1  # 1 mal 10 hoch 1\n\n[1] 10\n\n\n\n3.5e2  # 3.5 mal 10 hoch 2\n\n[1] 350\n\n\n\n1e-2  # 1 mal 10 hoch -2\n\n[1] 0.01\n\n\n\n1e-15  # 1 mal 10 hoch -15 = 0.000000000000001\n\n[1] 1e-15\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nMöchte man die Ausgabe von Zahlen in der wissenschaftlichen Notation weitgehend vermeiden, kann man zu Beginn einer Sitzung folgende Option setzen:\noptions(scipen=100)\nEine andere Möglichkeit ist es, Zahlen mit format() als normale Dezimalzahlen darzustellen. Dies ergibt aber immer einen Character-Vektor, erfordert dafür aber nicht das Ändern einer Option:\n\nformat(1e-15, scientific=FALSE)\n\n[1] \"0.000000000000001\"\n\n\n\n\n\n\n\nFür die Funktion stat.desc() gibt es keine direkte Variante für gruppierte Daten (so wie describeBy() für describe()). Es gibt aber in R die generische Funktion by(), welche beliebige Funktionen auf gruppierte Daten anwendet. Das erste Argument ist hier wie üblich der Datensatz, das zweite Argument ist die Gruppierungsspalte, und das dritte Argument ist die Funktion, die auf die gruppierten Daten angewendet werden soll.\n\nby(df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")], df$job, describe)\n\ndf$job: Lecturer\n         vars n    mean       sd median trimmed      mad   min   max range skew kurtosis      se\nfriends     1 5     2.4     2.07      2     2.4     2.97     0     5     5 0.11    -2.03    0.93\nalcohol     2 5    16.0     9.62     15    16.0     7.41     5    30    25 0.28    -1.72    4.30\nincome      3 5 33400.0 12561.85  35000 33400.0 19273.80 20000 50000 30000 0.10    -1.98 5617.83\nneurotic    4 5    15.0     4.18     14    15.0     4.45    10    21    11 0.25    -1.72    1.87\n------------------------------------------------------------------------------------------ \ndf$job: Student\n         vars n   mean      sd median trimmed     mad min   max range  skew kurtosis      se\nfriends     1 5   13.2    2.77     12    13.2    2.97  10    17     7  0.23    -1.89    1.24\nalcohol     2 5   19.2    3.56     18    19.2    2.97  16    25     9  0.66    -1.43    1.59\nincome      3 5 3622.0 4135.69   3000  3622.0 4299.54  10 10000  9990  0.48    -1.64 1849.54\nneurotic    4 5   11.2    3.03     13    11.2    1.48   7    14     7 -0.37    -2.01    1.36\n\n\nMöchte man der Funktion im dritten Argument selbst Argumente übergeben (z.B. norm=TRUE für die Funktion stat.desc()), kann man dies mit weiteren Argumenten ganz am Ende tun:\n\nby(\n    df[, c(\"friends\", \"alcohol\", \"income\", \"neurotic\")],\n    df$job,\n    stat.desc,\n    norm=TRUE\n)\n\ndf$job: Lecturer\n                 friends    alcohol        income   neurotic\nnbr.val       5.00000000  5.0000000  5.000000e+00  5.0000000\nnbr.null      1.00000000  0.0000000  0.000000e+00  0.0000000\nnbr.na        0.00000000  0.0000000  0.000000e+00  0.0000000\nmin           0.00000000  5.0000000  2.000000e+04 10.0000000\nmax           5.00000000 30.0000000  5.000000e+04 21.0000000\nrange         5.00000000 25.0000000  3.000000e+04 11.0000000\nsum          12.00000000 80.0000000  1.670000e+05 75.0000000\nmedian        2.00000000 15.0000000  3.500000e+04 14.0000000\nmean          2.40000000 16.0000000  3.340000e+04 15.0000000\nSE.mean       0.92736185  4.3011626  5.617829e+03  1.8708287\nCI.mean.0.95  2.57476927 11.9419419  1.559759e+04  5.1942532\nvar           4.30000000 92.5000000  1.578000e+08 17.5000000\nstd.dev       2.07364414  9.6176920  1.256185e+04  4.1833001\ncoef.var      0.86401839  0.6011058  3.761032e-01  0.2788867\nskewness      0.11304669  0.2832618  9.869949e-02  0.2458756\nskew.2SE      0.06191822  0.1551489  5.405994e-02  0.1346716\nkurtosis     -2.03411574 -1.7235062 -1.980205e+00 -1.7239184\nkurt.2SE     -0.50852893 -0.4308766 -4.950513e-01 -0.4309796\nnormtest.W    0.95235149  0.9787162  9.342460e-01  0.9785712\nnormtest.p    0.75397300  0.9276364  6.255965e-01  0.9268345\n------------------------------------------------------------------------------------------ \ndf$job: Student\n                friends    alcohol        income   neurotic\nnbr.val       5.0000000  5.0000000  5.000000e+00  5.0000000\nnbr.null      0.0000000  0.0000000  0.000000e+00  0.0000000\nnbr.na        0.0000000  0.0000000  0.000000e+00  0.0000000\nmin          10.0000000 16.0000000  1.000000e+01  7.0000000\nmax          17.0000000 25.0000000  1.000000e+04 14.0000000\nrange         7.0000000  9.0000000  9.990000e+03  7.0000000\nsum          66.0000000 96.0000000  1.811000e+04 56.0000000\nmedian       12.0000000 18.0000000  3.000000e+03 13.0000000\nmean         13.2000000 19.2000000  3.622000e+03 11.2000000\nSE.mean       1.2409674  1.5937377  1.849536e+03  1.3564660\nCI.mean.0.95  3.4454778  4.4249254  5.135136e+03  3.7661534\nvar           7.7000000 12.7000000  1.710392e+07  9.2000000\nstd.dev       2.7748874  3.5637059  4.135689e+03  3.0331502\ncoef.var      0.2102187  0.1856097  1.141825e+00  0.2708170\nskewness      0.2291423  0.6649718  4.835220e-01 -0.3663862\nskew.2SE      0.1255064  0.3642200  2.648359e-01 -0.2006780\nkurtosis     -1.8935200 -1.4346010 -1.644573e+00 -2.0145180\nkurt.2SE     -0.4733800 -0.3586503 -4.111433e-01 -0.5036295\nnormtest.W    0.9385501  0.8852008  8.943871e-01  0.8576350\nnormtest.p    0.6557061  0.3335463  3.796449e-01  0.2198809\n\nby(df[, -c(1, 2)], df$job, summary)\n\ndf$job: Lecturer\n    friends       alcohol       income         neurotic \n Min.   :0.0   Min.   : 5   Min.   :20000   Min.   :10  \n 1st Qu.:1.0   1st Qu.:10   1st Qu.:22000   1st Qu.:13  \n Median :2.0   Median :15   Median :35000   Median :14  \n Mean   :2.4   Mean   :16   Mean   :33400   Mean   :15  \n 3rd Qu.:4.0   3rd Qu.:20   3rd Qu.:40000   3rd Qu.:17  \n Max.   :5.0   Max.   :30   Max.   :50000   Max.   :21  \n------------------------------------------------------------------------------------------ \ndf$job: Student\n    friends        alcohol         income         neurotic   \n Min.   :10.0   Min.   :16.0   Min.   :   10   Min.   : 7.0  \n 1st Qu.:12.0   1st Qu.:17.0   1st Qu.:  100   1st Qu.: 9.0  \n Median :12.0   Median :18.0   Median : 3000   Median :13.0  \n Mean   :13.2   Mean   :19.2   Mean   : 3622   Mean   :11.2  \n 3rd Qu.:15.0   3rd Qu.:20.0   3rd Qu.: 5000   3rd Qu.:13.0  \n Max.   :17.0   Max.   :25.0   Max.   :10000   Max.   :14.0  \n\nby(df$friends, df$job, mean)\n\ndf$job: Lecturer\n[1] 2.4\n------------------------------------------------------------------------------------------ \ndf$job: Student\n[1] 13.2"
  },
  {
    "objectID": "06/06.html#test-auf-normalverteilung",
    "href": "06/06.html#test-auf-normalverteilung",
    "title": "6 – Deskriptive Statistiken",
    "section": "Test auf Normalverteilung",
    "text": "Test auf Normalverteilung\nDie Funktion stat.desc() liefert bereits das Ergebnis des Shapiro-Wilk-Tests auf Normalverteilung (die Einträge normtest.W und normtest.p enthalten den Wert der Teststatistik bzw. die Signifikanz). Wenn normtest.p signifikant ist (z.B. kleiner als 0.05), dann kann man die Nullhypothese der Normalverteilung verwerfen. Man kann den Shapiro-Wilk-Test auch direkt mit der Funktion shapiro.test() aufrufen:\n\nshapiro.test(df$income)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$income\nW = 0.89721, p-value = 0.2041\n\n\nMit der by()-Funktion kann man den Test auch getrennt auf verschiedene Gruppen anwenden.\n\nby(df$income, df$job, shapiro.test)\n\ndf$job: Lecturer\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.93425, p-value = 0.6256\n\n------------------------------------------------------------------------------------------ \ndf$job: Student\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.89439, p-value = 0.3796\n\n\nDer Kolmogorov-Smirnov-Test kann gegebene Daten auf beliebige Verteilungen testen, d.h. natürlich auch auf Normalverteilung. Im Falle der Normalverteilung ist aber der Shapiro-Wilk-Test vorzuziehen, da dieser speziell auf die Normalverteilung zugeschnitten ist und daher mehr statistische Power besitzt.\n\nks.test(df$income, \"pnorm\", mean(df$income), sd(df$income))\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  df$income\nD = 0.18182, p-value = 0.8389\nalternative hypothesis: two-sided\n\n\nDa die Stichprobengröße in unserem Beispiel nur sehr klein ist, lassen sich aber ohnehin keine vernünftigen Aussagen über die Verteilung der Daten treffen."
  },
  {
    "objectID": "06/06.html#test-auf-varianzhomogenität",
    "href": "06/06.html#test-auf-varianzhomogenität",
    "title": "6 – Deskriptive Statistiken",
    "section": "Test auf Varianzhomogenität",
    "text": "Test auf Varianzhomogenität\nDer Levene-Test prüft auf Gleichheit der Varianzen (Homoskedastizität) von zwei oder mehr Gruppen. Die Nullhypothese ist, dass die Varianzen in allen Gruppen gleich sind. In R führt man den Test mit der Funktion leveneTest() aus dem Paket car durch. Dazu sehen wir uns die Beispieldaten Moore an, welche mit dem Paket car automatisch geladen werden.\n\nlibrary(car)\nhead(Moore, 4)\n\n  partner.status conformity fcategory fscore\n1            low          8       low     37\n2            low          4      high     57\n3            low          8      high     65\n4            low          7       low     20\n\ntail(Moore, 4)\n\n   partner.status conformity fcategory fscore\n42           high         13      high     57\n43           high         16       low     35\n44           high         10      high     52\n45           high         15    medium     44\n\nsummary(Moore)\n\n partner.status   conformity     fcategory      fscore     \n high:23        Min.   : 4.00   high  :15   Min.   :15.00  \n low :22        1st Qu.: 8.00   low   :15   1st Qu.:35.00  \n                Median :12.00   medium:15   Median :43.00  \n                Mean   :12.13               Mean   :43.11  \n                3rd Qu.:15.00               3rd Qu.:55.00  \n                Max.   :24.00               Max.   :68.00  \n\n\nDer Levene-Test für die Spalte conformity gruppiert nach der Spalte fcategory wird wie folgt aufgerufen:\n\nleveneTest(Moore$conformity, Moore$fcategory)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   0.046 0.9551\n      42               \n\n\nIn diesem Beispiel kann die Nullhypothese der Varianzgleichheit der Variable conformity über die Gruppen in fcategory also nicht verworfen werden, da der p-Wert (hier als Pr(&gt;F) bezeichnet) mit 0.9551 extrem groß ist."
  },
  {
    "objectID": "06/06.html#übungen",
    "href": "06/06.html#übungen",
    "title": "6 – Deskriptive Statistiken",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nBerechnen Sie statistische Kenngrößen wie Mittelwert, Median, Minimum und Maximum für die vier numerischen Variablen Global_active_power, Global_reactive_power, Voltage und Global_intensity aus den Daten die Sie in der vorigen Einheit bereits importiert haben (Individual Household Electric Power Consumption).\n\nBerechnen Sie die Kenngrößen mit der Funktion sapply().\nBerechnen Sie die Kenngrößen mit der Funktion summary().\nBerechnen Sie die Kenngrößen mit der Funktion describe() aus dem Paket psych.\nWenden Sie die Funktion stat.desc() aus dem Paket pastecs auf die Daten an (runden Sie die Ergebnisse auf eine Nachkommastelle).\nWie groß ist die gemessene mittlere Spannung Voltage?\nWie groß ist der Median der globalen Wirkleistung Global_active_power?\nWie viele fehlende Werte gibt es insgesamt bzw. pro Spalte?\n\n\n\nÜbung 2\nDas Paket palmerpenguins beinhaltet im Tibble penguins Messdaten über physische Eigenschaften von drei Pinguinarten (Adélie, Chinstrap und Gentoo). Aktivieren Sie das Paket und beantworten Sie mit geeigneten R-Befehlen folgende Fragen:\n\nAus wie vielen Zeilen und Spalten besteht der Datensatz penguins?\nDie Spalten species, island und sex sind Faktoren – wie viele Stufen gibt es jeweils?\nBerechnen Sie deskriptive Statistiken getrennt für die drei Pinguinspezies! Wie lauten die Mittelwerte der Spalten bill_length_mm, bill_depth_mm und flipper_length_mm für die drei Spezies?\nGibt es fehlende Werte in den Daten?\n\n\n\nÜbung 3\nIm Datensatz mtcars befinden sich diverse Kenngrößen von 32 Autos.\n\nBerechnen Sie das Minimum und Maximum sowie den Mittelwert und den Median von allen Spalten.\nÜberprüfen Sie, ob die Daten in der Spalte mpg normalverteilt sind.\nÜberprüfen Sie, ob Varianzhomogenität für die Daten in der Spalte mpg gegeben ist für die drei Gruppen, die durch die Spalte cyl definiert sind.\n\n\n\nÜbung 4\nLaden Sie die Beispieldaten lecturer.dat und berechnen Sie die Mittelwerte aller numerischen Spalten gruppiert nach der Spalte job. Verwenden Sie dazu die Funktion by(). Warum funktioniert diese Berechnung nicht mit der Funktion mean?\n\n\nÜbung 5\nErzeugen Sie mit der Funktion rnorm() einen Vektor mit 5001 standardnormalverteilten Zufallszahlen und überprüfen Sie, ob dieser Vektor normalverteilt ist. Verwenden Sie dazu den Shapiro-Wilk-Test. Wie interpretieren Sie das Ergebnis?"
  },
  {
    "objectID": "05/05.html",
    "href": "05/05.html",
    "title": "5 – Daten importieren",
    "section": "",
    "text": "R kann mit Daten in vielen unterschiedlichen Formaten umgehen, beispielsweise mit Excel-Tabellen oder auch SPSS-Datensätzen. Diese Formate sind allerdings proprietär und daher für das Abspeichern eigener Daten weniger gut geeignet. Idealerweise speichert man Daten in einem offenen und möglichst einfach aufgebauten Format ab, welches man mit einer Vielzahl an (frei verfügbaren) Programmen öffnen kann. Ein Beispiel für ein einfaches Format ist eine Textdatei, die man mit jedem beliebigen Texteditor öffnen kann."
  },
  {
    "objectID": "05/05.html#allgemeines",
    "href": "05/05.html#allgemeines",
    "title": "5 – Daten importieren",
    "section": "",
    "text": "R kann mit Daten in vielen unterschiedlichen Formaten umgehen, beispielsweise mit Excel-Tabellen oder auch SPSS-Datensätzen. Diese Formate sind allerdings proprietär und daher für das Abspeichern eigener Daten weniger gut geeignet. Idealerweise speichert man Daten in einem offenen und möglichst einfach aufgebauten Format ab, welches man mit einer Vielzahl an (frei verfügbaren) Programmen öffnen kann. Ein Beispiel für ein einfaches Format ist eine Textdatei, die man mit jedem beliebigen Texteditor öffnen kann."
  },
  {
    "objectID": "05/05.html#textdateien",
    "href": "05/05.html#textdateien",
    "title": "5 – Daten importieren",
    "section": "Textdateien",
    "text": "Textdateien\nDaten in Textdateien werden häufig mit Kommas voneinander getrennt – so kann man Werte aus mehreren Spalten einer Tabelle darstellen. Solche speziell formatierten Textdateien haben daher häufig die Endung .csv (“comma-separated values”). Es gibt aber auch andere Möglichkeiten, Werte (bzw. Spalten) voneinander zu trennen, z.B. mit Semikolons (Strichpunkten) oder Tabulatoren. Solche Dateien haben dann oft die Endungen .dat oder .tsv (“tab-separated values”).\n\n\n\n\n\n\nHinweis\n\n\n\nDateiendungen sind eine Konvention und müssen nicht zwingend etwas mit dem tatsächlichen Inhalt der Datei zu tun haben. Textdateien werden oft mit den bereits erwähnten Erweiterungen .csv, .tsv und .dat versehen. Es kann aber durchaus vorkommen, dass eine Textdatei z.B. mit der Endung .csv Werte mit Tabulatoren oder Strichpunkten trennt statt mit Kommas. Letztendlich kann man das nur herausfinden, indem man die Datei öffnet.\n\n\nDas Tidyverse beinhaltet das Paket readr, welches Textdateien in den unterschiedlichsten Formaten importieren kann. Dies funktioniert oft besser und schneller als mit den Funktionen, die standardmäßig mit R mitgeliefert werden. Die mit dem readr-Paket eingelesenen Daten stehen außerdem gleich als Tibble zur Verfügung. Daher werden wir in dieser Lehrveranstaltung ausschließlich Funktionen aus readr zum Importieren von Textdateien verwenden.\n\nlibrary(readr)\n\n\n\n\n\n\n\nWichtig\n\n\n\nFunktionen zum Importieren von Textdaten aus dem readr-Paket beginnen alle mit read_, wohingegen die mit R mitgelieferten Funktionen mit read. beginnen.\n\n\nDie wichtigste Funktion aus dem Paket readr heißt read_delim(). Hier kann man sehr viele Parameter genau auf die einzulesende Datei abstimmen (z.B. das Spaltentrennzeichen, ob es eine Kopfzeile mit Spaltennamen gibt, ob Kommentare oder fehlende Werte vorhanden sind, usw.). Für Daten im CSV-Format (Spalten sind durch Kommas getrennt) gibt es die Wrapper-Funktion read_csv(), welche sinnvolle Standardwerte für diverse Argumente annimmt. Für Daten, die mit Tabulatoren getrennt sind, gibt es die Wrapper-Funktion read_tsv().\n\n\n\n\n\n\nHinweis\n\n\n\nDiese “Wrapper-Funktionen” rufen im Grunde also einfach read_delim() mit speziellen Argumenten auf.\n\n\nAls Beispiel sehen wir uns eine Textdatei namens lecturer.csv an. Wenn man nicht weiß, wie die Daten in einer Textdatei tatsächlich formatiert sind, kann man deren Inhalt mit der Funktion file.show() im RStudio-Editor öffnen:\nfile.show(\"lecturer.csv\")\nname,birth_date,job,friends,alcohol,income,neurotic\nBen,7/3/1977,1,5,10,20000,10\nMartin,5/24/1969,1,2,15,40000,17\nAndy,6/21/1973,1,0,20,35000,14\nPaul,7/16/1970,1,4,5,22000,13\nGraham,10/10/1949,1,1,30,50000,21\nCarina,11/5/1983,2,10,25,5000,7\nKarina,10/8/1987,2,12,20,100,13\nDoug,1/23/1989,2,15,16,3000,9\nMark,5/20/1973,2,12,17,10000,14\nZoe,11/12/1984,2,17,18,10,13\nMan erkennt, dass bei dieser konkreten Datei die einzelnen Spalten tatsächlich durch Kommas getrennt sind (hier passt also die Endung .csv zum Inhalt). Außerdem ist eine Kopfzeile mit den Spaltennamen vorhanden.\nDie Funktion read_delim() erkennt das Spaltentrennzeichen meistens automatisch, d.h. es reicht, wenn man ihr lediglich den Dateinamen als Argument übergibt:\n\nread_delim(\"lecturer.csv\")\n\n# A tibble: 10 × 7\n   name   birth_date   job friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977       1       5      10  20000       10\n 2 Martin 5/24/1969      1       2      15  40000       17\n 3 Andy   6/21/1973      1       0      20  35000       14\n 4 Paul   7/16/1970      1       4       5  22000       13\n 5 Graham 10/10/1949     1       1      30  50000       21\n 6 Carina 11/5/1983      2      10      25   5000        7\n 7 Karina 10/8/1987      2      12      20    100       13\n 8 Doug   1/23/1989      2      15      16   3000        9\n 9 Mark   5/20/1973      2      12      17  10000       14\n10 Zoe    11/12/1984     2      17      18     10       13\n\n\nFalls die automatische Erkennung einmal nicht funktionieren sollte, kann man das Trennzeichen aber mit dem Argument delim explizit übergeben:\n\nread_delim(\"lecturer.csv\", delim=\",\")\n\n# A tibble: 10 × 7\n   name   birth_date   job friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977       1       5      10  20000       10\n 2 Martin 5/24/1969      1       2      15  40000       17\n 3 Andy   6/21/1973      1       0      20  35000       14\n 4 Paul   7/16/1970      1       4       5  22000       13\n 5 Graham 10/10/1949     1       1      30  50000       21\n 6 Carina 11/5/1983      2      10      25   5000        7\n 7 Karina 10/8/1987      2      12      20    100       13\n 8 Doug   1/23/1989      2      15      16   3000        9\n 9 Mark   5/20/1973      2      12      17  10000       14\n10 Zoe    11/12/1984     2      17      18     10       13\n\n\nDas Ergebnis (ein Tibble) sieht korrekt aus – es hat 10 Zeilen und 7 Spalten mit sinnvollen Datentypen in allen Spalten. Wir können es direkt einer Variablen zuweisen, um damit weiterarbeiten zu können:\n\ndf = read_delim(\"lecturer.csv\")\n\nFür diese Datei hätten wir alternativ auch die Wrapper-Funktion read_csv() verwenden können, hier wird standardmäßig ein Komma als Spaltentrennzeichen angenommen:\nread_csv(\"lecturer.csv\")\nSehen wir uns als zweites Beispiel dieselben Daten an, die aber diesmal mit Tabulatoren voneinander getrennt in einer .dat-Datei vorliegen. Um diese Datei namens lecturer.dat einzulesen, können wir entweder die generische Funktion read_delim() mit der automatischen Erkennung verwenden (bzw. falls das nicht funktionieren sollte mit delim=\"\\t\", wobei \\t die Darstellung des Tabulator-Zeichens ist) oder direkt die spezialisierte Funktion read_tsv():\nread_delim(\"lecturer.dat\")\nread_delim(\"lecturer.dat\", delim=\"\\t\")\nread_tsv(\"lecturer.dat\")\nEin weiteres wichtiges Merkmal solcher Textdateien ist das verwendete Dezimaltrennzeichen bei Kommazahlen. In der englischen Schreibweise wird ein Punkt als Dezimaltrennzeichen verwendet (z.B. 12.3 oder 3.1415). In der deutschen Schreibweise wird hingegen ein Komma verwendet (z.B. 12,3 oder 3,1415). Das Dezimaltrennzeichen kann in der Funktion read_delim() mit dem Argument locale festgelegt werden. Es ist standardmäßig auf einen Punkt gesetzt, ebenso bei den Funktionen read_csv() und read_tsv(). Sollten Zahlen jedoch in der deutschen Schreibweise vorliegen, können die Spalten nicht auch durch Kommas getrennt sein – hier werden diese dann oft durch Semikolons getrennt. Für solche Textdateien setzt man daher die Argumente delim=\";\" und locale=locale(decimal_mark=\",\") bzw. verwendet die Wrapper-Funktion read_csv2() (welche ein Semikolon als Spaltentrennzeichen und ein Komma als Dezimaltrennzeichen annimmt).\n\n\n\n\n\n\nWichtig\n\n\n\nUnabhängig davon, wie Dezimaltrennzeichen in den Textdateien dargestellt werden, verwendet R für Dezimalzahlen immer einen Punkt!"
  },
  {
    "objectID": "05/05.html#daten-aus-spss",
    "href": "05/05.html#daten-aus-spss",
    "title": "5 – Daten importieren",
    "section": "Daten aus SPSS",
    "text": "Daten aus SPSS\nSollen bereits vorhandene SPSS-Datensätze (.sav) importiert werden, kann man dazu die Funktion read_sav() aus dem Paket haven verwenden. Das Ergebnis ist wieder ein Tibble. Das haven-Paket kann übrigens auch Daten aus SAS und Stata importieren. Das folgende Beispiel importiert Daten aus der Datei lecturer.sav:\nlibrary(haven)\ndf = read_sav(\"lecturer.sav\")"
  },
  {
    "objectID": "05/05.html#daten-aus-excel",
    "href": "05/05.html#daten-aus-excel",
    "title": "5 – Daten importieren",
    "section": "Daten aus Excel",
    "text": "Daten aus Excel\nWenn Daten als Excel-Datei (Endung .xlsx oder .xls) vorliegen, verwendet man zum Einlesen die Funktion read_excel() aus dem Paket readxl. Da dieses Paket Teil des Tidyverse ist, bekommt man auch hier ein Tibble zurückgeliefert, wie das folgende Beispiel anhand von lecturer.xlsx zeigt:\nlibrary(readxl)\ndf = read_excel(\"lecturer.xlsx\")"
  },
  {
    "objectID": "05/05.html#importieren-mit-rstudio",
    "href": "05/05.html#importieren-mit-rstudio",
    "title": "5 – Daten importieren",
    "section": "Importieren mit RStudio",
    "text": "Importieren mit RStudio\nMit RStudio kann man Daten in vielen Formaten auch mit einem grafischen Dialog importieren. Praktischerweise bekommt man immer den dazugehörigen R-Code mitgeliefert, welcher die Daten korrekt importiert – diesen Code kann man dann in eigenen Scripts verwenden.\nDazu klickt man in der Files-Ansicht rechts unten auf die gewünschte Datei und wählt Import Dataset aus. Alternativ kann man in der Environment-Ansicht rechts oben auf Import Dataset, gefolgt vom gewünschten Format, klicken. Diese Funktionalität ist auch im File-Menü unter dem Eintrag Import Dataset zu finden. Es öffnet sich ein neues Fenster, in dem man eine Vorschau der zu importierenden Datei sieht. Es gibt hier auch die Möglichkeit, Optionen zu ändern – wie sich das auf die eingelesenen Daten auswirkt, ist in der Vorschau unmittelbar ersichtlich. Der zugehörige Code befindet sich dann rechts unten. Im folgenden Bild sieht man diesen Dialog beispielhaft für eine zu importierende SPSS-Datei."
  },
  {
    "objectID": "05/05.html#daten-aufbereiten",
    "href": "05/05.html#daten-aufbereiten",
    "title": "5 – Daten importieren",
    "section": "Daten aufbereiten",
    "text": "Daten aufbereiten\nIn welchem Dateiformat die Daten auch immer vorliegen, schlussendlich landen sie in einem Data Frame (oder Tibble), welches wir dann in R weiterverwenden können. In den allermeisten Fällen wird man das Data Frame noch aufbereiten müssen, damit jede Spalte auch wirklich im gewünschten Format vorliegt. Sehen wir uns das anhand unserer Beispieldaten an:\n\n(df = read_csv(\"lecturer.csv\"))\n\n# A tibble: 10 × 7\n   name   birth_date   job friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977       1       5      10  20000       10\n 2 Martin 5/24/1969      1       2      15  40000       17\n 3 Andy   6/21/1973      1       0      20  35000       14\n 4 Paul   7/16/1970      1       4       5  22000       13\n 5 Graham 10/10/1949     1       1      30  50000       21\n 6 Carina 11/5/1983      2      10      25   5000        7\n 7 Karina 10/8/1987      2      12      20    100       13\n 8 Doug   1/23/1989      2      15      16   3000        9\n 9 Mark   5/20/1973      2      12      17  10000       14\n10 Zoe    11/12/1984     2      17      18     10       13\n\n\nMan erkennt, dass die numerischen Werte korrekt als Zahlen erkannt wurden (die Abkürzung &lt;dbl&gt; bedeutet “double” und entspricht Dezimalzahlen, d.h. einem numerischen Vektor). Allerdings besitzt die dritte Spalte job lediglich zwei Werte, welche eigentlich zwei verschiedene Berufe repräsentieren (1 steht für den Beruf “Lecturer”, 2 für den Beruf “Student”). Hier wäre eine kategorische Spalte (ein Faktor, dazu gleich mehr) besser geeignet als eine numerische. Die erste Spalte name wurde korrekt als chr (Character-Vektor) erkannt. Auch die zweite Spalte birth_date wurde als Charakter-Vektor erkannt, aber es gibt in R einen eigenen Datentyp für Datumswerte (was u.a. das Rechnen mit solchen Werten ermöglicht).\n\nFaktoren\nKategorische Variablen, d.h. Variablen, die nur eine bestimmte Anzahl an Werten annehmen können, werden in R mit dem Typ factor dargestellt. Die Funktion factor() kann einen entsprechenden Vektor erzeugen. Standardmäßig werden nicht geordnete (also nominale) Faktoren erzeugt. Setzt man das Argument ordered=TRUE, kann man auch einen geordneten Faktor (also eine ordinale Variable) erzeugen.\nIm Beispiel ist die Spalte job vom Typ dbl, sollte aber als Faktor interpretiert werden. Die Spalte kann entsprechend neu erstellt und überschrieben werden (hier wird vorausgesetzt, dass wir wissen, welche Zahlen den jeweiligen Faktorstufen entsprechen, also 1 entspricht “Lecturer” und 2 entspricht “Student”):\n\ndf$job = factor(df$job, levels=c(1, 2), labels=c(\"Lecturer\", \"Student\"))\ndf\n\n# A tibble: 10 × 7\n   name   birth_date job      friends alcohol income neurotic\n   &lt;chr&gt;  &lt;chr&gt;      &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    7/3/1977   Lecturer       5      10  20000       10\n 2 Martin 5/24/1969  Lecturer       2      15  40000       17\n 3 Andy   6/21/1973  Lecturer       0      20  35000       14\n 4 Paul   7/16/1970  Lecturer       4       5  22000       13\n 5 Graham 10/10/1949 Lecturer       1      30  50000       21\n 6 Carina 11/5/1983  Student       10      25   5000        7\n 7 Karina 10/8/1987  Student       12      20    100       13\n 8 Doug   1/23/1989  Student       15      16   3000        9\n 9 Mark   5/20/1973  Student       12      17  10000       14\n10 Zoe    11/12/1984 Student       17      18     10       13\n\n\nDie drei Argumente haben dabei folgende Bedeutung:\n\ndf$job sind die Ausgangsdaten.\nlevels=c(1, 2) gibt an, welche Werte (Stufen) in den Ausgangsdaten vorkommen und wir auch verwenden möchten.\nlabels=c(\"Lecturer\", \"Student\") weist den verwendeten Stufen entsprechende Namen (Labels) zu.\n\nDie Spalte job hat nun den gewünschten Typ (&lt;fct&gt; in der Tibble-Darstellung). Dies können wir auch mit der altbekannten Funktion class() explizit überprüfen:\n\nclass(df$job)\n\n[1] \"factor\"\n\n\n\n\nDatumswerte\nFür Datumswerte gibt es in R ebenfalls einen eigenen Datentyp, der das Rechnen mit solchen Werten erleichtert. Die Funktion as.Date() wandelt eine Datumsangabe in Textform in diesen speziellen Typ um. Anzugeben ist hier insbesondere das Argument format, welches das Format der vorliegenden Datumswerte spezifiziert (d.h. man beschreibt damit, wie die Datumswerte ursprünglich aussehen).\n\ndf$birth_date = as.Date(df$birth_date, format=\"%m/%d/%Y\")\ndf\n\n# A tibble: 10 × 7\n   name   birth_date job      friends alcohol income neurotic\n   &lt;chr&gt;  &lt;date&gt;     &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ben    1977-07-03 Lecturer       5      10  20000       10\n 2 Martin 1969-05-24 Lecturer       2      15  40000       17\n 3 Andy   1973-06-21 Lecturer       0      20  35000       14\n 4 Paul   1970-07-16 Lecturer       4       5  22000       13\n 5 Graham 1949-10-10 Lecturer       1      30  50000       21\n 6 Carina 1983-11-05 Student       10      25   5000        7\n 7 Karina 1987-10-08 Student       12      20    100       13\n 8 Doug   1989-01-23 Student       15      16   3000        9\n 9 Mark   1973-05-20 Student       12      17  10000       14\n10 Zoe    1984-11-12 Student       17      18     10       13\n\n\nIn diesem Beispiel bedeutet das Argument format=\"%m/%d/%Y\", dass die Werte ursprünglich in der Reihenfolge Monat/Tag/Jahr vorliegen und mit einem / getrennt sind (%m steht also für einen Monat, %d für einen Tag und %Y für eine vierstellige Jahreszahl).\nBetrachten wir ein weiteres Beispiel. Nehmen wir an, wir hätten folgenden Character-Vektor dates mit Datumswerten:\n\n(dates = c(\"23.3.95\", \"17.7.96\", \"9.12.04\", \"1.1.10\", \"23.2.17\"))\n\n[1] \"23.3.95\" \"17.7.96\" \"9.12.04\" \"1.1.10\"  \"23.2.17\"\n\nclass(dates)\n\n[1] \"character\"\n\n\nDie einzelnen Zahlen sind mit einem . voneinander getrennt. Nun müssen wir die Bedeutung der drei Zahlen herausfinden. Durch betrachten aller Werte stellt man fest, dass die Reihenfolge Tag, Monat und Jahr (zweistellig) ist. Das entsprechende format-Argument lautet daher format=\"%d.%m.%y\":\n\n(dates = as.Date(dates, format=\"%d.%m.%y\"))\n\n[1] \"1995-03-23\" \"1996-07-17\" \"2004-12-09\" \"2010-01-01\" \"2017-02-23\"\n\nclass(dates)\n\n[1] \"Date\"\n\n\nDie Kürzel %d, %m, %y und noch viele weitere sind in der Hilfe von as.Date() bzw. eigentlich strptime() beschrieben – machen Sie sich daher vor allem mit letzterer vertraut (?strptime), denn diese Kürzel muss man sich nicht auswendig merken."
  },
  {
    "objectID": "05/05.html#daten-speichern",
    "href": "05/05.html#daten-speichern",
    "title": "5 – Daten importieren",
    "section": "Daten speichern",
    "text": "Daten speichern\nWenn man ein bestehendes Data Frame oder Tibble als Textdatei speichern möchte, geht das am einfachsten mit der Funktion write_delim() (bzw. write_csv() und write_tsv()) aus dem readr-Paket. Dies funktioniert prinzipiell ganz analog zu den oben beschriebenen Lesefunktionen, nur gibt man hier sowohl das zu speichernde Data Frame als auch den Dateinamen an.\n\n\n\n\n\n\nTipp\n\n\n\nEs sollten nur kleine bis mittelgroße Datensätze als Textdateien gespeichert werden. Bei großen Datensätzen würde die entstehende Datei sehr viel Speicherplatz benötigen – in solchen Fällen sollte man effizientere Datenformate verwenden wie z.B. das in R verfügbare Datenformat .RData, welches mit der Funktion save() erzeugt bzw. mit load() gelesen werden kann. Noch bessere Alternativen sind die hocheffizienten und offenen Formate Parquet und Feather, welche sowohl mit R als auch mit Python und vielen weiteren Programmiersprachen importiert werden können."
  },
  {
    "objectID": "05/05.html#übungen",
    "href": "05/05.html#übungen",
    "title": "5 – Daten importieren",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nSehen Sie sich die Hilfe zur Funktion read_delim() aus dem readr-Paket an. Welches Argument setzt das Trennzeichen der Spalten? Welches Argument setzt das Dezimaltrennzeichen? Mit welchem Argument können Sie das Zeichen für fehlende Werte festlegen?\n\n\nÜbung 2\nImportieren Sie die Datei homework.csv und geben Sie das entstehende Tibble am Bildschirm aus. Achten Sie darauf, dass Sie die in der Datei verwendeten Spalten- bzw. Dezimaltrennzeichen korrekt erkennen (Achtung: Kommazahlen sind im deutschen Format vorhanden). Welche Datentypen haben die vier Spalten?\n\n\nÜbung 3\nIn der Datei wahl16.csv befinden sich die Ergebnisse der Bundespräsidentenwahl 2016 (und zwar nach dem ersten Wahlgang mit den sechs ursprünglichen Kandidaten/Kandidatinnen). Importieren Sie diese Daten in ein Tibble namens wahl16und berechnen Sie die relative Gesamtanzahl an Stimmen für jede Kandidatin/jeden Kandidaten (die Funktionen colSums(), rowSums() sowie sum() könnten dabei hilfreich sein).\n\n\nÜbung 4\nDie Datei covid19.csv enthält Daten zu den täglichen Covid19-Neuinfektionen in Österreich im Zeitraum 26.2.2020 bis 26.6.2023. Importieren Sie diese Datei in R (achten Sie auf geeignete Argumente, um die Datei richtig einzulesen wie z.B. das korrekte Spaltentrennzeichen sowie Dezimaltrennzeichen)! Aus wie vielen Zeilen und Spalten besteht dieser Datensatz?\nAchten Sie besonders auf die Spalte SiebenTageInzidenzFaelle – diese beinhaltet Dezimalzahlen und sollte dementsprechend numerisch sein!\nKonvertieren Sie auch die Spalte Time in ein Datumsformat (die Uhrzeit in dieser Spalte können Sie einfach ignorieren)!\n\n\nÜbung 5\nDas UCI Machine Learning Repository stellt viele Datensätze zur freien Verwendung zur Verfügung. Wir betrachten für diese Übung den Datensatz Individual Household Electric Power Consumption, und zwar die Datei household_power_consumption.zip. Sie müssen diese Datei nicht entpacken, sondern können diese direkt mit der Funktion read_delim() laden.\nDie Datei beinhaltet die minütliche elektrische Leistungsaufnahme eines Haushalts in einem Zeitraum von fast vier Jahren. Insgesamt gibt es über 2 Millionen Messpunkte und 9 Variablen, welche durch Strichpunkte voneinander ; getrennt sind. Fehlende Werte sind mit einem Fragezeichen ? bzw. mit tatsächlich fehlenden Einträgen codiert.\nLesen Sie die Daten in ein Tibble namens df ein und geben Sie es am Bildschirm aus um folgende Fragen zu beantworten:\n\nWie viele Zeilen und Spalten hat das Data Frame?\nWelche Spalten könnte man eventuell noch in einen passenderen Typ umwandeln?\n\n\n\n\n\n\n\nTipp\n\n\n\nEs ist wichtig, dass R beim Einlesen der Daten fehlende Werte korrekt erkennt. Stellen Sie durch Setzen des entsprechenden Argumentes der Funktion read_delim() sicher, dass diese fehlenden Werte richtig eingelesen werden (und somit korrekt als NA interpretiert werden)! Es kommen sowohl \"?\" als auch \"\" als fehlende Werte in der Datei vor.\nDie korrekt importierten Daten sollten in einem Tibble mit den folgenden Spaltentypen vorhanden sein: eine Spalte chr, sieben Spalten dbl und eine Spalte time.\n\n\n\n\nÜbung 6\nGegeben sei der folgende numerische Vektor x:\nx = c(1, 2, 3)\nWas passiert, wenn Sie diesen Vektor in einen Faktor konvertieren möchten, aber nur die Stufen 1 und 2 (mit den Labels \"one\" und \"two\") verwenden möchten? Wie wird der Wert 3 behandelt?"
  },
  {
    "objectID": "07/07-solutions.html",
    "href": "07/07-solutions.html",
    "title": "7 – Lösungen",
    "section": "",
    "text": "library(tidyr)\n\npivot_wider(table2, names_from=type, values_from=count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "07/07-solutions.html#exercise-1",
    "href": "07/07-solutions.html#exercise-1",
    "title": "7 – Lösungen",
    "section": "",
    "text": "library(tidyr)\n\npivot_wider(table2, names_from=type, values_from=count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "07/07-solutions.html#exercise-2",
    "href": "07/07-solutions.html#exercise-2",
    "title": "7 – Lösungen",
    "section": "Exercise 2",
    "text": "Exercise 2\n\npivot_longer(table4a, `1999`:`2000`, names_to=\"year\", values_to=\"count\")\n\n# A tibble: 6 × 3\n  country     year   count\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766"
  },
  {
    "objectID": "07/07-solutions.html#exercise-3",
    "href": "07/07-solutions.html#exercise-3",
    "title": "7 – Lösungen",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nmtcars1 = mtcars |&gt;\n    subset(mpg &gt; 25)\n\nnrow(mtcars)  # 32 Zeilen\n\n[1] 32\n\nnrow(mtcars1)  # 6 Zeilen\n\n[1] 6"
  },
  {
    "objectID": "07/07-solutions.html#exercise-4",
    "href": "07/07-solutions.html#exercise-4",
    "title": "7 – Lösungen",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nlibrary(nycflights13)\nlibrary(tibble)\n\nflights |&gt;\n    subset(day == 1 & month == 1) |&gt;  # 842 Zeilen\n    subset(select=c(year:dep_time, arr_time, tailnum))\n\n# A tibble: 842 × 6\n    year month   day dep_time arr_time tailnum\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;  \n 1  2013     1     1      517      830 N14228 \n 2  2013     1     1      533      850 N24211 \n 3  2013     1     1      542      923 N619AA \n 4  2013     1     1      544     1004 N804JB \n 5  2013     1     1      554      812 N668DN \n 6  2013     1     1      554      740 N39463 \n 7  2013     1     1      555      913 N516JB \n 8  2013     1     1      557      709 N829AS \n 9  2013     1     1      557      838 N593JB \n10  2013     1     1      558      753 N3ALAA \n# ℹ 832 more rows\n\nflights |&gt;\n    transform(hours=air_time / 60) |&gt;\n    transform(km=distance * 1.60934) |&gt;\n    transform(speed=km / hours) |&gt;\n    subset(select=c(month, day, carrier, tailnum, speed)) |&gt;\n    as_tibble()\n\n# A tibble: 336,776 × 5\n   month   day carrier tailnum speed\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1     1     1 UA      N14228   596.\n 2     1     1 UA      N24211   602.\n 3     1     1 AA      N619AA   657.\n 4     1     1 B6      N804JB   832.\n 5     1     1 DL      N668DN   634.\n 6     1     1 UA      N39463   463.\n 7     1     1 B6      N516JB   651.\n 8     1     1 EV      N829AS   417.\n 9     1     1 B6      N593JB   651.\n10     1     1 AA      N3ALAA   513.\n# ℹ 336,766 more rows\n\nflights |&gt;\n    subset(dep_time &lt; 600)  # 8730 Zeilen\n\n# A tibble: 8,730 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; \n 1  2013     1     1      517            515         2      830            819        11 UA        1545 N14228  EWR   \n 2  2013     1     1      533            529         4      850            830        20 UA        1714 N24211  LGA   \n 3  2013     1     1      542            540         2      923            850        33 AA        1141 N619AA  JFK   \n 4  2013     1     1      544            545        -1     1004           1022       -18 B6         725 N804JB  JFK   \n 5  2013     1     1      554            600        -6      812            837       -25 DL         461 N668DN  LGA   \n 6  2013     1     1      554            558        -4      740            728        12 UA        1696 N39463  EWR   \n 7  2013     1     1      555            600        -5      913            854        19 B6         507 N516JB  EWR   \n 8  2013     1     1      557            600        -3      709            723       -14 EV        5708 N829AS  LGA   \n 9  2013     1     1      557            600        -3      838            846        -8 B6          79 N593JB  JFK   \n10  2013     1     1      558            600        -2      753            745         8 AA         301 N3ALAA  LGA   \n# ℹ 8,720 more rows\n# ℹ 6 more variables: dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nflights |&gt;\n    subset(arr_delay &lt; dep_delay)  # 221565 Zeilen\n\n# A tibble: 221,565 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; \n 1  2013     1     1      544            545        -1     1004           1022       -18 B6         725 N804JB  JFK   \n 2  2013     1     1      554            600        -6      812            837       -25 DL         461 N668DN  LGA   \n 3  2013     1     1      557            600        -3      709            723       -14 EV        5708 N829AS  LGA   \n 4  2013     1     1      557            600        -3      838            846        -8 B6          79 N593JB  JFK   \n 5  2013     1     1      558            600        -2      853            856        -3 B6          71 N657JB  JFK   \n 6  2013     1     1      558            600        -2      923            937       -14 UA        1124 N53441  EWR   \n 7  2013     1     1      559            559         0      702            706        -4 B6        1806 N708JB  JFK   \n 8  2013     1     1      559            600        -1      854            902        -8 UA        1187 N76515  EWR   \n 9  2013     1     1      600            600         0      851            858        -7 B6         371 N595JB  LGA   \n10  2013     1     1      601            600         1      844            850        -6 B6         343 N644JB  EWR   \n# ℹ 221,555 more rows\n# ℹ 6 more variables: dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistische Datenanalyse mit R",
    "section": "",
    "text": "Die folgenden Unterlagen dienen als Basis für die Lehrveranstaltung Statistische Datenanalyse mit R.\n\nOrganisatorisches\nGrundlagen (Lösungen)\nDie R-Umgebung (Lösungen)\nVektoren (Lösungen)\nTabellen (Lösungen)\nDaten importieren (Lösungen)\nDeskriptive Statistiken (Lösungen)\nDatenaufbereitung (Lösungen)\nGrafiken (Lösungen)\nKorrelation (Lösungen)\nLineare Regression (1) (Lösungen)\nLineare Regression (2) (Lösungen)\nMittelwertvergleich (Lösungen)\n\n\nZusätzliche Übungsaufgaben (Lösungen)"
  },
  {
    "objectID": "a1/a1.html",
    "href": "a1/a1.html",
    "title": "Übungen",
    "section": "",
    "text": "Wir betrachten das Data Frame penguins aus dem Paket palmerpenguins. Untersuchen Sie, ob sich die drei Spezies (Adélie, Chinstrap, Gentoo) in Bezug auf die Körpermasse (Spalte body_mass_g) unterscheiden. Führen Sie dazu folgende Schritte durch:\n\nEntfernen Sie gegebenenfalls Zeilen mit fehlenden Werten in der Spalte body_mass_g.\nBerechnen Sie die Mittelwerte und Standardabweichungen der Körpermasse für jede Spezies.\nFügen Sie dem Data Frame eine Spalte namens id hinzu, welche die Zeilennummer enthält (beginnend bei 1). Diese Spalte identifiziert jede Beobachtung (jeden Pinguin) eindeutig. Die Funktion nrow() könnte dabei hilfreich sein.\nInstallieren und aktivieren Sie das Paket afex. Führen Sie eine einfaktorielle ANOVA durch, um zu überprüfen, ob sich die Körpermasse zwischen den Spezies unterscheidet (verwenden Sie dazu die Funktion aov_ez()). Unterscheiden sich die Spezies signifikant in Bezug auf die Körpermasse?"
  },
  {
    "objectID": "a1/a1.html#übung-1",
    "href": "a1/a1.html#übung-1",
    "title": "Übungen",
    "section": "",
    "text": "Wir betrachten das Data Frame penguins aus dem Paket palmerpenguins. Untersuchen Sie, ob sich die drei Spezies (Adélie, Chinstrap, Gentoo) in Bezug auf die Körpermasse (Spalte body_mass_g) unterscheiden. Führen Sie dazu folgende Schritte durch:\n\nEntfernen Sie gegebenenfalls Zeilen mit fehlenden Werten in der Spalte body_mass_g.\nBerechnen Sie die Mittelwerte und Standardabweichungen der Körpermasse für jede Spezies.\nFügen Sie dem Data Frame eine Spalte namens id hinzu, welche die Zeilennummer enthält (beginnend bei 1). Diese Spalte identifiziert jede Beobachtung (jeden Pinguin) eindeutig. Die Funktion nrow() könnte dabei hilfreich sein.\nInstallieren und aktivieren Sie das Paket afex. Führen Sie eine einfaktorielle ANOVA durch, um zu überprüfen, ob sich die Körpermasse zwischen den Spezies unterscheidet (verwenden Sie dazu die Funktion aov_ez()). Unterscheiden sich die Spezies signifikant in Bezug auf die Körpermasse?"
  },
  {
    "objectID": "a1/a1.html#übung-2",
    "href": "a1/a1.html#übung-2",
    "title": "Übungen",
    "section": "Übung 2",
    "text": "Übung 2\nDie Funktion rnorm() erzeugt normalverteilte Zufallszahlen. Erzeugen Sie 1000 Zufallszahlen aus einer Normalverteilung mit einem Mittelwert von 10 und einer Standardabweichung von 2 und speichern Sie diese in in einem Data Frame df mit einer Spalte namens x. Fügen Sie eine weitere Spalte y hinzu, welche zufällige Werte aus dem Vektor LETTERS enthält; verwenden Sie dazu die Funktion sample() mit dem Argument replace=TRUE. Fügen Sie schließlich eine dritte Spalte namens z hinzu, welche die Quadrate der Werte in der Spalte x enthält.\n\n\n\n\n\n\nTipp\n\n\n\nBevor Sie die Zufallszahlen erzeugen, setzen Sie den Zufallszahlengenerator mit set.seed(123) in einen definierten Ausgangszustand, um reproduzierbare Ergebnisse zu erhalten.\n\n\n\nWie lauten die Mittelwerte der Spalten x und z?\nWie viele Werte in der Spalte y sind gleich \"A\"?\nWie viele Werte in der Spalte z sind größer als 100?\nWie viele Werte in der Spalte y sind gleich \"A\" und deren zugehörige Werte in der Spalte z größer als 100?"
  },
  {
    "objectID": "a1/a1.html#übung-3",
    "href": "a1/a1.html#übung-3",
    "title": "Übungen",
    "section": "Übung 3",
    "text": "Übung 3\nGegeben sei das Data Frame penguins aus dem Paket palmerpenguins. Wie viele weibliche und männliche Pinguine gibt es im Datensatz? Wie viele fehlende Werte gibt es in der Spalte sex? Erzeugen Sie zwei neue Data Frames penguins_f und penguins_m, welche nur die weiblichen bzw. männlichen Pinguine enthalten!"
  },
  {
    "objectID": "a1/a1.html#übung-4",
    "href": "a1/a1.html#übung-4",
    "title": "Übungen",
    "section": "Übung 4",
    "text": "Übung 4\nErzeugen Sie einen Vektor x mit den ungeraden Zahlen von 1 bis 100. Berechnen Sie den Mittelwert, die Standardabweichung, den Median, das 25. Perzentil und das 75. Perzentil von x."
  },
  {
    "objectID": "a1/a1.html#übung-5",
    "href": "a1/a1.html#übung-5",
    "title": "Übungen",
    "section": "Übung 5",
    "text": "Übung 5\n\nErstellen Sie eine Matrix m1, welche aus 25 Zeilen und 5 Spalten besteht. Die fünf Spalten sollen jeweils mit den Zahlen 1 bis 5 gefüllt werden. Berechnen Sie die Zeilen- und Spaltensummen!\nErstellen eine zweite Matrix m2 mit denselben Dimensionen, wobei nun die 25 Zeilen jeweils mit den Zahlen von 1 bis 25 gefüllt werden sollen. Berechnen Sie die Zeilen- und Spaltensummen!"
  },
  {
    "objectID": "a1/a1.html#übung-6",
    "href": "a1/a1.html#übung-6",
    "title": "Übungen",
    "section": "Übung 6",
    "text": "Übung 6\nDie offiziellen Daten zu den österreichischen Treibhausgasemissionen sind hier verfügbar. Laden Sie diesen Datensatz (im Long-Format) herunter bzw. verwenden Sie die Datei thg-emissionen_1990-2022_nach_crf_long.csv.\nImportieren Sie diesen Datensatz in R und beantworten Sie folgende Fragen:\n\nWie viele verschiedene Schadstoffe werden in diesem Datensatz erfasst?\nWie viele verschiedene CRF-Codes gibt es (CRF bedeutet Common Reporting Format und definiert u.a. die Aufteilung der Emissionen in verursachende Sektoren)?\nErstellen Sie ein neues Data Frame df, welches nur die CO₂-Emissionen über alle Sektoren (entspricht einem CRF-Code von 0) enthält."
  },
  {
    "objectID": "a1/a1.html#übung-7",
    "href": "a1/a1.html#übung-7",
    "title": "Übungen",
    "section": "Übung 7",
    "text": "Übung 7\nDer eingebaute Datensatz InsectSprays enthält 72 Beobachtungen von zwei Variablen und dokumentiert die Wirksamkeit von sechs verschiedenen Insektiziden, die in der Spalte spray identifiziert werden. Die Spalte count enthält die Anzahl der Insekten, die in einer standardisierten Testeinheit gefunden wurden – weniger Insekten deuten auf eine höhere Wirksamkeit hin.\nVerwenden Sie die Funktion by(), um die durchschnittliche Anzahl der Insekten für jedes der sechs Insektizide zu berechnen. Erstellen Sie ein Boxplot, um die Verteilung der Insektenanzahl für jedes Insektizid zu visualisieren. Beschriften Sie die Achsen und geben Sie der Grafik einen aussagekräftigen Titel."
  },
  {
    "objectID": "a1/a1.html#übung-8",
    "href": "a1/a1.html#übung-8",
    "title": "Übungen",
    "section": "Übung 8",
    "text": "Übung 8\nDie Datei temperature.csv enthält monatliche Durchschnittstemperaturen (in °C) für Innsbruck im Zeitraum von 1906 bis 2012.\nFügen Sie dem Datensatz eine neue Spalte namens Average hinzu, welche die mittlere Jahrestemperatur beinhaltet (die Funktion rowMeans() kann dabei hilfreich sein).\nErstellen Sie anschließend ein Liniendiagramm, das den Verlauf der durchschnittlichen Jahrestemperatur über den gesamten Zeitraum veranschaulicht. Beschriften Sie die Achsen und geben Sie der Grafik einen Titel."
  },
  {
    "objectID": "a1/a1.html#übung-9",
    "href": "a1/a1.html#übung-9",
    "title": "Übungen",
    "section": "Übung 9",
    "text": "Übung 9\nWie viele Autos im eingebauten mtcars-Datensatz haben eine hohe Kraftstoffeffizienz von mindestens 20 Meilen pro Gallone (Spalte mpg), mindestens 110 PS (Spalte hp) und sechs Zylinder (Spalte cyl)? Erstellen Sie ein neues Data Frame, welches nur die Zeilen enthält, die diese Kriterien erfüllen."
  },
  {
    "objectID": "a1/a1.html#übung-10",
    "href": "a1/a1.html#übung-10",
    "title": "Übungen",
    "section": "Übung 10",
    "text": "Übung 10\nErstellen Sie die folgenden sechs Vektoren a bis f:\n\na besteht aus absteigenden ganzen Zahlen von 100 bis 0 in Schritten von 4 (also 100, 96, 92, …, 4, 0).\nb enthält die Zahlen von 2.5 bis 10 in Schritten von 0.3.\nc besteht aus 35 gleichmäßig verteilten Zahlen im Bereich von 14 bis 15.\nd besteht aus 50 Wiederholungen des Zeichens A, gefolgt von 50 Wiederholungen von B, gefolgt von 50 Wiederholungen von C.\ne enthält die geraden Zahlen von 4 bis 96.\nf enthält die ungeraden Zahlen von 3 bis 97."
  },
  {
    "objectID": "a1/a1.html#übung-11",
    "href": "a1/a1.html#übung-11",
    "title": "Übungen",
    "section": "Übung 11",
    "text": "Übung 11\nDer eingebaute Datensatz attitude enthält Umfragedaten zur Zufriedenheit in verschiedenen Abteilungen eines großen Unternehmens. Verwenden Sie ein lineares Regressionsmodell, um zu untersuchen, ob die erste Spalte (rating) durch die verbleibenden sechs Variablen vorhergesagt werden kann, und beantworten Sie dabei folgende Punkte:\n\nIdentifizieren Sie, welche Variablen signifikante Prädiktoren sind, und geben Sie deren Regressionskoeffizienten an.\nGeben Sie standardisierte Regressionskoeffizienten für alle Prädiktoren an.\nWie hoch ist der Bestimmtheitsmaß \\(R^2\\)?\nIst das Modell statistisch signifikant? Begründen Sie Ihre Antwort."
  },
  {
    "objectID": "a1/a1.html#übung-12",
    "href": "a1/a1.html#übung-12",
    "title": "Übungen",
    "section": "Übung 12",
    "text": "Übung 12\nGegeben sei ein Vektor x = 1:100 und der folgende Ausdruck:\nlog(sqrt(mean(x^2 + 3, trim=0.25)), base=2)\nSchreiben Sie diesen Ausdruck unter Verwendung von drei Pipe-Operatoren um!"
  },
  {
    "objectID": "a1/a1.html#übung-13",
    "href": "a1/a1.html#übung-13",
    "title": "Übungen",
    "section": "Übung 13",
    "text": "Übung 13\nGegeben sei das folgende Data Frame:\n\nset.seed(1)\ndf = data.frame(\n    a=sample(0:1000, 20, replace=TRUE),\n    b=sample(0:1000, 20, replace=FALSE)\n)\n\nBerechnen Sie zunächst die Spaltenmittelwerte für beide Spalten in diesem Data Frame. Berechnen Sie anschließend die Spaltenmittelwerte erneut, diesmal jedoch nur für die Zeilen, in denen die Werte in der Spalte a ungerade sind."
  },
  {
    "objectID": "a1/a1.html#übung-14",
    "href": "a1/a1.html#übung-14",
    "title": "Übungen",
    "section": "Übung 14",
    "text": "Übung 14\nErstellen Sie einen Character-Vektor x mit den folgenden sieben Elementen: 1, 9, X, 13, Y, 8 und 27. Konvertieren Sie diesen Vektor mit einer geeigneten Funktion in einen numerischen Vektor y. Verwenden Sie dann sum(), um die Summe aller Elemente in y zu berechnen, wobei fehlende Werte ignoriert werden sollen. Erstellen Sie schließlich ein Data Frame df aus den beiden Vektoren x und y und fügen Sie eine dritte Spalte namens z hinzu, die nur Einsen (als numerische Werte) enthält."
  },
  {
    "objectID": "a1/a1.html#übung-15",
    "href": "a1/a1.html#übung-15",
    "title": "Übungen",
    "section": "Übung 15",
    "text": "Übung 15\nBerechnen Sie das Ergebnis des folgenden Ausdrucks und weisen Sie es der Variablen result zu:\n\\[\\sqrt{\\left(\\frac{25 + 7.1}{5 \\pi}\\right)^4 + \\frac{(-6.2 \\cdot \\frac{2}{3})^2}{0.8 + 3^{\\frac{2}{7}}}}\\]\nVerwenden Sie die Funktion round(), um das Ergebnis auf drei Dezimalstellen gerundet auszugeben."
  },
  {
    "objectID": "a1/a1.html#übung-16",
    "href": "a1/a1.html#übung-16",
    "title": "Übungen",
    "section": "Übung 16",
    "text": "Übung 16\nGegeben seien die folgenden beiden Vektoren x und y:\n\nset.seed(4)\nx = rnorm(100, mean=-4, sd=5)\ny = 0.29 * x + 2 * rnorm(100, mean=2, sd=2)\n\nErstellen Sie ein Streudiagramm der beiden Vektoren einschließlich der Regressionsgeraden. Berechnen Sie die Pearson-Korrelation zwischen x und y und geben Sie an, ob diese Korrelation signifikant ist (bei einem Signifikanzniveau von \\(\\alpha = 0.05\\)). Begründen Sie Ihre Antwort, indem Sie die Faktoren erklären, die die statistische Signifikanz in diesem Zusammenhang bestimmen."
  },
  {
    "objectID": "a1/a1.html#übung-17",
    "href": "a1/a1.html#übung-17",
    "title": "Übungen",
    "section": "Übung 17",
    "text": "Übung 17\nImportieren Sie die Daten aus der Datei inflation.csv und erstellen Sie ein Diagramm, das die zeitliche Entwicklung der Inflation in Österreich (AT), Deutschland (DE) und der Schweiz (CH) zeigt. Das Diagramm sollte unterschiedliche Farben für die drei Länder verwenden, eine Legende in der oberen linken Ecke enthalten, klar beschriftete Achsen haben und fehlende Werte korrekt behandeln.\n\n\n\n\n\n\nHinweis\n\n\n\nFehlende Werte sind in der Datei als : kodiert. Damit die Legende farbige Linien anzeigt, müssen Sie das Argument lty=1 im Funktionsaufruf von legend() verwenden."
  },
  {
    "objectID": "a1/a1.html#übung-18",
    "href": "a1/a1.html#übung-18",
    "title": "Übungen",
    "section": "Übung 18",
    "text": "Übung 18\nGegeben sei der folgende Vektor x:\n\nset.seed(4)\nx = sample(-10:10, 500, replace=TRUE)\n\nErstellen Sie einen neuen Vektor y, indem Sie nur die ungeraden Werte von x verwenden. Wie viele Elemente hat der ursprüngliche Vektor x und wie viele Elemente hat der neue Vektor y? Welcher der beiden Vektoren hat einen größeren Mittelwert?"
  },
  {
    "objectID": "a1/a1.html#übung-19",
    "href": "a1/a1.html#übung-19",
    "title": "Übungen",
    "section": "Übung 19",
    "text": "Übung 19\nBerechnen Sie den Mittelwert, die Standardabweichung und die Quantile, die den Wahrscheinlichkeiten 0, 0.25, 0.33, 0.5, 0.66, 0.75 und 1 des folgenden Vektors x entsprechen:\n\nset.seed(4)\nx = sample(c(-10:10, NA), 500, replace=TRUE)"
  },
  {
    "objectID": "a1/a1.html#übung-20",
    "href": "a1/a1.html#übung-20",
    "title": "Übungen",
    "section": "Übung 20",
    "text": "Übung 20\nImportieren Sie die Datei winequality-white.csv in R. Dieser Datensatz enthält verschiedene chemische Messungen für verschiedene Arten von Weißweinen.\nWie viele Zeilen und Spalten hat dieser Datensatz? Verwenden Sie die Funktion describe() aus dem Paket psych, um verschiedene deskriptive Statistiken für alle Spalten anzuzeigen. Beantworten Sie anschließend basierend auf dieser Ausgabe folgende Fragen:\n\nWie hoch ist der durchschnittliche Alkoholgehalt (Spalte alcohol)?\nWie hoch ist der Median der Spalte free sulfur dioxide?\nWie groß ist die Differenz zwischen dem Maximum und Minimum der Spalte pH?\n\nBerechnen Sie außerdem den Mittelwert der Spalte pH separat für jede Stufe der Spalte quality."
  },
  {
    "objectID": "a1/a1.html#übung-21",
    "href": "a1/a1.html#übung-21",
    "title": "Übungen",
    "section": "Übung 21",
    "text": "Übung 21\nGegeben sei das folgende Data Frame df:\n\nset.seed(4)\ndf = data.frame(\n    A=sample(-1000:1000, 500, replace=TRUE),\n    B=runif(500, -1, 10)\n)\n\nZählen Sie die Zeilen in df, in denen die Spalte A negative Werte aufweist. Berechnen Sie anschließend den Mittelwert und die Standardabweichung der Spalte B für diese Zeilen."
  },
  {
    "objectID": "a1/a1.html#übung-22",
    "href": "a1/a1.html#übung-22",
    "title": "Übungen",
    "section": "Übung 22",
    "text": "Übung 22\nGegeben sei der folgende Vektor x:\n\nset.seed(4)\nx = sample(-10:10, 500, replace=TRUE)\n\nErstellen Sie die folgenden vier Vektoren durch Indizieren aus x:\n\ny1 soll alle Werte von x enthalten, außer den Elementen an den Positionen 20 und 37.\ny2 soll alle Werte von x enthalten, die größer als 4 sind.\ny3 soll die ersten 25 Werte von x enthalten.\ny4 soll die Elemente von x an den Positionen 2, 128 und 37 enthalten (in dieser Reihenfolge)."
  },
  {
    "objectID": "a1/a1.html#übung-23",
    "href": "a1/a1.html#übung-23",
    "title": "Übungen",
    "section": "Übung 23",
    "text": "Übung 23\nGegeben sei der folgende Vektor x:\n\nset.seed(4)\nx = sample(-10:10, 500, replace=TRUE)\n\nErstellen Sie neue Vektoren x_odd und x_even, die die ungeraden bzw. geraden Elemente von x enthalten. Wie viele Elemente enthalten diese Vektoren?"
  },
  {
    "objectID": "a1/a1.html#übung-24",
    "href": "a1/a1.html#übung-24",
    "title": "Übungen",
    "section": "Übung 24",
    "text": "Übung 24\nDer eingebaute Datensatz sleep enthält Daten über die Wirkung von zwei Schlafmitteln bei 10 Patienten (gemessen als Zunahme der Schlafdauer im Vergleich zu einer Kontrollgruppe). Verwenden Sie einen geeigneten t-Test, um die Wirkungen der beiden Medikamente zu vergleichen. Fassen Sie die Ergebnisse zusammen (Teststatistik und p-Wert) und ziehen Sie eine Schlussfolgerung unter Verwendung eines Signifikanzniveaus von \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "a1/a1.html#übung-25",
    "href": "a1/a1.html#übung-25",
    "title": "Übungen",
    "section": "Übung 25",
    "text": "Übung 25\nGegeben sei ein Vektor x = 1:100 und der folgende Ausdruck:\nsum(log(exp(quantile(x, probs=0.4)), base=10))\nSchreiben Sie diesen Ausdruck unter Verwendung von Pipe-Operatoren um!"
  },
  {
    "objectID": "a1/a1.html#übung-26",
    "href": "a1/a1.html#übung-26",
    "title": "Übungen",
    "section": "Übung 26",
    "text": "Übung 26\nAktivieren Sie das Paket palmerpenguins und erstellen Sie ein neues Data Frame basierend auf dem penguins-Datensatz, das nur die Gentoo-Spezies enthält und nur die zwei Spalten island und sex sowie eine neu zu erstellende Spalte mass umfasst – diese soll die Körpermasse in kg enthalten, welche aus der gegebenen Spalte body_mass_g (Masse in g) berechnet werden kann. Verwenden Sie den Pipe-Operator |&gt; sowie die Funktionen subset() und transform(), um dieses Problem zu lösen!"
  },
  {
    "objectID": "a1/a1.html#übung-27",
    "href": "a1/a1.html#übung-27",
    "title": "Übungen",
    "section": "Übung 27",
    "text": "Übung 27\nGegeben sei folgendes Data Frame df:\nset.seed(1)\ndf = data.frame(\n    A=sample(0:1000, 20, replace=TRUE),\n    B=sample(0:1000, 20, replace=FALSE)\n)\n\nBerechnen Sie die beiden Spaltenmittelwerte dieses Data Frames.\nBerechnen Sie die Spaltenmittelwerte, wenn Sie nur jene Zeilen berücksichtigen, in denen die Werte in Spalte B im Bereich 250 bis 750 (beides inklusive) liegen.\nBerechnen Sie die Spaltenmittelwerte, wenn Sie nur jene Zeilen berücksichtigen, in denen die Werte in Spalte A größer als in Spalte B sind."
  },
  {
    "objectID": "a1/a1.html#übung-28",
    "href": "a1/a1.html#übung-28",
    "title": "Übungen",
    "section": "Übung 28",
    "text": "Übung 28\nDie Excel-Datei luis-daten.xls enthält die Feinstaubkonzentration für die Stadt Graz von Februar 2005 bis Juni 2024. Auf zwei separaten Arbeitsblättern finden sich Daten von zwei verschiedenen Messstationen Graz Don Bosco (Graz-DB) und Graz Süd (Graz-S).\n\nImportieren Sie die Daten von Graz-DB und Graz-S in zwei separate Data Frames.\nKonvertieren Sie die Spalte Datum in beiden Data Frames in ein Datumsformat.\nStellen Sie für beide Messstationen den Zeitverlauf der Feinstaubkonzentrationen als Liniengrafiken dar. Dabei soll Graz-DB in blau und Graz-S in rot dargestellt werden.\nBerechnen Sie die Pearson-Korrelation zwischen den Feinstaubkonzentrationen der beiden Messstationen und geben Sie an, ob diese Korrelation signifikant ist (bei einem Signifikanzniveau von \\(\\alpha = 0.05\\)).\n\n\n\n\n\n\n\nHinweis\n\n\n\nMit der Funktion lines() kann man eine Liniengrafik zu einer bereits bestehenden Grafik hinzufügen."
  },
  {
    "objectID": "a1/a1.html#übung-29",
    "href": "a1/a1.html#übung-29",
    "title": "Übungen",
    "section": "Übung 29",
    "text": "Übung 29\nDie Datei temperature-graz.csv beinhaltet historische Temperaturjahresmittelwerte in Graz. Bearbeiten Sie folgende Aufgabenstellungen:\n\nImportieren Sie diese Datei in R (beachten Sie, dass fehlende Werte in der Datei als 999.90 codiert sind)!\nErstellen Sie eine Grafik, welche den Temperaturverlauf über die Zeit zeigt (Spalten metANN bzw. YEAR). Stellen Sie die Datenpunkte mit Punkten dar und verbinden Sie die einzelnen Punkte mit Linien. Erstellen Sie passende Achsenbeschriftungen sowie einen aussagekräftigen Titel.\nWie lautet der minimale bzw. maximale Temperaturjahresmittelwert im gesamten Zeitraum?\n\n\n\n\n\n\n\nHinweis\n\n\n\nEine kombinierte Punkte- und Liniengrafik kann mit dem Argument type=\"b\" in der Funktion plot() erstellt werden."
  },
  {
    "objectID": "a1/a1.html#übung-30",
    "href": "a1/a1.html#übung-30",
    "title": "Übungen",
    "section": "Übung 30",
    "text": "Übung 30\nLesen Sie die Datei divorce_margarine.csv in R ein. Dieser Datensatz beinhaltet die Anzahl der Scheidungen pro 1000 Personen im US-Bundesstaat Maine und den Margarineverbrauch pro Kopf und Jahr (in Pfund) im Zeitraum 2000 bis 2009.\nErstellen Sie einen Scatterplot mit einer überlagerten Regressionsgeraden, um den Zusammenhang zwischen diesen beiden Messgrößen darzustellen.\nBerechnen Sie anschließend die Pearson-Korrelation zwischen den beiden Variablen und testen Sie ob diese Korrelation signifikant ist (unter der Annahme eines Signifikanzniveaus von \\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "a1/a1.html#übung-31",
    "href": "a1/a1.html#übung-31",
    "title": "Übungen",
    "section": "Übung 31",
    "text": "Übung 31\nGegeben sei folgendes Data Frame df:\nset.seed(1)\ndf = data.frame(\n    A=rep(c(\"A\", \"B\", \"C\"), each=10),\n    B=sample(0:100, 30, replace=TRUE),\n    X=rep(1:10, 3)\n)\nErzeugen Sie ein neues Data Frame, welches dieselben Daten wie df enthält, aber diese im Wide-Format darstellt. Im ursprünglichen Data Frame df ist die Spalte A die Indikatorspalte, die Spalte B enthält die Werte, und die Spalte X identifiziert die einzelnen Fälle."
  },
  {
    "objectID": "a1/a1.html#übung-32",
    "href": "a1/a1.html#übung-32",
    "title": "Übungen",
    "section": "Übung 32",
    "text": "Übung 32\nAktivieren Sie das Paket dplyr, um den Datensatz starwars verwenden zu können. Es soll der Zusammenhang zwischen Körpergewicht (mass) und Körpergröße (height) untersucht werden.\nEntfernen Sie zunächst alle Datenpunkte mit einer Masse von 1000 kg oder mehr. Erstellen Sie mit den bereinigten Daten ein Streudiagramm inklusive Regressionsgeraden (mass auf der x-Achse und height auf der y-Achse).\nBerechnen Sie dann ein lineares Regressionsmodel mit mass als Prädiktor und height als abhängiger Variable. Beantworten Sie folgende Fragen:\n\nIst die Steigung signifikant?\nIst das Regressionsmodel insgesamt signifikant?\nWelche Modellannahmen könnten hier verletzt sein (betrachten Sie dazu die diagnostischen Plots)?\n\nBegründen Sie Ihre Antworten!"
  },
  {
    "objectID": "a1/a1.html#übung-33",
    "href": "a1/a1.html#übung-33",
    "title": "Übungen",
    "section": "Übung 33",
    "text": "Übung 33\nGegeben sind folgende zwei Vektoren x und y:\nset.seed(4)\nx = rnorm(500, mean=-4, sd=5)\ny = rnorm(500, mean=-2, sd=3) + 0.05 * x\nBerechnen Sie die Spearman-Korrelation zwischen x und y und geben Sie an, ob diese Korrelation signifikant ist (bei \\(\\alpha = 0.01\\))."
  },
  {
    "objectID": "a1/a1.html#übung-34",
    "href": "a1/a1.html#übung-34",
    "title": "Übungen",
    "section": "Übung 34",
    "text": "Übung 34\nBerechnen Sie das Ergebnis des folgenden mathematischen Ausdrucks und weisen Sie das Ergebnis der Variablen result zu:\n\\[ \\frac{e^{2.74} + \\sin(0.55 \\pi + 1.23)}{\\sqrt{1 + 8.3^{1.4}}} \\]\nGeben Sie das Ergebnis mit round() auf drei Dezimalstellen gerundet aus."
  },
  {
    "objectID": "a1/a1.html#übung-35",
    "href": "a1/a1.html#übung-35",
    "title": "Übungen",
    "section": "Übung 35",
    "text": "Übung 35\nBerechnen Sie den Mittelwert, die Standardabweichung und die Quantile zu den Wahrscheinlichkeiten 0, 0.25, 0.33, 0.5, 0.66, 0.75 und 1 des folgenden Vektors x:\n\nset.seed(4)\nx = sample(c(-10:10, NA), 500, replace=TRUE)"
  },
  {
    "objectID": "a1/a1.html#übung-36",
    "href": "a1/a1.html#übung-36",
    "title": "Übungen",
    "section": "Übung 36",
    "text": "Übung 36\nImportieren Sie den Datensatz wine.csv, welcher Eigenschaften verschiedener französischer Weine enthält (diese wurden 1983 erhoben):\n\nYear: Jahrgang des Weins\nPrice: Preis des Weins (relativer Preisindex)\nWinterRain: Niederschlag im Winter (in mm)\nAGST: Durchschnittliche Temperatur während der Wachstumsperiode (in °C)\nHarvestRain: Niederschlag während der Erntezeit (in mm)\nAge: Alter des Weins (in Jahren)\nFrancePop: Bevölkerung Frankreichs (in Millionen)\n\nErstellen Sie ein lineares Regressionsmodell, um den Preis des Weins (Price) anhand der anderen Variablen vorherzusagen. Beachten Sie dabei, dass zwei Variablen perfekt korreliert sind und entfernen Sie daher eine davon aus dem Modell (begründen Sie Ihre Wahl).\nGeben Sie die Zusammenfassung des Modells aus und beantworten Sie folgende Fragen:\n\nWelche Variablen sind statistisch signifikante Prädiktoren?\nWie hoch ist der \\(R^2\\)-Wert des Modells?\nIst das Modell insgesamt statistisch signifikant?\nWelche Variablen haben den größten Einfluss auf den Preis des Weins? Berechnen Sie dazu die standardisierten Koeffizienten und nennen Sie die drei wichtigsten Variablen (geordnet nach Einfluss)."
  },
  {
    "objectID": "a1/a1.html#übung-37",
    "href": "a1/a1.html#übung-37",
    "title": "Übungen",
    "section": "Übung 37",
    "text": "Übung 37\nErzeugen Sie einen Vektor x mit den geraden Zahlen von \\(-76\\) bis 120. Bestimmen Sie die Anzahl der Elemente sowie den Mittelwert, die Standardabweichung, den Median, das 25. Perzentil und das 75. Perzentil von x.\nBestimmen Sie außerdem die Anzahl der Elemente, die größer als der Mittelwert sind."
  },
  {
    "objectID": "a1/a1.html#übung-38",
    "href": "a1/a1.html#übung-38",
    "title": "Übungen",
    "section": "Übung 38",
    "text": "Übung 38\nDer folgende Code importiert einen Datensatz von IMDb (Internet Movie Database):\nlibrary(readr)\ndf = read_tsv(\"imdb-ratings.tsv.zip\")\nDieser Datensatz beinhaltet die folgenden drei Spalten:\n\ntconst (Film-ID)\naverageRating (durchschnittliche Bewertung des Films)\nnumVotes (Anzahl der abgegebenen Bewertungen)\n\nBerechnen Sie eine neue Spalte weightedRating, welche die gewichtete Bewertung eines Films beinhalten soll. Diese gewichtete Bewertung \\(R_w\\) wird wie folgt berechnet:\n\\[ R_w = \\frac{R \\cdot n + \\bar{R} \\cdot v}{n + v} \\]\n\n\\(R\\): Bewertung des Films (Spalte averageRating)\n\\(n\\): Anzahl der Bewertungen des Films (Spalte numVotes)\n\\(\\bar{R}\\): Arithmetisches Mittel aller Bewertungen\n\\(v\\): Das 0.90-Quantil der Anzahl der Bewertungen\n\nWelcher Film hat die höchste gewichtete Bewertung? Sortieren Sie dazu den Datensatz nach weightedRating in absteigender Reihenfolge:\ndf[order(-df$weightedRating), ]"
  },
  {
    "objectID": "a1/a1.html#übung-39",
    "href": "a1/a1.html#übung-39",
    "title": "Übungen",
    "section": "Übung 39",
    "text": "Übung 39\nGegeben sei ein Vektor x:\nset.seed(123)\nx = sample(-100:120, size=100, replace=TRUE)\nSchreiben Sie den folgenden Befehl ohne Verwendung von Pipe-Operatoren an:\nx |&gt; unique() |&gt; mean(trim=0.25) |&gt; round(2)"
  },
  {
    "objectID": "01/01.html",
    "href": "01/01.html",
    "title": "1 – Grundlagen",
    "section": "",
    "text": "In dieser Lehrveranstaltung werden die Grundlagen der statistischen Programmierumgebung R vermittelt. In den ersten Einheiten werden wichtige Eigenschaften und Datentypen eingeführt, und erst danach werden elementare Konzepte der Datenanalyse (Importieren von Daten, deskriptive Statistiken, Erzeugen von Grafiken) sowie einfache statistische Modelle (Korrelation, lineare Regression, Mittelwertvergleich) beschrieben."
  },
  {
    "objectID": "01/01.html#überblick",
    "href": "01/01.html#überblick",
    "title": "1 – Grundlagen",
    "section": "",
    "text": "In dieser Lehrveranstaltung werden die Grundlagen der statistischen Programmierumgebung R vermittelt. In den ersten Einheiten werden wichtige Eigenschaften und Datentypen eingeführt, und erst danach werden elementare Konzepte der Datenanalyse (Importieren von Daten, deskriptive Statistiken, Erzeugen von Grafiken) sowie einfache statistische Modelle (Korrelation, lineare Regression, Mittelwertvergleich) beschrieben."
  },
  {
    "objectID": "01/01.html#was-ist-r",
    "href": "01/01.html#was-ist-r",
    "title": "1 – Grundlagen",
    "section": "Was ist R?",
    "text": "Was ist R?\nR ist eine auf Datenanalyse und Statistik spezialisierte Programmiersprache. Mit spezialisierten Programmiersprachen kann man manche Dinge schneller, besser, eleganter oder kürzer lösen als mit universellen Programmiersprachen. Im Gegensatz dazu ist man mit universellen Programmiersprachen (wie beispielsweise Python) aber nicht auf ein relativ spezifisches Gebiet eingeschränkt. Mit Python kann man nicht nur Datenanalysen, numerische Berechnungen und statistische Auswertungen durchführen, sondern auch Webanwendungen oder interaktive Programme mit grafischen Oberflächen relativ einfach erstellen – etwas, das mit R eher schwierig oder umständlich ist.\n\n\n\n\n\n\nTipp\n\n\n\nEs ist wichtig, für ein gegebenes Problem eine geeignete Programmiersprache zu finden. Wenn das Problem den Bereichen Datenanalyse oder statistische Modellierung zugeordnet werden kann, dann ist R sehr wahrscheinlich eine ausgezeichnete Wahl.\n\n\nR kann man wie folgt charakterisieren:\n\nOpen Source (nicht nur kostenlos, sondern offen einsehbarer Quellcode)\nPlattformübergreifend (läuft auf Windows, macOS und Linux)\nRelativ einfach zu erlernen (zumindest die Grundlagen für die Anwendung)\nEinfaches Arbeiten mit komplexen Datenstrukturen\nUmfangreiche Hilfe und Dokumentation\nGroße und hilfsbereite Community\nRiesige Anzahl an Zusatzpaketen verfügbar"
  },
  {
    "objectID": "01/01.html#popularität",
    "href": "01/01.html#popularität",
    "title": "1 – Grundlagen",
    "section": "Popularität",
    "text": "Popularität\nR ist mittlerweile sehr weit verbreitet. Auch in verschiedenen Beliebtheitsranglisten zählt R regelmäßig zu den populärsten Programmiersprachen (z.B. PYPL, IEEE Spectrum Top Programming Languages und TIOBE). Das ist besonders bemerkenswert, da R keine universelle Sprache ist und damit mit Abstand die beliebteste statistische Programmiersprache, weit vor kommerziellen Statistikpaketen wie SPSS, Stata und Statistica.\nIn der Praxis ist die Popularität einer Programmiersprache durchaus relevant, denn je größer und aktiver die Community einer Sprache ist, desto einfacher wird es, bestehende Lösungen für Probleme zu finden oder Antworten auf neue Fragen zu bekommen."
  },
  {
    "objectID": "01/01.html#wie-sieht-r-code-aus",
    "href": "01/01.html#wie-sieht-r-code-aus",
    "title": "1 – Grundlagen",
    "section": "Wie sieht R-Code aus?",
    "text": "Wie sieht R-Code aus?\nR bedient man, indem man Textbefehle in der sogenannten Console eingibt. Statistische Berechnungen werden daher nicht wie z.B. in SPSS üblich durch Navigieren in einer grafischen Oberfläche durchgeführt. Zwar gibt es in SPSS ebenfalls die Möglichkeit, eine sogenannte Syntax zu verwenden, um Analysen per Code auszuführen. Im Gegensatz zu R ist dies in SPSS jedoch optional – die meisten Anwenderinnen und Anwender nutzen ausschließlich die grafische Oberfläche und verzichten auf die Syntax. In R hingegen ist die Arbeit mit Code die einzige Möglichkeit, Analysen durchzuführen. Dies ist aber keineswegs ein Nachteil, sondern hat ganz im Gegenteil mehrere Vorteile – insbesondere wird die Datenanalyse dadurch automatisch reproduzierbar, weil diese vollständig durch die verwendeten Befehle definiert ist.\nTypische Ein- und Ausgaben in R sehen beispielsweise wie folgt aus:\n\nx = c(1, 3, 8, 12, 13, 27)\nmean(x)\n\n[1] 10.66667\n\nsd(x)\n\n[1] 9.309493\n\nset.seed(1)\ny = -0.1 * x + rnorm(6, mean=-10, sd=4)\ncor.test(x, y)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = -0.66124, df = 4, p-value = 0.5446\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8969629  0.6677722\nsample estimates:\n       cor \n-0.3139076 \n\nplot(x, y, pch=20)\n\n\n\n\n\n\n\n\nIn diesen Unterlagen werden R-Befehle in grauen Kästchen gezeigt, während direkt darunter die Ergebnisse der Befehle folgen. Im Moment sind die gezeigten Beispiele vielleicht noch vollkommen unverständlich, aber im Laufe dieser Lehrveranstaltung werden wir all das (und noch einiges mehr) kennenlernen und verstehen."
  },
  {
    "objectID": "01/01.html#installation",
    "href": "01/01.html#installation",
    "title": "1 – Grundlagen",
    "section": "Installation",
    "text": "Installation\nBevor wir R verwenden können, müssen wir zwei Programme installieren, nämlich R und RStudio:\n\nAlle relevanten Informationen zur Installation von R sind auf der offiziellen Website zu finden. Hier finden sich detaillierte Anleitungen für jede unterstützte Plattform. Der Download-Link befindet sich in der linken Spalte ganz oben unter “Download” – “CRAN”. Es ist sinnvoll, stets mit der aktuellsten Version von R zu arbeiten.\nObwohl die mit R mitgelieferte Oberfläche relativ komfortabel ist (und man auch komplett ohne grafische Oberfläche auskommen könnte), verwenden sehr viele Personen RStudio, um mit R zu arbeiten. RStudio ist “nur” eine grafische Oberfläche, d.h. es wird eine zuvor installierte R-Umgebung vorausgesetzt. Auch hier ist es ratsam, stets die aktuellste Version zu verwenden."
  },
  {
    "objectID": "01/01.html#erste-schritte",
    "href": "01/01.html#erste-schritte",
    "title": "1 – Grundlagen",
    "section": "Erste Schritte",
    "text": "Erste Schritte\nR kann als umfangreicher Taschenrechner verwendet werden. Um das auszuprobieren, starten Sie jetzt RStudio. Die Eingabe von mathematischen Ausdrücken ist intuitiv, ein sogenannter Prompt (das &gt;-Symbol) in der Console (im linken Bereich des RStudio-Fensters) signalisiert, dass R bereit für Eingaben ist. Nach dem Eingeben eines Befehls und Bestätigen mit Enter wird das Ergebnis der Berechnung in der nächsten Zeile ausgegeben.\nDie Grundrechenarten Addition, Subtraktion, Multiplikation und Division funktionieren wie erwartet:\n\n13 + 7\n\n[1] 20\n\n\n\n1001 - 93\n\n[1] 908\n\n\n\n81 * 76\n\n[1] 6156\n\n\n\n1563 / 43\n\n[1] 36.34884\n\n\n\n\n\n\n\n\nTipp\n\n\n\nAus Gründen der besseren Lesbarkeit sollten Leerzeichen vor und nach dem Operator eingefügt werden, also besser 13 + 7 und nicht 13+7. Für R ist zwar beides korrekt, aber die erste Variante ist leichter lesbar.\n\n\nFür eine Ganzzahl-Division verwendet man den aus drei Zeichen bestehenden Operator %/%:\n\n1563 %/% 43\n\n[1] 36\n\n\nDen Rest einer Division erhält man mit %%:\n\n1563 %% 43\n\n[1] 15\n\n\nPotenzieren kann man mit ^ oder **:\n\n16^2\n\n[1] 256\n\n\n\n16**2\n\n[1] 256\n\n\nR kennt selbstverständlich auch die korrekten Vorrangsregeln bei Verkettung von mehreren Operationen (inklusive Klammersetzung):\n\n(13 + 6) * 8 - 12**2 / (2.5 + 1.6)\n\n[1] 116.878\n\n\nDie Quadratwurzel berechnet man mit sqrt (aus dem englischen square root):\n\nsqrt(144)\n\n[1] 12\n\n\nDie Kreiszahl \\(\\pi \\approx 3.141593\\) ist als pi verfügbar:\n\npi\n\n[1] 3.141593\n\n\nDie Eulersche Zahl \\(e \\approx 2.718282\\) ist nicht unmittelbar verfügbar, kann aber mit Hilfe der Exponentialfunktion eingegeben werden:\n\nexp(1)\n\n[1] 2.718282\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nBeachten Sie, dass R die englische Zahlenschreibweise mit einem Punkt als Dezimaltrennzeichen verwendet und nicht das im deutschen Sprachraum übliche Komma. Kommazahlen müssen daher immer mit einem Punkt eingegeben werden, ganz egal welche Sprache im Betriebssystem eingestellt ist. In diesen Unterlagen wird ebenfalls ein Punkt als Dezimaltrennzeichen verwendet."
  },
  {
    "objectID": "01/01.html#übungen",
    "href": "01/01.html#übungen",
    "title": "1 – Grundlagen",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nInstallieren Sie die neuesten Versionen von R und RStudio auf Ihrem Rechner. Starten Sie dann RStudio und suchen Sie nach einer Möglichkeit, die Versionsnummern sowohl von R als auch von RStudio anzuzeigen. Welche Versionen beider Programme haben Sie installiert? Sind das die aktuellsten Versionen?\n\n\nÜbung 2\nDie Erde kann näherungsweise als Kugel mit einem Radius von 6371 km betrachtet werden. Berechnen Sie damit die Oberfläche der Erde! Die Formel für die Oberfläche \\(A\\) einer Kugel mit Radius \\(r\\) lautet:\n\\[A = 4 \\pi r^2\\]\n\n\nÜbung 3\nGegeben seien folgende Messwerte: 11, 27, 15, 10, 33, 18, 25, 22, 39, 11. Berechnen Sie den arithmetischen sowie den geometrischen Mittelwert (unter Verwendung von Grundrechenarten). Führen Sie die Berechnung mit jeweils einem einzigen Befehl (ohne Zwischenergebnisse) durch.\nDie Formeln für den arithmetischen bzw. geometrischen Mittelwert lauten:\n\\[\\bar x = \\frac{1}{n} \\sum_{i=1}^n x_i\\]\n\\[\\bar x_g = \\sqrt[n]{\\prod_{i=1}^n x_i}\\]\n\n\n\n\n\n\nHinweis\n\n\n\nDie n-te Wurzel kann man auch als Potenz anschreiben, also \\(\\sqrt[n]{x}\\) ist gleichbedeutend mit \\(x^\\frac{1}{n}\\).\n\n\n\n\nÜbung 4\nBerechnen Sie das Ergebnis des folgenden Ausdrucks mit einem Befehl (d.h. in einer Zeile):\n\\[\\sqrt 2 \\cdot \\frac{(5^5 - \\pi) \\cdot 18}{\\left(\\frac{7}{5} + 13.2 \\right) \\cdot 7^\\frac{2}{3}}\\]\n\n\n\n\n\n\nHinweis\n\n\n\nAchten Sie auf die Klammersetzung! Der letzte Term im Nenner \\(7^\\frac{2}{3}\\) lautet “sieben hoch zwei Drittel”.\n\n\n\n\nÜbung 5\nWarum funktioniert folgender Code zur Berechnung des Umfangs eines Kreises mit einem Radius von \\(r = 1\\) nicht?\n\n2 * 1 * PI"
  },
  {
    "objectID": "10/10-solutions.html",
    "href": "10/10-solutions.html",
    "title": "10 – Lösungen",
    "section": "",
    "text": "library(ggplot2)\nlibrary(readr)\n\n(cars = read_csv(\"cars.csv\"))\n\n# A tibble: 50 × 2\n   speed  dist\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     4     2\n 2     4    10\n 3     7     4\n 4     7    22\n 5     8    16\n 6     9    10\n 7    10    18\n 8    10    26\n 9    10    34\n10    11    17\n# ℹ 40 more rows\n\nplot(\n    x=cars$speed,\n    y=cars$dist,\n    pch=21,\n    bg=rgb(0, 0, 0, 0.5),\n    xlab=\"Speed (miles per hour)\",\n    ylab=\"Distance (feet)\"\n)\nabline(lm(dist ~ speed, data=cars), col=\"blue\", lwd=2)"
  },
  {
    "objectID": "10/10-solutions.html#übung-1",
    "href": "10/10-solutions.html#übung-1",
    "title": "10 – Lösungen",
    "section": "",
    "text": "library(ggplot2)\nlibrary(readr)\n\n(cars = read_csv(\"cars.csv\"))\n\n# A tibble: 50 × 2\n   speed  dist\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     4     2\n 2     4    10\n 3     7     4\n 4     7    22\n 5     8    16\n 6     9    10\n 7    10    18\n 8    10    26\n 9    10    34\n10    11    17\n# ℹ 40 more rows\n\nplot(\n    x=cars$speed,\n    y=cars$dist,\n    pch=21,\n    bg=rgb(0, 0, 0, 0.5),\n    xlab=\"Speed (miles per hour)\",\n    ylab=\"Distance (feet)\"\n)\nabline(lm(dist ~ speed, data=cars), col=\"blue\", lwd=2)"
  },
  {
    "objectID": "10/10-solutions.html#übung-2",
    "href": "10/10-solutions.html#übung-2",
    "title": "10 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nmodel = lm(dist ~ speed, data=cars)\nsummary(model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12"
  },
  {
    "objectID": "10/10-solutions.html#übung-3",
    "href": "10/10-solutions.html#übung-3",
    "title": "10 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nr = with(cars, cor(speed, dist))\nr**2  # identical to R² from linear regression\n\n[1] 0.6510794\n\nsummary(model)$r.squared\n\n[1] 0.6510794"
  },
  {
    "objectID": "10/10-solutions.html#übung-4",
    "href": "10/10-solutions.html#übung-4",
    "title": "10 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\nDie Geradengleichung lautet:\n\\[y = b_0 + b_1 \\cdot x\\]\nEingesetzt mit den Werten der Koeffizienten:\n\\[y = -17.5791 + 3.9324 \\cdot x\\]\nUnd die Vorhersagen:\n\\[-17.5791 + 3.9324 \\cdot 5 = 2.08\\] \\[-17.5791 + 3.9324 \\cdot 65 = 238.03\\]\n\npredict(model, data.frame(speed=c(5, 65)))\n\n         1          2 \n  2.082949 238.027474"
  },
  {
    "objectID": "08/08-solutions.html",
    "href": "08/08-solutions.html",
    "title": "8 – Lösungen",
    "section": "",
    "text": "library(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\nplot(\n    penguins$bill_length_mm,\n    penguins$bill_depth_mm,\n    xlab=\"Bill length (mm)\",\n    ylab=\"Bill depth (mm)\",\n    pch=19,\n    col=\"orange\"\n)"
  },
  {
    "objectID": "08/08-solutions.html#übung-1",
    "href": "08/08-solutions.html#übung-1",
    "title": "8 – Lösungen",
    "section": "",
    "text": "library(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\nplot(\n    penguins$bill_length_mm,\n    penguins$bill_depth_mm,\n    xlab=\"Bill length (mm)\",\n    ylab=\"Bill depth (mm)\",\n    pch=19,\n    col=\"orange\"\n)"
  },
  {
    "objectID": "08/08-solutions.html#übung-2",
    "href": "08/08-solutions.html#übung-2",
    "title": "8 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nplot(\n    penguins$bill_length_mm,\n    penguins$bill_depth_mm,\n    xlab=\"Bill length (mm)\",\n    ylab=\"Bill depth (mm)\",\n    type=\"n\"\n)\nwith(\n    subset(penguins, species==\"Adelie\"),\n    points(bill_length_mm, bill_depth_mm, col=\"red\", pch=19)\n)\nwith(\n    subset(penguins, species==\"Gentoo\"),\n    points(bill_length_mm, bill_depth_mm, col=\"blue\", pch=19)\n)\nwith(\n    subset(penguins, species==\"Chinstrap\"),\n    points(bill_length_mm, bill_depth_mm, col=\"green\", pch=19)\n)\nlegend(\n    \"bottomright\",\n    pch=19,\n    col=c(\"red\", \"blue\", \"green\"),\n    legend=c(\"Adelie\", \"Gentoo\", \"Chinstrap\")\n)"
  },
  {
    "objectID": "08/08-solutions.html#übung-3",
    "href": "08/08-solutions.html#übung-3",
    "title": "8 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nboxplot(ToothGrowth$len ~ ToothGrowth$supp + ToothGrowth$dose)"
  },
  {
    "objectID": "08/08-solutions.html#übung-4",
    "href": "08/08-solutions.html#übung-4",
    "title": "8 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\nboxplot(mtcars$mpg ~ mtcars$cyl)\n\n\n\n\n\n\n\n\nMit steigender Zylinderanzahl sinkt die Treibstoffeffizienz (d.h. der Treibstoffverbrauch steigt)."
  },
  {
    "objectID": "08/08-solutions.html#übung-5",
    "href": "08/08-solutions.html#übung-5",
    "title": "8 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\nlayout(matrix(c(1, 1, 2, 3), nrow=2, byrow=TRUE))\nboxplot(mtcars$mpg ~ mtcars$cyl)\nplot(mtcars$mpg, mtcars$drat)\nhist(mtcars$mpg)"
  },
  {
    "objectID": "03/03.html",
    "href": "03/03.html",
    "title": "3 – Vektoren",
    "section": "",
    "text": "Der grundlegende (atomare) Datentyp in R ist der Vektor. Ein Vektor ist eine Datenstruktur, welche aus einem oder mehreren Elementen besteht. Demnach ist also auch eine Zahl (ein Skalar) wie z.B. 5 ein Vektor (mit einem Element).\nEin Vektor kann mit der Funktion c() explizit erzeugt werden (steht für combine oder concatenate). Im folgenden Beispiel erzeugen wir einen Vektor mit vier Elementen und weisen ihm den Namen y zu:\n\n(y = c(1, 2, 3.1415, -100))\n\n[1]    1.0000    2.0000    3.1415 -100.0000\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nWenn man in R einen Befehl in Klammern setzt, dann wird der daraus resultierende Wert immer auch in der Console ausgegeben. Normalerweise ist das nämlich bei Zuweisungen nicht der Fall. Das obige Beispiel könnte man also auch in zwei Zeilen schreiben, was aber natürlich mehr Tipparbeit ist:\ny = c(1, 2, 3.1415, -100)\ny\n\n\nDie Länge eines Vektors (also die Anzahl der Elemente des Vektors) kann mit der Funktion length() bestimmt werden:\n\nlength(y)\n\n[1] 4\n\n\nIm folgenden Beispiel erzeugen wir einen Vektor a mit einem einzigen Element – in diesem Fall muss man die Funktion c() also gar nicht verwenden:\n\na = 6\nlength(a)\n\n[1] 1\n\n\nMit c() können also Vektoren beliebiger Längen miteinander kombiniert werden:\n\nc(666, y, 666, c(23, 24))\n\n[1]  666.0000    1.0000    2.0000    3.1415 -100.0000  666.0000   23.0000   24.0000"
  },
  {
    "objectID": "03/03.html#erstellen-von-vektoren",
    "href": "03/03.html#erstellen-von-vektoren",
    "title": "3 – Vektoren",
    "section": "",
    "text": "Der grundlegende (atomare) Datentyp in R ist der Vektor. Ein Vektor ist eine Datenstruktur, welche aus einem oder mehreren Elementen besteht. Demnach ist also auch eine Zahl (ein Skalar) wie z.B. 5 ein Vektor (mit einem Element).\nEin Vektor kann mit der Funktion c() explizit erzeugt werden (steht für combine oder concatenate). Im folgenden Beispiel erzeugen wir einen Vektor mit vier Elementen und weisen ihm den Namen y zu:\n\n(y = c(1, 2, 3.1415, -100))\n\n[1]    1.0000    2.0000    3.1415 -100.0000\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nWenn man in R einen Befehl in Klammern setzt, dann wird der daraus resultierende Wert immer auch in der Console ausgegeben. Normalerweise ist das nämlich bei Zuweisungen nicht der Fall. Das obige Beispiel könnte man also auch in zwei Zeilen schreiben, was aber natürlich mehr Tipparbeit ist:\ny = c(1, 2, 3.1415, -100)\ny\n\n\nDie Länge eines Vektors (also die Anzahl der Elemente des Vektors) kann mit der Funktion length() bestimmt werden:\n\nlength(y)\n\n[1] 4\n\n\nIm folgenden Beispiel erzeugen wir einen Vektor a mit einem einzigen Element – in diesem Fall muss man die Funktion c() also gar nicht verwenden:\n\na = 6\nlength(a)\n\n[1] 1\n\n\nMit c() können also Vektoren beliebiger Längen miteinander kombiniert werden:\n\nc(666, y, 666, c(23, 24))\n\n[1]  666.0000    1.0000    2.0000    3.1415 -100.0000  666.0000   23.0000   24.0000"
  },
  {
    "objectID": "03/03.html#typen-von-vektoren",
    "href": "03/03.html#typen-von-vektoren",
    "title": "3 – Vektoren",
    "section": "Typen von Vektoren",
    "text": "Typen von Vektoren\nEin Vektor besteht immer aus Elementen desselben Typs. Man nennt Vektoren daher homogene Datentypen. Bis jetzt haben wir numerische Vektoren kennengelernt, welche ausschließlich aus Zahlen bestehen. Es gibt aber auch Vektoren, die logische Elemente oder Zeichen/Buchstaben enthalten. Man unterscheidet in R daher grob zwischen folgenden Vektortypen:\n\nNumerische Vektoren\nLogische Vektoren\nZeichenkettenvektoren\n\nIn einer der nächsten Einheiten werden wir dann noch Faktoren als vierten wichtigen Typ kennenlernen (grundsätzlich gibt es in R aber noch viele weitere spezifischere Datentypen).\n\nNumerische Vektoren\nNumerische Vektoren sind vom Typ numeric und beinhalten ausschließlich Zahlen, zum Beispiel:\n\nc(2, 13, 15, 17)\n\n[1]  2 13 15 17\n\n\nMit der Funktion class() kann man den Typ (die Klasse) eines Objektes bestimmen. Beispiele:\n\nclass(2)\n\n[1] \"numeric\"\n\nz = c(1.11, 2.33)\nclass(z)\n\n[1] \"numeric\"\n\nclass(c(3.1, 2.2, 10))\n\n[1] \"numeric\"\n\n\n\n\nLogische Vektoren\nLogische Vektoren bestehen ausschließlich aus den Werten TRUE oder FALSE (achten Sie auf die korrekte Schreibweise). Sie sind vom Typ logical.\n\nclass(TRUE)\n\n[1] \"logical\"\n\nclass(c(FALSE, FALSE, TRUE))\n\n[1] \"logical\"\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nEs ist möglich, die Werte TRUE und FALSE mit T und F abzukürzen. Aufgrund der schlechteren Lesbarkeit sollte man aber auf diese Abkürzungen verzichten.\n\n\nLogische Vektoren entstehen unter anderem durch Vergleiche zweier Vektoren:\n\nx = 5\nclass(x)\n\n[1] \"numeric\"\n\n\nIm folgenden Beispiel möchten wir wissen, ob x kleiner als 1 ist (wir vergleichen also x mit 1):\n\nx &lt; 1\n\n[1] FALSE\n\n\nDieser Vergleich ergibt FALSE, also einen logischen Vektor:\n\nclass(x &lt; 1)\n\n[1] \"logical\"\n\n\nIn R gibt es folgende Vergleichsoperatoren: &gt;, &gt;=, &lt;, &lt;=, == und !=. Vergleiche (bzw. logische Vektoren) können mit | (oder) und & (und) verknüpft und mit ! negiert werden. Gruppierungen durch Klammersetzung sind ebenfalls möglich.\n\n!TRUE\n\n[1] FALSE\n\n!FALSE\n\n[1] TRUE\n\n(3 &gt; 5) & (4 == 4)\n\n[1] FALSE\n\n(TRUE == TRUE) | (TRUE == FALSE)\n\n[1] TRUE\n\n((111 &gt;= 111) | !(TRUE)) & ((4 + 1) == 5)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nDer Gleichheitsoperator lautet == (also zwei Gleichheitszeichen) und nicht = (ein einziges Gleichheitszeichen, dies ist der Zuweisungsoperator).\n\n\n\n\nZeichenkettenvektoren\nZeichenkettenvektoren (Typ character) bestehen aus Zeichen, welche innerhalb von Anführungszeichen eingegeben werden. Es können sowohl einfache ' als auch doppelte \" Anführungszeichen verwendet werden. Eine Zeichenkette kann aus Buchstaben, Ziffern und Sonderzeichen bestehen.\n\n\"Hello!\"\n\n[1] \"Hello!\"\n\n'Hello!'\n\n[1] \"Hello!\"\n\nclass(\"Hello!\")\n\n[1] \"character\"\n\n(s = c(\"What's\", 'your', \"name?\"))\n\n[1] \"What's\" \"your\"   \"name?\" \n\nclass(s)\n\n[1] \"character\"\n\n\nDie Funktion length() gibt die Länge des Vektors (d.h. die Anzahl der Elemente) und nicht die Anzahl der Zeichen einer Zeichenkette zurück. Dafür kann man die Funktion nchar() verwenden:\n\ns = c('Hello!', 'world')\nlength(s)\n\n[1] 2\n\nnchar(s)\n\n[1] 6 5"
  },
  {
    "objectID": "03/03.html#zwang-coercion",
    "href": "03/03.html#zwang-coercion",
    "title": "3 – Vektoren",
    "section": "Zwang (Coercion)",
    "text": "Zwang (Coercion)\nVektoren sind homogene Datentypen, d.h. sie enthalten nur Elemente desselben Typs. Wenn man versucht, einen Vektor mit Elementen mit unterschiedlichen Typen zu erstellen, wird dieser automatisch in einen Typ “gezwungen”, welcher alle Elemente abbilden kann. Wenn man also Zahlen mit Zeichenketten mischt, werden alle Elemente in Zeichenketten umgewandelt (da Zeichenketten im Allgemeinen nicht als Zahlen dargestellt werden können, umgekehrt können aber Zahlen als Zeichenketten sehr wohl dargestellt werden).\n\n(x = c(1, 2.14, \"5\", 6))\n\n[1] \"1\"    \"2.14\" \"5\"    \"6\"   \n\nclass(x)\n\n[1] \"character\"\n\n\nRechnen kann man mit dem Vektor im obigen Beispiel allerdings nicht mehr, da die Elemente nun Zeichenketten und keine Zahlen mehr sind.\nMan kann mit folgenden Funktionen auch explizit eine Umwandlung in einen gewünschten Typ durchführen:\n\nas.numeric()\nas.logical()\nas.character()\n\nFolgendes Beispiel wandelt einen Zeichenketten-Vektor in einen numerischen Vektor um (dies funktioniert, weil im Beispiel alle Zeichenketten als Zahlen interpretiert werden können):\n\nas.numeric(c(\"1\", \"2.12\", \"66\"))\n\n[1]  1.00  2.12 66.00\n\n\nWenn das nicht geht, wird eine Warnung ausgegeben und NA (steht für “Not Available”, also ein fehlender Wert) für das Element, welches sich nicht umwandeln lässt, angenommen:\n\nas.numeric(c(\"1\", \"2.12\", \"X\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 1.00 2.12   NA\n\n\nMehr Details zu fehlenden Werten werden am Ende dieser Einheit behandelt."
  },
  {
    "objectID": "03/03.html#rechnen-mit-vektoren",
    "href": "03/03.html#rechnen-mit-vektoren",
    "title": "3 – Vektoren",
    "section": "Rechnen mit Vektoren",
    "text": "Rechnen mit Vektoren\nMit numerischen Vektoren kann man Rechenoperationen durchführen – diese werden stets elementweise angewendet:\n\n(y = c(1, 2, 3, 4))\n\n[1] 1 2 3 4\n\ny * 100 + 2  # Berechnung wird für jedes der 4 Elemente separat durchgeführt\n\n[1] 102 202 302 402\n\n\nWie wir bereits wissen, gibt es die üblichen Operatoren +, -, *, und / für die Addition, Subtraktion, Multiplikation und Division. Das Zeichen ^ oder ** steht für “hoch” (berechnet also die Potenz von einer Basis zum Exponenten). Der Operator %/% berechnet die ganzzahlige Division und %% berechnet den Rest. Weitere praktische Funktionen sind abs() für den Betrag und sqrt() für die Quadratwurzel einer Zahl. Die Funktionen log() bzw. exp() berechnen den (natürlichen) Logarithmus bzw. die Exponentialfunktion. Mit sin() bzw. cos() kann man den Sinus bzw. Cosinus berechnen. All diese Operatoren und Funktionen werden stets elementweise auf Vektoren angewendet.\n\nRecycling\nWenn zwei Vektoren in einer Berechnung unterschiedlich lang sind, dann wiederholt R die Werte des kürzeren Vektors, sodass dieser dann gleich viele Elemente hat wie der längere Vektor. Man bezeichnet dies als Recycling. Dies ist z.B. auch schon der Fall, wenn man einen Vektor mit vier Elementen mit einem Skalar (Vektor mit einem Element) multipliziert, wie im folgenden Beispiel:\n\nc(1, 2, 3, 4) * 2\n\n[1] 2 4 6 8\n\n\nDer skalare Vektor 2 wird automatisch auf den Vektor c(2, 2, 2, 2) erweitert, daher entspricht die obige Operation eigentlich folgender elementweisen Berechnung:\n\nc(1, 2, 3, 4) * c(2, 2, 2, 2)\n\n[1] 2 4 6 8\n\n\nWeiteres Beispiel:\n\nc(1, 2, 3, 4) + c(0, 10)\n\n[1]  1 12  3 14\n\n\nDer kürzere Vektor c(0, 10) wird verdoppelt und die Berechnung wird elementweise durchgeführt:\n\nc(1, 2, 3, 4) + c(0, 10, 0, 10)\n\n[1]  1 12  3 14\n\n\nWenn sich das Recycling nicht genau ausgeht, d.h. wenn die Länge des längeren Vektors kein ganzzahliges Vielfaches des kürzeren Vektors ist, dann funktioniert das Recycling zwar grundsätzlich trotzdem, aber es wird eine Warnung ausgegeben:\n\nc(1, 2, 3, 4) + c(0, 10, 8)\n\nWarning in c(1, 2, 3, 4) + c(0, 10, 8): longer object length is not a multiple of shorter object length\n\n\n[1]  1 12 11  4\n\n\nDie Berechnung entspricht daher folgender Operation:\n\nc(1, 2, 3, 4) + c(0, 10, 8, 0)\n\n[1]  1 12 11  4"
  },
  {
    "objectID": "03/03.html#erstellen-von-zahlenfolgen",
    "href": "03/03.html#erstellen-von-zahlenfolgen",
    "title": "3 – Vektoren",
    "section": "Erstellen von Zahlenfolgen",
    "text": "Erstellen von Zahlenfolgen\nVektoren mit definierten Zahlenfolgen erstellt man mit : oder mit seq(). Bei der ersten Option ist die Schrittweite immer 1, bei der zweiten Option kann diese beliebig angepasst werden.\n\n1:20\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\npi:10\n\n[1] 3.141593 4.141593 5.141593 6.141593 7.141593 8.141593 9.141593\n\n9:2\n\n[1] 9 8 7 6 5 4 3 2\n\nseq(1, 20)  # äquivalent zu 1:20\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nseq(20, 1)  # äquivalent zu 20:1\n\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1\n\nseq(0, 5, by=0.5)  # Schrittweite 0.5\n\n [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nseq(5, 0, by=-0.5)  # negative Schrittweite notwendig!\n\n [1] 5.0 4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0\n\nseq(0, 20, 2)  # gerade Zahlen\n\n [1]  0  2  4  6  8 10 12 14 16 18 20\n\nseq(1, 20, 2)  # ungerade Zahlen\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\nseq(1, 3, length.out=10)  # Gesamtlänge des Ergebnisses soll 10 sein\n\n [1] 1.000000 1.222222 1.444444 1.666667 1.888889 2.111111 2.333333 2.555556 2.777778 3.000000\n\n\nMit seq() kann man also Zahlenfolgen mit bestimmter Schrittweite (Argument by) oder mit bestimmter Gesamtlänge (Argument length.out) erzeugen.\nDie Funktion rep() wiederholt gegebene Werte:\n\nrep(0, 90)  # erzeuge einen Vektor mit 90 Nullen\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[59] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n\n\n\nTipp\n\n\n\nHier kann man auch die Bedeutung der Werte in eckigen Klammern erkennen, die vor jeder Ausgabezeile stehen: sie geben den Index des ersten Elements der Zeile an (also im Beispiel [1] für die erste Zeile und [59] für die zweite Zeile).\n\n\nBeachten Sie auch die unterschiedlichen Ergebnisse durch Verwendung der Argumente times bzw. each:\n\nrep(c(0, 1, 2), times=10)\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2\n\nrep(c(0, 1, 2), each=10)\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n\nrep(c(0, 1, 2), times=c(10, 10, 10))  # gleiches Ergebnis wie mit each\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2"
  },
  {
    "objectID": "03/03.html#indizieren",
    "href": "03/03.html#indizieren",
    "title": "3 – Vektoren",
    "section": "Indizieren",
    "text": "Indizieren\nVektoren können indiziert werden, d.h. einzelne Elemente können herausgegriffen werden. Im Gegensatz zu vielen anderen Programmiersprachen beginnt R mit 1 zu zählen (d.h. das erste Element entspricht dem Index 1). Man verwendet dazu eckige Klammern, innerhalb derer die gewünschten Elemente angegeben werden.\nBetrachten wir in den folgenden Beispielen den Vektor x, welcher aus 11 Elementen besteht:\n\n(x = seq(10, 110, 10))\n\n [1]  10  20  30  40  50  60  70  80  90 100 110\n\nlength(x)\n\n[1] 11\n\n\nNun erzeugen wir durch Indizieren neue Untermengen des bestehenden Vektors:\n\nx[1]  # 1. Element\n\n[1] 10\n\nx[4]  # 4. Element\n\n[1] 40\n\nx[1:5]  # Elemente 1-5\n\n[1] 10 20 30 40 50\n\nx[c(1, 4, 8)]  # Elemente 1, 4 und 8\n\n[1] 10 40 80\n\n\nNegative Indizes bedeuten “alle Elemente außer die angegebenen”:\n\nx[c(-1, -10)]\n\n[1]  20  30  40  50  60  70  80  90 110\n\nx[-c(1, 10)]\n\n[1]  20  30  40  50  60  70  80  90 110\n\n\nMan kann auch mit logischen Vektoren indizieren. Dazu erstellt man zuerst einen logischen Vektor (z.B. durch einen Vergleich) und verwendet diesen dann als Index innerhalb der eckigen Klammern (dies kann direkt in einem Schritt gemacht werden). Dabei werden jene Elemente herausgegriffen, für die der Indexvektor TRUE ist.\n\nx\n\n [1]  10  20  30  40  50  60  70  80  90 100 110\n\nx[c(TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE)]\n\n[1] 10 30 90\n\n\nDas folgende Beispiel illustriert die Erstellung des logischen Indexvektors durch einen Vergleich:\n\nx &gt; 40\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx[x &gt; 40]\n\n[1]  50  60  70  80  90 100 110\n\n\nIdealerweise ist der logische Indexvektor gleich lang wie der zu indizierende Vektor. Falls der Indexvektor kürzer ist, findet wieder Recycling statt:\n\nx[c(FALSE, TRUE)]\n\n[1]  20  40  60  80 100"
  },
  {
    "objectID": "03/03.html#benannte-vektoren",
    "href": "03/03.html#benannte-vektoren",
    "title": "3 – Vektoren",
    "section": "Benannte Vektoren",
    "text": "Benannte Vektoren\nVektoren können auch Elemente mit Namen enthalten. So kann man auch mit den Elementnamen anstatt mit der Position indizieren.\n\n(vect = c(a=11, b=2, c=NA))  # Argumentnamen werden als Elementnamen verwendet\n\n a  b  c \n11  2 NA \n\nvect[2]\n\nb \n2 \n\nvect[\"b\"]\n\nb \n2 \n\n\nDie Funktion names() gibt die Namen der Elemente zurück:\n\nnames(vect)\n\n[1] \"a\" \"b\" \"c\"\n\n\nMit dieser Funktion kann man die Elementnamen eines Vektors auch nachträglich setzen:\n\nx = 1:3\nnames(x)\n\nNULL\n\nnames(x) = c(\"test\", \"value\", \"x\")\nx\n\n test value     x \n    1     2     3"
  },
  {
    "objectID": "03/03.html#fehlende-werte",
    "href": "03/03.html#fehlende-werte",
    "title": "3 – Vektoren",
    "section": "Fehlende Werte",
    "text": "Fehlende Werte\nIn R können fehlende Werte mit dem speziellen Wert NA (Not Available) codiert werden.\n\n(vect = c(15, 1.12, NA, 12, NA, 33.22))\n\n[1] 15.00  1.12    NA 12.00    NA 33.22\n\n\nMit der Funktion is.na() können die fehlenden Werte bestimmt werden. So kann man alle Werte aus einem Vektor extrahieren, die nicht NA sind.\n\nis.na(vect)  # die fehlenden Werte\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n!is.na(vect)  # die nicht fehlenden Werte\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\nvect[!is.na(vect)]\n\n[1] 15.00  1.12 12.00 33.22\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nUm herauszufinden, welche Elemente eines Vektors fehlen, darf man nicht mit NA vergleichen:\n\nvect == NA\n\n[1] NA NA NA NA NA NA\n\n\nDieser Vergleich ergibt immer NA! Stattdessen sollte man die Funktion is.na() verwenden."
  },
  {
    "objectID": "03/03.html#übungen",
    "href": "03/03.html#übungen",
    "title": "3 – Vektoren",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nBerechnen Sie die Oberfläche (Grundflächen plus Mantelfläche) sowie das Volumen eines Zylinders mit Radius 5 und Höhe 9. Erzeugen Sie dafür die Variablen r und h. Speichern Sie die Ergebnisse in den Variablen A (Fläche) bzw. V (Volumen) ab. Wie lauten die Ergebnisse (also die Werte beider Variablen)?\n\n\nÜbung 2\nErstellen Sie einen Vektor x mit den Elementen 4, 18, -7, 16, 4 und -44. Erstellen Sie danach einen Vektor y, welcher die quadrierten Elemente aus x enthält (nutzen Sie dazu die Eigenschaft, dass R Rechenoperationen elementweise durchführt). Zum Schluss erstellen Sie einen Vektor z, indem Sie x und y aneinanderhängen. Mit welcher Funktion können Sie die Anzahl der Elemente in z bestimmen?\n\n\nÜbung 3\nGegeben sei folgender Vektor:\nx = c(44, 23, -56, 98, 99, 32, 45, 22)\nWelche Elemente aus x sind gerade? Welche Elemente sind ungerade? Erstellen Sie zwei entsprechende logische Vektoren, welche Sie dann zum Indizieren der geraden bzw. ungeraden Elemente von x verwenden können.\n\n\n\n\n\n\nHinweis\n\n\n\nErstellen Sie die logischen Indexvektoren mit einem Vergleich und nicht händisch (d.h. für die geraden Zahlen in diesem Beispiel wäre der gesuchte logische Indexvektor c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE), dies sollen Sie aber nicht so eingeben). Verwenden Sie stattdessen die Eigenschaft, dass gerade Zahlen einen Rest von 0 ergeben wenn man sie durch 2 dividiert. Ungerade Zahlen ergeben hier einen Rest von 1. Verwenden Sie daher den Operator %% für den Rest einer Division. Erstellen Sie dann mit einem Vergleich jeweils einen logischen Indexvektor (für die geraden Zahlen vergleichen Sie ob der Rest 0 ist, für die ungeraden Zahlen ob der Rest 1 ist), welchen Sie dann zum Indizieren verwenden können.\n\n\n\n\nÜbung 4\nErstellen Sie folgende Vektoren und geben Sie sie am Bildschirm aus:\n\nEinen Vektor mit den ganzen Zahlen von 15 bis 40.\nEinen Vektor mit den absteigenden Zahlen von 75 bis 61 in Dreierschritten.\nEinen Vektor bestehend aus genau 35 Zahlen zwischen 14 und 15.\n\n\n\nÜbung 5\nErstellen Sie einen Zeichenketten-Vektor mit folgenden Einträgen: zuerst 10 mal \"Placebo\", dann 10 mal \"Group 1\" und schließlich 10 mal \"Group 2\" (d.h. das Ergebnis soll 30 Elemente haben).\n\n\nÜbung 6\nErstellen Sie einen Vektor k mit den geraden Zahlen von 0 bis 20 (am besten mit der Funktion seq() und der entsprechenden Schrittweite). Geben Sie dann durch Indizieren die folgenden Elemente dieses Vektors am Bildschirm aus:\n\nAlle Elemente bis auf das 3. und 7. Element\nDie ersten fünf Elemente\nDie Elemente 2, 5 und 16 (fällt Ihnen hier etwas auf?)\nAlle Elemente, die größer als 11 sind\n\n\n\nÜbung 7\nErstellen Sie folgenden Vektor:\nt = c(10, 20, NA, 30, 40)\nBerechnen Sie dann mit der Funktion mean() den Mittelwert von t. Was bewirkt der fehlende Wert NA? Sehen Sie in der Hilfe zum Befehl mean() nach, wie Sie fehlende Werte bei der Berechnung ignorieren können (welches optionale Argument müssen Sie setzen?) und führen Sie diese Berechnung durch.\nAlternativ könnten Sie mit is.na() alle fehlenden Werte aus t identifizieren und dann die Funktion mean() auf alle nicht fehlenden Werte aus t anwenden (so wie in den Unterlagen gezeigt).\n\n\nÜbung 8\nGegeben seien folgende Standardabweichungen von fünf Messgrößen in einem Vektor s:\ns = c(1, 11.3, 7.8, 3.4, 6)\nWie können Sie daraus in einem Schritt die fünf Varianzen berechnen?\n\n\nÜbung 9\nGegeben sei folgender Vektor x:\nx = c(2, 0, -5, 0, 1, -1, 0, 3, 0, 0, 7)\nAngenommen, Sie sind an den Werten interessiert, die gleich 0 sind. Wie können Sie diese finden? Wie viele Werte sind das? An welchen Positionen befinden sich diese Werte? Verwenden Sie zur Beantwortung der letzten beiden Fragen die Funktionen sum() bzw. which()!\n\n\nÜbung 10\nAngenommen, Sie möchten das zweite, vierte und sechste Element aus dem Vektor x = 1:10 herausgreifen und y zuweisen. Warum funktioniert das nicht wie im folgenden Beispiel dargestellt?\n\nx = 1:10\n\ny = x[2, 4, 6]\nGeben Sie den korrekten Code an!"
  },
  {
    "objectID": "11/11.html",
    "href": "11/11.html",
    "title": "11 – Lineare Regression (2)",
    "section": "",
    "text": "Die multiple lineare Regression ist eine Erweiterung der einfachen linearen Regression auf Situationen mit mehreren Prädiktoren. Das grundlegende Konzept bleibt aber unverändert, wir verwenden nach wie vor folgende allgemeine Modellgleichung:\n\\[y_i = \\hat{y}_i + \\varepsilon_i\\]\nDas Modell \\(\\hat{y}_i\\) mit \\(n\\) Prädiktoren \\(x_{1}\\), \\(x_{2}\\), \\(\\ldots\\), \\(x_{n}\\) wird nun wie folgt formuliert:\n\\[y_i = (b_0 + b_1 x_{1i} + b_2 x_{2i} + \\ldots + b_n x_{ni}) + \\varepsilon_i\\]\nJeder Prädiktor erhält also ein eigenes Gewicht bzw. einen eigenen Regressionskoeffizienten. Die Koeffizienten werden mittels Least Squares wieder so berechnet, dass die entstehende “Gerade” (technisch spricht man hier von einer Hyperebene) den mittleren quadratischen Fehler minimiert.\nDie Quadratsummen SST, SSM und SSR werden analog wie bei der einfachen Regression berechnet. Wieder kann man \\(R^2\\) berechnen, welches den Anteil der Varianz in der abhängigen Variable angibt, welche durch das Modell erklärt wird. Je höher dieser Wert ist, desto besser kann das Modell die Daten beschreiben. Aus \\(R^2\\) kann man hier die Pearson-Korrelation zwischen den vorhergesagten Werten und den tatsächlich beobachteten Werten berechnen."
  },
  {
    "objectID": "11/11.html#multiple-lineare-regression",
    "href": "11/11.html#multiple-lineare-regression",
    "title": "11 – Lineare Regression (2)",
    "section": "",
    "text": "Die multiple lineare Regression ist eine Erweiterung der einfachen linearen Regression auf Situationen mit mehreren Prädiktoren. Das grundlegende Konzept bleibt aber unverändert, wir verwenden nach wie vor folgende allgemeine Modellgleichung:\n\\[y_i = \\hat{y}_i + \\varepsilon_i\\]\nDas Modell \\(\\hat{y}_i\\) mit \\(n\\) Prädiktoren \\(x_{1}\\), \\(x_{2}\\), \\(\\ldots\\), \\(x_{n}\\) wird nun wie folgt formuliert:\n\\[y_i = (b_0 + b_1 x_{1i} + b_2 x_{2i} + \\ldots + b_n x_{ni}) + \\varepsilon_i\\]\nJeder Prädiktor erhält also ein eigenes Gewicht bzw. einen eigenen Regressionskoeffizienten. Die Koeffizienten werden mittels Least Squares wieder so berechnet, dass die entstehende “Gerade” (technisch spricht man hier von einer Hyperebene) den mittleren quadratischen Fehler minimiert.\nDie Quadratsummen SST, SSM und SSR werden analog wie bei der einfachen Regression berechnet. Wieder kann man \\(R^2\\) berechnen, welches den Anteil der Varianz in der abhängigen Variable angibt, welche durch das Modell erklärt wird. Je höher dieser Wert ist, desto besser kann das Modell die Daten beschreiben. Aus \\(R^2\\) kann man hier die Pearson-Korrelation zwischen den vorhergesagten Werten und den tatsächlich beobachteten Werten berechnen."
  },
  {
    "objectID": "11/11.html#überprüfen-der-datenpunkte",
    "href": "11/11.html#überprüfen-der-datenpunkte",
    "title": "11 – Lineare Regression (2)",
    "section": "Überprüfen der Datenpunkte",
    "text": "Überprüfen der Datenpunkte\nEin gegebenes Modell sollte man immer darauf überprüfen, wie gut es die gemessenen Daten tatsächlich beschreibt. Lineare Modelle können sehr sensitiv auf einzelne Datenpunkte reagieren, wenn diese nicht zum generellen (linearen) Trend der Daten passen. Einen tatsächlichen Einfluss auf das Modell üben solche Ausreißer aber nur aus, wenn sie weit weg vom Mittelwert der Prädiktoren liegen – man bezeichnet diesen potentiellen Einfluss jedes Datenpunkts als Leverage.\nKritisch sind also jene Punkte, welche einen potentiellen hohen Einfluss haben (hohe Leverage) und gleichzeitig nicht zum generellen Trend der Daten passen, also Ausreißer sind. Die folgende Grafik veranschaulicht die drei möglichen Situationen. Dabei sind die vier Ausgangsdatenpunkte schwarz dargestellt und die zugehörige Regressionsgerade ist schwarz strichliert. Der zusätzliche fünfte Datenpunkt ist rot dargestellt, und die Regressionsgerade durch alle fünf Datenpunkte ist ebenfalls rot. Durch einen einzigen zusätzlichen Datenpunkt ändert sich also das ursprüngliche Modell mehr oder weniger stark (von schwarz strichliert auf rot durchgezogen).\n\n\n\n\n\n\n\n\n\nJe weniger Datenpunkte vorhanden sind, desto größer ist der Einfluss von Ausreißern auf das lineare Modell. Die folgende Grafik veranschaulicht die Auswirkung eines Ausreißers für drei verschiedene Stichprobengrößen. Man erkennt, dass ein Ausreißer das Modell nur wenig verändert, wenn sehr viele Datenpunkte vorhanden sind. Wenn es aber nur wenige Datenpunkte gibt, kann ein einziger Ausreißer das Modell stark verändern."
  },
  {
    "objectID": "11/11.html#modellannahmen",
    "href": "11/11.html#modellannahmen",
    "title": "11 – Lineare Regression (2)",
    "section": "Modellannahmen",
    "text": "Modellannahmen\nUm mit einem linearen Regressionsmodell Vorhersagen auf ungesehene Daten machen zu können, müssen folgende Annahmen erfüllt sein:\n\nDie abhängige Variable muss intervallskaliert sein.\nDie unabhängigen Variablen (Prädiktoren) müssen intervallskaliert (oder nominalskaliert in zwei Kategorien) sein.\nDie Prädiktoren müssen Varianzen ungleich 0 haben (sie dürfen also nicht konstant sein).\nEs darf keine Multikollinearität bestehen, d.h. zwei oder mehrere Prädiktoren dürfen nicht linear voneinander abhängig sein. Dies kann z.B. mit der VIF-Statistik (Variance Inflation Factor) überprüft werden.\nHomoskedastizität, d.h. die Varianz der Residuen muss konstant über die Werte der Prädiktoren sein (Varianzhomogenität).\nDie Residuen müssen normalverteilt sein (dies ist insbesondere für kleine Stichproben wichtig).\n\n\n\n\n\n\nWichtig\n\n\n\nDiese Voraussetzung der Normalverteilung gilt für die Residuen und nicht für die Prädiktoren!\n\n\nDie Residuen müssen unabhängig voneinander sein (kann z.B. mit dem Durbin-Watson-Test überprüft werden).\nDie Beziehung zwischen unabhängigen Variablen und abhängiger Variable muss linear sein."
  },
  {
    "objectID": "11/11.html#beispiel",
    "href": "11/11.html#beispiel",
    "title": "11 – Lineare Regression (2)",
    "section": "Beispiel",
    "text": "Beispiel\nIm folgenden Beispiel sehen wir uns wieder die Anzahl der Verkäufe von Musikalben in Abhängigkeit der Höhe des Werbebudgets an. Zusätzlich gibt es jetzt aber zwei weitere Prädiktoren, nämlich die Anzahl an Airplay-Stunden im größten nationalen Radiosender und die Attraktivität der Bandmitglieder. Wir beginnen mit dem Laden der Daten sales2.dat:\n\nlibrary(readr)\nalbum2 = read_tsv(\"sales2.dat\")\n\nBerechnen wir nun ein lineares Regressionsmodell. Als Vergleichsmodell führen wir zuerst eine einfache Regression mit dem einzigen Prädiktor Werbebudget durch:\n\nmodel1 = lm(sales ~ adverts, data=album2)\n\nZusätzliche Faktoren kann man nun in einem zweiten Modell einfach durch den +-Operator hinzufügen:\n\nmodel2 = lm(sales ~ adverts + airplay + attract, data=album2)\n\nAnschließend können wir uns die zusammengefassten Ergebnisse der beiden Modelle anzeigen lassen:\n\nsummary(model1)\n\n\nCall:\nlm(formula = sales ~ adverts, data = album2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-152.949  -43.796   -0.393   37.040  211.866 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\nsummary(model2)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + attract, data = album2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-121.324  -28.336   -0.451   28.967  144.132 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -26.612958  17.350001  -1.534    0.127    \nadverts       0.084885   0.006923  12.261  &lt; 2e-16 ***\nairplay       3.367425   0.277771  12.123  &lt; 2e-16 ***\nattract      11.086335   2.437849   4.548 9.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.09 on 196 degrees of freedom\nMultiple R-squared:  0.6647,    Adjusted R-squared:  0.6595 \nF-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nDa das erste Modell identisch mit jenem aus der vorigen Einheit ist, kennen wir die Ergebnisse bereits. Wenden wir uns daher dem zweiten Modell zu. \\(R^2\\) ist hier 0.6647, das heißt das Modell kann nun 66% der Varianz erklären. Im Vergleich zum ersten Modell mit nur einem Prädiktor ist das eine Steigerung um 33%, d.h. die beiden Prädiktoren Airplay und Attraktivität können zusätzliche Varianz im Ausmaß von 33% erklären.\nDie Regressionskoeffizienten werden ebenfalls in der Ausgabe dargestellt. Wir können daher das lineare Modell wie folgt schreiben:\n\\[y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 = -26.613 + 0.085 \\cdot x_1 + 3.367 \\cdot x_2 + 11.086 \\cdot x_3\\]\nHier stehen \\(b_0\\) für den Intercept (d.h. jener Wert von \\(y\\), wenn alle Prädiktoren 0 sind), \\(b_1\\) für das Werbebudget adverts, \\(b_2\\) für die Airplay-Stunden airplay und \\(b_3\\) für die Attraktivität attract.\nDie Regressionskoeffizienten geben Auskunft, um wie viel sich die abhängige Variable ändert, wenn man einen Prädiktor um eine Einheit erhöht und dabei alle anderen Prädiktoren konstant hält. Dies bedeutet im Beispiel:\n\nWenn adverts um eine Einheit erhöht wird, dann erhöht sich sales um 0.085 Einheiten. D.h. wenn man 1000 EUR mehr für Werbung ausgibt, verkauft man um 85 Alben mehr.\nWenn airplay um eine Einheit erhöht wird, dann erhöht sich sales um 3.37 Einheiten. D.h. wenn man das Album um eine Stunde mehr im Radio spielt, verkauft man um 3367 Alben mehr.\nWenn attract um eine Einheit erhöht wird, dann erhöht sich sales um 11.086 Einheiten (d.h. 11086 zusätzliche Alben).\n\nFür jeden Regressionskoeffizienten wird ein \\(t\\)-Test gerechnet, welcher angibt, ob sich der Koeffizient signifikant von 0 unterscheidet (d.h. ob er signifikant zum Modell beiträgt). Die Größe der \\(t\\)-Statistik lässt auf den Einfluss der Koeffizienten schließen, d.h. adverts und airplay haben einen ähnlich großen Einfluss auf das Modell, wo hingegen attract einen geringeren Einfluss hat.\nOft ist es hilfreich, nicht nur die Regressionskoeffizienten zu analysieren, sondern auch die standardisierten Regressionskoeffizienten. Diese kann man berechnen, in dem man zuerst alle Variablen standardisiert und danach das lineare Modell berechnet. Standardisierte Variablen haben einen Mittelwert von 0 und eine Standardabweichung von 1. Man könnte die Standardisierung daher relativ einfach selbst vornehmen, in dem man für jede Variable zuerst deren Mittelwert subtrahiert und danach durch deren Standardabweichung dividiert.\n\n\n\n\n\n\nTipp\n\n\n\nDie Funktion scale() kann verwendet werden, um die Spalten eine Data Frames zu standardisieren. Diese liefert aber immer eine Matrix zurück, d.h. wenn man ein Data Frame bzw. Tibble standardisieren möchte, muss man danach noch as.data.frame() bzw. tibble::as_tibble() anwenden.\n\n\nAlternativ kann man dazu auch nachträglich die Funktion lm.beta() aus dem lm.beta-Paket benutzen:\n\nlibrary(lm.beta)\nlm.beta(model2)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + attract, data = album2)\n\nStandardized Coefficients::\n(Intercept)     adverts     airplay     attract \n         NA   0.5108462   0.5119881   0.1916834 \n\n\nDie standardisierten Regressionskoeffizienten werden üblicherweise mit \\(\\beta_i\\) bezeichnet. Da alle Variablen nun in Standardabweichungen gemessen werden, kann man diese direkt miteinander vergleichen. Man sieht im Beispiel also:\n\nWenn adverts um eine Standardabweichung erhöht wird (485655 EUR), dann erhöht sich sales um 0.511 Standardabweichungen (41240 Alben).\nWenn airplay um eine Standardabweichung erhöht wird (12.270), dann erhöht sich sales um 0.512 Standardabweichungen (41320 Alben).\nWenn attract um eine Standardabweichung erhöht wird (1.395), dann erhöht sich sales um 0.192 Standardabweichungen (15490 Alben).\n\nKonfidenzintervalle für die (nicht standardisierten) Regressionskoeffizienten erhält man mit der Funktion confint() (standardmäßig werden 95%-Intervalle berechnet):\n\nconfint(model2)\n\n                   2.5 %      97.5 %\n(Intercept) -60.82960967  7.60369295\nadverts       0.07123166  0.09853799\nairplay       2.81962186  3.91522848\nattract       6.27855218 15.89411823\n\n\nZwei (oder mehrere) Modelle können mit der \\(F\\)-Statistik verglichen werden. Der \\(F\\)-Wert, der bei der zusammenfassenden Beschreibung eines Modells angezeigt wird, vergleicht das Modell standardmäßig mit dem einfachsten Mittelwertmodell. Möchte man das Modell mit einem anderen Modell vergleichen, ist zu beachten, dass model2 eine Erweiterung von model1 sein muss, d.h. model2 muss alle Terme von model1 beinhalten plus eventuelle zusätzliche Faktoren. In R gibt man hier folgenden Befehl ein:\n\nanova(model1, model2)\n\nAnalysis of Variance Table\n\nModel 1: sales ~ adverts\nModel 2: sales ~ adverts + airplay + attract\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    198 862264                                  \n2    196 434575  2    427690 96.447 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDer \\(F\\)-Wert beträgt also 96.447 und ist signifikant, d.h. das zweite Modell ist signifikant besser als das erste.\n\nDatenpunkte mit großem Einfluss\nUm die diversen Ausreißerstatistiken für jeden einzelnen Wert übersichtlich beurteilen zu können, kann man die Werte mit folgenden Funktionen berechnen:\n\nresid(): Residuen\nrstandard(): Standardisierte Residuen\nrstudent(): Studentisierte Residuen (berechnet mit Leave-One-Out)\nhatvalues(): Leverage\ndfbeta(): Unterschied der Regressionskoeffizienten mittels Leave-One-Out\ncooks.distance(): Cook’s Distanz\ndffits(): Unterschied im vorhergesagtem Wert mittels Leave-One-Out\n\n\n\n\n\n\n\nHinweis\n\n\n\n“Leave-One-Out” bedeutet, dass der jeweilige Datenpunkt aus der Berechnung des Werts ausgeschlossen wird.\n\n\nSehr praktisch ist die Funktion influence.measures(), welche mehrere Ausreißerstatistiken für jeden Datenpunkt übersichtlich aufbereitet ausgibt.\ninfluence.measures(model2)\n\n\nModellannahmen\nMultikollinearität kann mit der VIF-Statistik beurteilt werden; in R kann man dazu die Funktion vif() aus dem car-Paket verwenden.\n\nlibrary(car)\nvif(model2)\n\n adverts  airplay  attract \n1.014593 1.042504 1.038455 \n\n\nDer größte VIF-Wert sollte nicht größer als 10 sein (für einen konservativeren Schwellwert kann man auch 5 benutzen). Der durchschnittliche VIF sollte nicht wesentlich größer als 1 sein, was man wie folgt überprüfen kann:\n\nmean(vif(model2))\n\n[1] 1.03185\n\n\nWenn man den plot()-Befehl auf das Modell anwendet, werden vier diagnostische Plots erstellt.\n\npar(mfrow=c(2, 2), cex=0.75)\nplot(model2)\n\n\n\n\n\n\n\n\nIm Plot links oben sind die vorhergesagten Werte gegen die Residuen aufgetragen. Hier kann man die Linearitätsannahme (die rote Linie sollte immer ungefähr gleich Null sein) sowie die Homoskedastizitätsannahme (die Streuung der Datenpunkte sollte sich entlang der x-Achse nicht ändern) überprüfen. Der Plot links unten ist ähnlich, nur ist hier statt den (absoluten) Residuen die Wurzel aus dem Betrag der standardisierten Residuen aufgetragen. Auch hier lässt sich beurteilen, ob die Annahme der Varianzhomogenität erfüllt ist oder nicht. Im Plot rechts oben lässt sich die Normalverteilungsannahme der Residuen mit einem QQ-Plot überprüfen. Im Plot rechts unten sind Punkte mit großem Einfluss dargestellt (gemessen an der Leverage); Cook’s Distanz ist ebenfalls im Plot ersichtlich.\nDie Annahme über die Unabhängigkeit der Residuen kann mit dem Durbin-Watson-Test durbinWatsonTest() aus dem car-Paket überprüft werden.\n\ndurbinWatsonTest(model2)\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.0026951      1.949819    0.71\n Alternative hypothesis: rho != 0\n\n\nIn diesem Beispiel kann man davon ausgehen, dass die Residuen unabhängig sind, da wegen \\(p\\approx 0.7\\) die Nullhypothese nicht verworfen werden kann."
  },
  {
    "objectID": "11/11.html#übungen",
    "href": "11/11.html#übungen",
    "title": "11 – Lineare Regression (2)",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nLaden Sie die Daten aus der Datei sales2.dat wie in den Unterlagen gezeigt. Standardisieren Sie danach alle Variablen und berechnen Sie dann ein lineares Regressionsmodell. Vergleichen Sie die Regressionskoeffizienten mit den Ergebnissen der Funktion lm.beta(), welche auf ein Modell mit nicht standardisierten Daten angewendet werden kann.\n\n\nÜbung 2\nLaden Sie den Datensatz aggression.dat, welcher (komplett erfundene) Daten über Aggressionen unter Kindern enthält. Es wurden 666 Kinder untersucht und folgende Variablen erhoben:\n\nErziehungsstil (hoher Wert entspricht schlechtem Stil)\nComputerspielen (hoher Wert entspricht viel Computerspielen)\nFernsehen (hoher Wert entspricht viel Fernsehen)\nErnährung (hoher Wert entspricht gesunder Ernährung)\nAggressionen der Geschwister (hoher Wert entspricht hoher Aggression)\n\nVon früheren Studien weiß man, dass Erziehungsstil sowie Aggressionen der Geschwister signifikante Prädiktoren für das Aggressionslevel eines Kindes sind.\nStellen Sie zwei lineare Regressionsmodelle auf. Das erste soll nur die beiden Faktoren beinhalten, welche erwiesenermaßen einen Einfluss auf die Aggression haben. Das zweite Modell soll alle Faktoren beinhalten. Beantworten Sie anschließend folgende Punkte:\n\nBestimmen Sie für beide Modelle das Bestimmtheitsmaß \\(R^2\\) und geben Sie die Tabelle der Regressionskoeffizienten aus.\nInterpretieren Sie für beide Modelle getrennt die einzelnen Koeffizienten hinsichtlich Relevanz (hier sind standardisierte Koeffizienten hilfreich) und Signifikanz.\nVergleichen Sie beide Modelle miteinander. Ist das zweite Modell eine signifikante Verbesserung zum ersten?\n\n\n\nÜbung 3\nÜberprüfen Sie für das zweite Modell (mit allen Prädiktoren) aus der vorigen Übung folgende Voraussetzungen:\n\nSind die unabhängigen Variablen kollinear (VIF)?\nSind die Residuen unabhängig (Durbin-Watson-Test)?\nSind die Residuen normalverteilt (QQ-Plot)?\nSind die Abhängigkeiten linear und ist die Varianz homogen (Plot Residuen vs. vorhergesagte Werte)?\nGibt es Datenpunkte mit großem Einfluss auf das Modell (Plot Residuen vs. Leverage)?\n\nHinweis: Sehen Sie sich die Hilfe zur Funktion plot.lm() an (damit können Sie ein lineares Modell plotten und die Grafiken aus den letzten drei Fragen erstellen; mit dem Argument which können Sie sich die gewünschte Grafik herauspicken)."
  },
  {
    "objectID": "11/11-solutions.html",
    "href": "11/11-solutions.html",
    "title": "11 – Lösungen",
    "section": "",
    "text": "library(readr)\nlibrary(tibble)\n\nsales = read_tsv(\"sales2.dat\")\nsales = as_tibble(scale(sales))\nmodel = lm(sales ~ adverts + airplay + attract, data=sales)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + attract, data = sales)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.50342 -0.35113 -0.00559  0.35895  1.78605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.586e-17  4.126e-02   0.000        1    \nadverts      5.108e-01  4.166e-02  12.261  &lt; 2e-16 ***\nairplay      5.120e-01  4.223e-02  12.123  &lt; 2e-16 ***\nattract      1.917e-01  4.215e-02   4.548 9.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5835 on 196 degrees of freedom\nMultiple R-squared:  0.6647,    Adjusted R-squared:  0.6595 \nF-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\ncoef(model)  # stimmt mit lm.beta überein\n\n  (Intercept)       adverts       airplay       attract \n-5.585796e-17  5.108462e-01  5.119881e-01  1.916834e-01"
  },
  {
    "objectID": "11/11-solutions.html#übung-1",
    "href": "11/11-solutions.html#übung-1",
    "title": "11 – Lösungen",
    "section": "",
    "text": "library(readr)\nlibrary(tibble)\n\nsales = read_tsv(\"sales2.dat\")\nsales = as_tibble(scale(sales))\nmodel = lm(sales ~ adverts + airplay + attract, data=sales)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + attract, data = sales)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.50342 -0.35113 -0.00559  0.35895  1.78605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.586e-17  4.126e-02   0.000        1    \nadverts      5.108e-01  4.166e-02  12.261  &lt; 2e-16 ***\nairplay      5.120e-01  4.223e-02  12.123  &lt; 2e-16 ***\nattract      1.917e-01  4.215e-02   4.548 9.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5835 on 196 degrees of freedom\nMultiple R-squared:  0.6647,    Adjusted R-squared:  0.6595 \nF-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\ncoef(model)  # stimmt mit lm.beta überein\n\n  (Intercept)       adverts       airplay       attract \n-5.585796e-17  5.108462e-01  5.119881e-01  1.916834e-01"
  },
  {
    "objectID": "11/11-solutions.html#übung-2",
    "href": "11/11-solutions.html#übung-2",
    "title": "11 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nlibrary(lm.beta)  # für lm.beta\n\naggression = read_tsv(\"aggression.dat\")\n\nmodel1 = lm(Aggression ~ Parenting_Style + Sibling_Aggression, data=aggression)\nsummary(model1)  # beide Variablen erwartungsgemäß signifikant\n\n\nCall:\nlm(formula = Aggression ~ Parenting_Style + Sibling_Aggression, \n    data = aggression)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09755 -0.17180  0.00092  0.15405  1.23037 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -0.005784   0.012065  -0.479    0.632    \nParenting_Style     0.061984   0.012257   5.057 5.51e-07 ***\nSibling_Aggression  0.093409   0.037505   2.491    0.013 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3113 on 663 degrees of freedom\nMultiple R-squared:  0.05325,   Adjusted R-squared:  0.05039 \nF-statistic: 18.64 on 2 and 663 DF,  p-value: 1.325e-08\n\nlm.beta(model1)  # Parenting_Style hat größeren Einfluss als Sibling_Aggression\n\n\nCall:\nlm(formula = Aggression ~ Parenting_Style + Sibling_Aggression, \n    data = aggression)\n\nStandardized Coefficients::\n       (Intercept)    Parenting_Style Sibling_Aggression \n                NA         0.19406149         0.09557412 \n\nmodel2 = lm(\n    Aggression ~ Parenting_Style + Sibling_Aggression + Television +\n                 Computer_Games + Diet,\n    data=aggression\n)\nsummary(model2)\n\n\nCall:\nlm(formula = Aggression ~ Parenting_Style + Sibling_Aggression + \n    Television + Computer_Games + Diet, data = aggression)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.12629 -0.15253 -0.00421  0.15222  1.17669 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -0.004988   0.011983  -0.416 0.677350    \nParenting_Style     0.056648   0.014557   3.891 0.000110 ***\nSibling_Aggression  0.081684   0.038780   2.106 0.035550 *  \nTelevision          0.032916   0.046057   0.715 0.475059    \nComputer_Games      0.142161   0.036920   3.851 0.000129 ***\nDiet               -0.109054   0.038076  -2.864 0.004315 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 660 degrees of freedom\nMultiple R-squared:  0.08258,   Adjusted R-squared:  0.07563 \nF-statistic: 11.88 on 5 and 660 DF,  p-value: 5.025e-11\n\nlm.beta(model2)\n\n\nCall:\nlm(formula = Aggression ~ Parenting_Style + Sibling_Aggression + \n    Television + Computer_Games + Diet, data = aggression)\n\nStandardized Coefficients::\n       (Intercept)    Parenting_Style Sibling_Aggression         Television     Computer_Games               Diet \n                NA         0.17735588         0.08357717         0.03192490         0.15211518        -0.11503080 \n\nanova(model1, model2)  # Vergleich beider Modelle\n\nAnalysis of Variance Table\n\nModel 1: Aggression ~ Parenting_Style + Sibling_Aggression\nModel 2: Aggression ~ Parenting_Style + Sibling_Aggression + Television + \n    Computer_Games + Diet\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    663 64.23                                  \n2    660 62.24  3      1.99 7.0339 0.0001166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "11/11-solutions.html#übung-3",
    "href": "11/11-solutions.html#übung-3",
    "title": "11 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nlibrary(car)  # für dwt, vif\n\nLoading required package: carData\n\n# Kollinearität\nvif(model2)\n\n   Parenting_Style Sibling_Aggression         Television     Computer_Games               Diet \n          1.494296           1.132618           1.435525           1.122719           1.160466 \n\n# Unabhängigkeit der Residuen\ndwt(model2)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.04218005      1.912808   0.296\n Alternative hypothesis: rho != 0\n\n# Normalverteilung Residuen\nplot(model2, which=2)\n\n\n\n\n\n\n\n# Linearität und Varianzhomogenität\nplot(model2, which=1)\n\n\n\n\n\n\n\n# Punkte mit großem Einfluss\nplot(model2, which=5)"
  },
  {
    "objectID": "03/03-solutions.html",
    "href": "03/03-solutions.html",
    "title": "3 – Lösungen",
    "section": "",
    "text": "r = 5  # Radius\nh = 9  # Höhe\n(A = 2 * r * pi * (r + h))  # Oberfläche\n\n[1] 439.823\n\n(V = r**2 * pi * h)  # Volumen\n\n[1] 706.8583"
  },
  {
    "objectID": "03/03-solutions.html#übung-1",
    "href": "03/03-solutions.html#übung-1",
    "title": "3 – Lösungen",
    "section": "",
    "text": "r = 5  # Radius\nh = 9  # Höhe\n(A = 2 * r * pi * (r + h))  # Oberfläche\n\n[1] 439.823\n\n(V = r**2 * pi * h)  # Volumen\n\n[1] 706.8583"
  },
  {
    "objectID": "03/03-solutions.html#übung-2",
    "href": "03/03-solutions.html#übung-2",
    "title": "3 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\n(x = c(4, 18, -7, 16, 4, -44))\n\n[1]   4  18  -7  16   4 -44\n\n(y = x**2)\n\n[1]   16  324   49  256   16 1936\n\n(z = c(x, y))\n\n [1]    4   18   -7   16    4  -44   16  324   49  256   16 1936\n\nlength(z)\n\n[1] 12"
  },
  {
    "objectID": "03/03-solutions.html#übung-3",
    "href": "03/03-solutions.html#übung-3",
    "title": "3 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nx = c(44, 23, -56, 98, 99, 32, 45, 22)\nx %% 2 == 0  # logischer Vektor, TRUE bei geraden Elementen\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\nx[x %% 2 == 0]  # gerade Elemente von x\n\n[1]  44 -56  98  32  22\n\nx %% 2 != 0  # logischer Vektor, TRUE bei ungeraden Elemente\n\n[1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\nx[x %% 2 != 0]  # ungerade Elemente von x\n\n[1] 23 99 45"
  },
  {
    "objectID": "03/03-solutions.html#übung-4",
    "href": "03/03-solutions.html#übung-4",
    "title": "3 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\n15:40\n\n [1] 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\nseq(75, 61, -3)\n\n[1] 75 72 69 66 63\n\nseq(14, 15, length.out=35)\n\n [1] 14.00000 14.02941 14.05882 14.08824 14.11765 14.14706 14.17647 14.20588 14.23529 14.26471 14.29412 14.32353\n[13] 14.35294 14.38235 14.41176 14.44118 14.47059 14.50000 14.52941 14.55882 14.58824 14.61765 14.64706 14.67647\n[25] 14.70588 14.73529 14.76471 14.79412 14.82353 14.85294 14.88235 14.91176 14.94118 14.97059 15.00000"
  },
  {
    "objectID": "03/03-solutions.html#übung-5",
    "href": "03/03-solutions.html#übung-5",
    "title": "3 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\nrep(c(\"Placebo\", \"Gruppe 1\", \"Gruppe 2\"), each=10)\n\n [1] \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\"  \"Placebo\" \n[11] \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\" \"Gruppe 1\"\n[21] \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\" \"Gruppe 2\""
  },
  {
    "objectID": "03/03-solutions.html#übung-6",
    "href": "03/03-solutions.html#übung-6",
    "title": "3 – Lösungen",
    "section": "Übung 6",
    "text": "Übung 6\n\nk = seq(0, 20, 2)\nk[-c(3, 7)]\n\n[1]  0  2  6  8 10 14 16 18 20\n\nk[1:5]\n\n[1] 0 2 4 6 8\n\nk[c(2, 5, 16)]  # das 16. Element gibt es nicht -&gt; NA\n\n[1]  2  8 NA\n\nk[k &gt; 11]\n\n[1] 12 14 16 18 20"
  },
  {
    "objectID": "03/03-solutions.html#übung-7",
    "href": "03/03-solutions.html#übung-7",
    "title": "3 – Lösungen",
    "section": "Übung 7",
    "text": "Übung 7\n\nt = c(10, 20, NA, 30, 40)\nmean(t)  # Ergebnis ist NA\n\n[1] NA\n\nmean(t, na.rm=TRUE)  # NA-Elemente werden ignoriert\n\n[1] 25"
  },
  {
    "objectID": "03/03-solutions.html#übung-8",
    "href": "03/03-solutions.html#übung-8",
    "title": "3 – Lösungen",
    "section": "Übung 8",
    "text": "Übung 8\n\ns = c(1, 11.3, 7.8, 3.4, 6)  # Standardabweichungen\ns**2  # Varianzen (elementweises Quadrieren)\n\n[1]   1.00 127.69  60.84  11.56  36.00\n\n\n\nÜbung 9\n\nx = c(2, 0, -5, 0, 1, -1, 0, 3, 0, 0, 7)\nx == 0\n\n [1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n\nsum(x == 0)\n\n[1] 5\n\nwhich(x == 0)\n\n[1]  2  4  7  9 10\n\n\n\n\nÜbung 10\nInnerhalb der eckigen Klammer muss immer exakt ein Vektor sein. Möchte man also drei Positionen indizieren, muss dieser Vektor also mit c() erzeugt werden. Deswegen funktioniert x[2, 4, 6] nicht, weil hier innerhalb der eckigen Klammern drei Vektoren angegeben werden (mit Kommas voneinander getrennt).\nDer korrekte Code lautet also:\n\nx = 1:10\nx[c(2, 4, 6)]\n\n[1] 2 4 6"
  },
  {
    "objectID": "00/00.html",
    "href": "00/00.html",
    "title": "Organisatorisches",
    "section": "",
    "text": "In dieser Lehrveranstaltung werden Sie das Statistikpaket R kennenlernen. Da diese Lehrveranstaltung die Grundlagen von R abdeckt und keinerlei Vorwissen voraussetzt, ist es aufgrund der begrenzten Zeit nicht möglich, alle in der Praxis relevanten statistischen Verfahren abzudecken. Sie werden durch ein umfassendes Basiswissen aber darauf vorbereitet, sich weitere Methoden selbst anzueignen.\nDer Plan für dieses Semester sieht wie folgt aus:\n\n\n\n\n\n\n\n\nEinheit\nDatum\nInhalt\n\n\n\n\n1\n2.3.2026\nGrundlagen\n\n\n2\n9.3.2026\nDie R-Umgebung\n\n\n3\n16.3.2026\nVektoren\n\n\n4\n23.3.2026\nTabellen\n\n\n5\n13.4.2026\nDaten importieren\n\n\n6\n20.4.2026\nDeskriptive Statistiken\n\n\n7\n27.4.2026\nDatenaufbereitung\n\n\n8\n4.5.2026\nGrafiken\n\n\n9\n11.5.2026\nKorrelation\n\n\n10\n18.5.2026\nLineare Regression (1)\n\n\n11\n1.6.2026\nLineare Regression (2)\n\n\n12\n8.6.2026\nMittelwertvergleich\n\n\n13\n15.6.2026\nPrüfung\n\n\n14\n22.6.2026\nPrüfung (Ersatztermin)\n\n\n\nAlle Informationen und Unterlagen zu dieser Lehrveranstaltung werden auf Moodle zur Verfügung gestellt. Insbesondere wird auch die Kommunikation (Ankündigungen, Forumsdiskussionen, Fragen & Antworten) über Moodle abgewickelt.\nDie Einheiten finden grundsätzlich in Präsenz statt. Sollte das einmal nicht möglich sein, wird dies auf Moodle bekanntgegeben und die betreffende Einheit wird gegebenenfalls zur geplanten Zeit online abgehalten."
  },
  {
    "objectID": "00/00.html#allgemeines",
    "href": "00/00.html#allgemeines",
    "title": "Organisatorisches",
    "section": "",
    "text": "In dieser Lehrveranstaltung werden Sie das Statistikpaket R kennenlernen. Da diese Lehrveranstaltung die Grundlagen von R abdeckt und keinerlei Vorwissen voraussetzt, ist es aufgrund der begrenzten Zeit nicht möglich, alle in der Praxis relevanten statistischen Verfahren abzudecken. Sie werden durch ein umfassendes Basiswissen aber darauf vorbereitet, sich weitere Methoden selbst anzueignen.\nDer Plan für dieses Semester sieht wie folgt aus:\n\n\n\n\n\n\n\n\nEinheit\nDatum\nInhalt\n\n\n\n\n1\n2.3.2026\nGrundlagen\n\n\n2\n9.3.2026\nDie R-Umgebung\n\n\n3\n16.3.2026\nVektoren\n\n\n4\n23.3.2026\nTabellen\n\n\n5\n13.4.2026\nDaten importieren\n\n\n6\n20.4.2026\nDeskriptive Statistiken\n\n\n7\n27.4.2026\nDatenaufbereitung\n\n\n8\n4.5.2026\nGrafiken\n\n\n9\n11.5.2026\nKorrelation\n\n\n10\n18.5.2026\nLineare Regression (1)\n\n\n11\n1.6.2026\nLineare Regression (2)\n\n\n12\n8.6.2026\nMittelwertvergleich\n\n\n13\n15.6.2026\nPrüfung\n\n\n14\n22.6.2026\nPrüfung (Ersatztermin)\n\n\n\nAlle Informationen und Unterlagen zu dieser Lehrveranstaltung werden auf Moodle zur Verfügung gestellt. Insbesondere wird auch die Kommunikation (Ankündigungen, Forumsdiskussionen, Fragen & Antworten) über Moodle abgewickelt.\nDie Einheiten finden grundsätzlich in Präsenz statt. Sollte das einmal nicht möglich sein, wird dies auf Moodle bekanntgegeben und die betreffende Einheit wird gegebenenfalls zur geplanten Zeit online abgehalten."
  },
  {
    "objectID": "00/00.html#ablauf-einer-einheit",
    "href": "00/00.html#ablauf-einer-einheit",
    "title": "Organisatorisches",
    "section": "Ablauf einer Einheit",
    "text": "Ablauf einer Einheit\nZu jeder Einheit gibt es auf Moodle ausführliche Unterlagen, welche das jeweilige Thema detailliert behandeln. Diese Unterlagen sind so gestaltet, dass Sie sich die Inhalte im Selbststudium aneignen können. Alle inhaltlichen Einheiten (mit Ausnahme der ersten) werden nach dem Prinzip des Flipped Classroom abgehalten. Das bedeutet, dass Sie die Inhalte der jeweiligen Einheit vor dem tatsächlichen Termin erarbeiten müssen. Besonders wichtig sind die Übungsbeispiele am Ende jedes Kapitels. Diese überprüfen Ihr Verständnis der vermittelten Inhalte und sind außerdem eine essenzielle Vorbereitung für die abschließende Prüfung.\n\n\n\n\n\n\nTipp\n\n\n\nNutzen Sie während der Vorbereitungszeit das Diskussionsforum auf Moodle – dort können Sie Fragen stellen und selbstverständlich auch beantworten. Je mehr Personen sich aktiv im Forum beteiligen, desto hilfreicher wird es für alle!\n\n\n\n\n\n\n\n\nTipp\n\n\n\nWenn Sie die Möglichkeit haben, sich die Inhalte gemeinsam mit anderen Studierenden zu erarbeiten, ist das natürlich umso besser! Gerade bei Programmieraufgaben ist es oft hilfreich, sich auszutauschen und gemeinsam Lösungen zu erarbeiten (Pair Programming).\n\n\nJede Einheit beginnt mit einem kurzen Quiz zum Thema der jeweiligen Einheit (dazu später mehr). Danach werden die von Ihnen vorbereiteten Übungsbeispiele aus den Unterlagen detailliert besprochen und Musterlösungen präsentiert. Dabei haben Sie die Möglichkeit, Fragen zu stellen bzw. Ihre individuellen Lösungsansätze zu diskutieren. Falls darüber hinaus noch Zeit bleiben sollte, wird das jeweilige Thema durch vertiefende Beispiele bzw. Erklärungen ergänzt.\n\n\n\n\n\n\nHinweis\n\n\n\nDer Einsatz von Künstlicher Intelligenz, insbesondere von Large Language Models (wie z.B. ChatGPT, Gemini, Mistral oder Claude), ist für die Vorbereitung der Einheiten ausdrücklich erlaubt und wird sogar empfohlen. Diese Werkzeuge können helfen, Konzepte zu verstehen, Codebeispiele nachzuvollziehen und alternative Erklärungen zu finden. Allerdings sollten Sie immer versuchen, die Übungsaufgaben selbständig zu lösen, um ein tiefes Verständnis der Materie zu entwickeln."
  },
  {
    "objectID": "00/00.html#beurteilungskriterien",
    "href": "00/00.html#beurteilungskriterien",
    "title": "Organisatorisches",
    "section": "Beurteilungskriterien",
    "text": "Beurteilungskriterien\n\nAnwesenheit\nDurch den immanenten Prüfungscharakter dieser Lehrveranstaltung besteht Anwesenheitspflicht. Ihre Anwesenheit wird zu einem zufälligen Zeitpunkt während der Einheit überprüft. Sie können allerdings ohne Auswirkungen auf die Beurteilung bei bis zu drei Einheiten fehlen (eine Begründung ist nicht erforderlich).\n\n\nQuizzes\nZu Beginn jeder Einheit (mit Ausnahme der ersten) findet ein Quiz auf Moodle statt. Es werden kurze Fragen zum jeweiligen Thema der Einheit gestellt (meist Multiple-Choice). Hilfsmittel (wie z.B. die Unterlagen) sind nicht erlaubt.\nSie können bei einem Quiz maximal 10 Punkte erreichen (es gibt auch Teilpunkte). Das Quiz muss vor Ort auf einem Universitäts-PC absolviert werden. Ein nicht absolviertes Quiz (z.B. durch Fehlen oder Zuspätkommen) zählt 0 Punkte.\n\n\n\n\n\n\nHinweis\n\n\n\nStellen Sie vorab sicher, dass Ihre aktuellen Zugangsdaten (einschließlich der Zwei-Faktor-Authentifizierung) für die Anmeldung funktionieren!\n\n\n\n\nPrüfung\nAm Ende des Semesters findet eine Prüfung über alle Inhalte der Lehrveranstaltung statt, welche vor Ort auf einem Universitäts-PC absolviert wird. Als einziges Hilfsmittel ist ein von Ihnen erstelltes, zweiseitig beschriebenes A4-Blatt erlaubt, auf dem Sie die wichtigsten Inhalte der Lehrveranstaltung zusammenfassen können. Fertig gelöste Beispiele dürfen darauf jedoch nicht enthalten sein. Da Sie mit RStudio arbeiten werden (Ihre Lösung ist dann als R-Script auf Moodle abzugeben), können Sie natürlich auch die integrierte Hilfe von R nutzen, um Informationen zu Funktionen oder Paketen abzurufen.\nEs können maximal 10 Punkte erreicht werden (Teilpunkte sind möglich).\nDie Prüfung kann bei Bedarf wiederholt werden. In diesem Fall zählen die Punkte des zweiten Termins, d.h. man kann sich sowohl verbessern als auch verschlechtern."
  },
  {
    "objectID": "00/00.html#benotung",
    "href": "00/00.html#benotung",
    "title": "Organisatorisches",
    "section": "Benotung",
    "text": "Benotung\nDie Gesamtnote ergibt sich aus den drei beschriebenen Kriterien:\n\nWenn Sie nicht öfter als drei Mal gefehlt haben, ist das Anwesenheitskriterium erfüllt. Mehr als drei Fehleinheiten führen automatisch zu einer negativen Beurteilung der Lehrveranstaltung.\nAus den Punkten aller Quizzes werden zunächst die niedrigsten drei Bewertungen entfernt. Aus den übrigen Bewertungen wird der arithmetische Mittelwert berechnet – dieser Mittelwert fließt dann zu 50% in die Gesamtnote ein.\nDie Punkte aus der Prüfung fließen ebenfalls zu 50% in die Gesamtnote ein. Es müssen allerdings mindestens 6 Punkte bei der Prüfung erreicht werden, um die Lehrveranstaltung positiv abzuschließen.\n\nSofern diese Kriterien erfüllt sind, ergibt sich die Gesamtnote aus dem Mittelwert der Punkte aus den Quizzes und der Prüfung wie folgt:\n\n\n\n\n\n\n\n\n\n\nSehr gut\nGut\nBefriedigend\nGenügend\nNicht genügend\n\n\n\n\n9–10 Punkte\n8–9 Punkte\n7–8 Punkte\n6–7 Punkte\n&lt; 6 Punkte\n\n\n\nDie niedrigste Punkteanzahl einer Kategorie wird immer der besseren Note zugeordnet. Beispielsweise entsprechen exakt 9 Punkte der Note “Sehr gut”. Die gemittelte Punkteanzahl wird auch nicht mehr auf- oder abgerundet, daher entsprechen z.B. 8.97 Punkte der Note “Gut”."
  },
  {
    "objectID": "08/08.html",
    "href": "08/08.html",
    "title": "8 – Grafiken",
    "section": "",
    "text": "In R gibt es mehrere Pakete, mit denen man Daten grafisch darstellen kann. Grafiken, welche mit unterschiedlichen Paketen erstellt wurden, lassen sich aber nicht miteinander kombinieren. Deshalb entscheidet man sich typischerweise vor der Erstellung einer Grafik für das zu verwendende Grafikpaket.\nDas graphics-Paket wird auch als Base-Plotting-System bezeichnet. Es ist Teil von R und muss nicht extra installiert werden. Das Base-Plotting-System eignet sich sehr gut für schnelle Visualisierungen – mit etwas Aufwand kann man aber auch schöne Grafiken für Publikationen erstellen.\nFür die folgenden Beispiele verwenden wir den in R enthaltenen airquality-Datensatz (eine kurze Beschreibung kann man dem Hilfetext ?airquality entnehmen):\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ..."
  },
  {
    "objectID": "08/08.html#allgemeines",
    "href": "08/08.html#allgemeines",
    "title": "8 – Grafiken",
    "section": "",
    "text": "In R gibt es mehrere Pakete, mit denen man Daten grafisch darstellen kann. Grafiken, welche mit unterschiedlichen Paketen erstellt wurden, lassen sich aber nicht miteinander kombinieren. Deshalb entscheidet man sich typischerweise vor der Erstellung einer Grafik für das zu verwendende Grafikpaket.\nDas graphics-Paket wird auch als Base-Plotting-System bezeichnet. Es ist Teil von R und muss nicht extra installiert werden. Das Base-Plotting-System eignet sich sehr gut für schnelle Visualisierungen – mit etwas Aufwand kann man aber auch schöne Grafiken für Publikationen erstellen.\nFür die folgenden Beispiele verwenden wir den in R enthaltenen airquality-Datensatz (eine kurze Beschreibung kann man dem Hilfetext ?airquality entnehmen):\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ..."
  },
  {
    "objectID": "08/08.html#die-plot-funktion",
    "href": "08/08.html#die-plot-funktion",
    "title": "8 – Grafiken",
    "section": "Die plot()-Funktion",
    "text": "Die plot()-Funktion\nEine der wichtigsten Funktionen im Base-Plotting-System ist plot(). Diese Funktion erstellt passende Visualisierungen in Abhängigkeit von den darzustellenden Daten.\nWenn man einen numerischen Vektor übergibt, erhält man eine Punktgrafik. Auf der x-Achse wird der Index der Datenpunkte dargestellt, auf der y-Achse die Datenwerte:\n\nplot(airquality$Ozone)\n\n\n\n\n\n\n\n\nWenn man zwei numerische Vektoren übergibt, erhält man einen sogenannten Scatterplot. Das erste Argument wird auf der x-Achse und das zweite Argument wird auf der y-Achse dargestellt:\n\nplot(airquality$Wind, airquality$Ozone)\n\n\n\n\n\n\n\n\nAuch mit Datumswerten kann plot() umgehen. Erstellen wir zunächst aus den beiden Spalten Month und Day eine neue Spalte namens date (aus der Beschreibung der Daten kann man entnehmen, dass diese im Jahr 1973 aufgezeichnet wurden):\n\nairquality$date = as.Date(\n    paste(airquality$Month, airquality$Day, \"1973\"),\n    format=\"%m %d %Y\"\n)\n\n\n\n\n\n\n\nTipp\n\n\n\nDie Funktion paste() fügt mehrere Character-Vektoren elementweise zu einem einzigen Vektor zusammen (standardmäßig wird zwischen den Elementen ein Leerzeichen eingefügt):\n\npaste(\"Hello\", \"World\")\n\n[1] \"Hello World\"\n\n\nDie einzelnen Argumente können Vektoren mit beliebiger Anzahl an Elementen sein:\n\npaste(c(\"Hello\", \"Sup\"), c(\"Jane\", \"John\"))\n\n[1] \"Hello Jane\" \"Sup John\"  \n\n\n\n\nDas Ergebnis sieht also so aus:\n\npaste(airquality$Month, airquality$Day, \"1973\")\n\n  [1] \"5 1 1973\"  \"5 2 1973\"  \"5 3 1973\"  \"5 4 1973\"  \"5 5 1973\"  \"5 6 1973\"  \"5 7 1973\"  \"5 8 1973\"  \"5 9 1973\" \n [10] \"5 10 1973\" \"5 11 1973\" \"5 12 1973\" \"5 13 1973\" \"5 14 1973\" \"5 15 1973\" \"5 16 1973\" \"5 17 1973\" \"5 18 1973\"\n [19] \"5 19 1973\" \"5 20 1973\" \"5 21 1973\" \"5 22 1973\" \"5 23 1973\" \"5 24 1973\" \"5 25 1973\" \"5 26 1973\" \"5 27 1973\"\n [28] \"5 28 1973\" \"5 29 1973\" \"5 30 1973\" \"5 31 1973\" \"6 1 1973\"  \"6 2 1973\"  \"6 3 1973\"  \"6 4 1973\"  \"6 5 1973\" \n [37] \"6 6 1973\"  \"6 7 1973\"  \"6 8 1973\"  \"6 9 1973\"  \"6 10 1973\" \"6 11 1973\" \"6 12 1973\" \"6 13 1973\" \"6 14 1973\"\n [46] \"6 15 1973\" \"6 16 1973\" \"6 17 1973\" \"6 18 1973\" \"6 19 1973\" \"6 20 1973\" \"6 21 1973\" \"6 22 1973\" \"6 23 1973\"\n [55] \"6 24 1973\" \"6 25 1973\" \"6 26 1973\" \"6 27 1973\" \"6 28 1973\" \"6 29 1973\" \"6 30 1973\" \"7 1 1973\"  \"7 2 1973\" \n [64] \"7 3 1973\"  \"7 4 1973\"  \"7 5 1973\"  \"7 6 1973\"  \"7 7 1973\"  \"7 8 1973\"  \"7 9 1973\"  \"7 10 1973\" \"7 11 1973\"\n [73] \"7 12 1973\" \"7 13 1973\" \"7 14 1973\" \"7 15 1973\" \"7 16 1973\" \"7 17 1973\" \"7 18 1973\" \"7 19 1973\" \"7 20 1973\"\n [82] \"7 21 1973\" \"7 22 1973\" \"7 23 1973\" \"7 24 1973\" \"7 25 1973\" \"7 26 1973\" \"7 27 1973\" \"7 28 1973\" \"7 29 1973\"\n [91] \"7 30 1973\" \"7 31 1973\" \"8 1 1973\"  \"8 2 1973\"  \"8 3 1973\"  \"8 4 1973\"  \"8 5 1973\"  \"8 6 1973\"  \"8 7 1973\" \n[100] \"8 8 1973\"  \"8 9 1973\"  \"8 10 1973\" \"8 11 1973\" \"8 12 1973\" \"8 13 1973\" \"8 14 1973\" \"8 15 1973\" \"8 16 1973\"\n[109] \"8 17 1973\" \"8 18 1973\" \"8 19 1973\" \"8 20 1973\" \"8 21 1973\" \"8 22 1973\" \"8 23 1973\" \"8 24 1973\" \"8 25 1973\"\n[118] \"8 26 1973\" \"8 27 1973\" \"8 28 1973\" \"8 29 1973\" \"8 30 1973\" \"8 31 1973\" \"9 1 1973\"  \"9 2 1973\"  \"9 3 1973\" \n[127] \"9 4 1973\"  \"9 5 1973\"  \"9 6 1973\"  \"9 7 1973\"  \"9 8 1973\"  \"9 9 1973\"  \"9 10 1973\" \"9 11 1973\" \"9 12 1973\"\n[136] \"9 13 1973\" \"9 14 1973\" \"9 15 1973\" \"9 16 1973\" \"9 17 1973\" \"9 18 1973\" \"9 19 1973\" \"9 20 1973\" \"9 21 1973\"\n[145] \"9 22 1973\" \"9 23 1973\" \"9 24 1973\" \"9 25 1973\" \"9 26 1973\" \"9 27 1973\" \"9 28 1973\" \"9 29 1973\" \"9 30 1973\"\n\n\nDieser Vektor kann dann mit der Funktion as.Date() mit dem Argument format=\"%m %d %Y\" in einen Datumsvektor konvertiert werden, welchen wir der Spalte date im Data Frame zugewiesen haben:\n\nclass(airquality$date)\n\n[1] \"Date\"\n\n\nAnschließend können wir diese Datumsspalte für die Darstellung auf der x-Achse verwenden. In diesem Beispiel werden dann automatisch die Namen für die fünf Monate auf der x-Achse angezeigt:\n\nplot(airquality$date, airquality$Ozone)\n\n\n\n\n\n\n\n\nÜbergibt man der Funktion einen Faktor, wird automatisch eine Balkengrafik mit den Häufigkeiten der einzelnen Stufen erzeugt:\n\nplot(factor(airquality$Month))"
  },
  {
    "objectID": "08/08.html#histogramme",
    "href": "08/08.html#histogramme",
    "title": "8 – Grafiken",
    "section": "Histogramme",
    "text": "Histogramme\nDie Funktion hist() erstellt ein Histogramm eines Vektors:\n\nhist(airquality$Ozone)\n\n\n\n\n\n\n\n\nMit Hilfe eines Histogramms kann man die Verteilung der Werte eines Vektors visualisieren. Die Anzahl der Säulen im Histogramm kann man mit dem Argument breaks explizit festlegen:\n\nhist(airquality$Ozone, breaks=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDas Argument breaks ist nur eine Empfehlung – die tatsächliche Anzahl an Balken wird so adaptiert, dass die Grafik immer noch übersichtlich darzustellen ist."
  },
  {
    "objectID": "08/08.html#boxplots",
    "href": "08/08.html#boxplots",
    "title": "8 – Grafiken",
    "section": "Boxplots",
    "text": "Boxplots\nEine weitere Möglichkeit, die Verteilung von Werten grafisch darzustellen, bietet die Funktion boxplot(). Ein Boxplot zeigt den Median, den Interquartilsabstand (IQR) sowie das Minimum und das Maximum (plus eventuelle Ausreißer) der Daten.\n\nboxplot(airquality$Temp)\n\n\n\n\n\n\n\n\nEin einzelner Boxplot ist für sich noch relativ wenig hilfreich. Wenn man statt eines Vektors eine sogenannte Formel angibt, kann man mehrere Boxplots in einer Grafik darstellen.\n\n\n\n\n\n\nHinweis\n\n\n\nEine Formel in R wird durch das Tilde-Zeichen (~) definiert, mit jeweiligen Ausdrücken links bzw. rechts davon. Ein Beispiel für eine Formel ist y ~ x mit y auf der linken Seite und x auf der rechten Seite. Die Bedeutung einer Formel hängt von der jeweiligen Funktion ab. Viele Funktionen verlangen Formeln als Argumente, wir werden dies vor allem bei der Berechnung von linearen Modellen intensiv nutzen.\n\n\nIm folgenden Beispiel werden Boxplots für airquality$Temp in Abhängigkeit von airquality$Month dargestellt:\n\nboxplot(airquality$Temp ~ airquality$Month)\n\n\n\n\n\n\n\n\nHier bestimmt also die linke Seite der Formel die Werte auf der y-Achse, und die rechte Seite die Werte auf der x-Achse."
  },
  {
    "objectID": "08/08.html#grafiken-anpassen",
    "href": "08/08.html#grafiken-anpassen",
    "title": "8 – Grafiken",
    "section": "Grafiken anpassen",
    "text": "Grafiken anpassen\nOft möchte man diverse Eigenschaften von Grafiken anpassen, wie z.B. die verwendete Linienart, Farben, Symbole, Titel, Achsenbeschriftungen, und so weiter. Viele Parameter kann man bereits bei der Erstellung der Grafik als Argument übergeben. Eine angepasste Version des Scatterplot-Beispiels von oben ist:\n\nplot(\n    airquality$Wind,\n    airquality$Ozone,\n    xlab=\"Wind (mph)\",  # x-Achsen-Titel\n    ylab=\"Ozone (ppb)\",  # y-Achsen-Titel\n    main=\"New York City air quality (1973)\",  # Titel\n    pch=21,  # Kreissymbol mit separater Rand- und Hintergrundfarbe\n    bg=\"lightblue\"  # Hintergrundfarbe\n)\n\n\n\n\n\n\n\n\nEine verbesserte Version des Ozonverlaufs über die Zeit erhält man, wenn man folgende Argumente setzt (hier wird mit type der Plot-Typ festgelegt, im Beispiel wird \"l\" für eine Liniengrafik gewählt):\n\nplot(\n    airquality$date,\n    airquality$Ozone,\n    xlab=\"\",\n    ylab=\"Ozone (ppb)\",\n    main=\"\",  # kein Titel\n    type=\"l\",  # Liniengrafik\n    col=\"orange\"\n)\n\n\n\n\n\n\n\n\nMan erkennt mit dieser Liniengrafik auch sofort, dass es fehlende Werte in den Daten gibt (dort wo die Linie unterbrochen ist).\nDie diversen Plot-Funktionen haben viele gemeinsame Parameter, mit denen man das Aussehen der Plots beeinflussen kann. In den vorigen Beispielen wurden folgende Parameter verändert:\n\nxlab: x-Achsenbeschriftung\nylab: y-Achsenbeschriftung\ntype: Plot-Typ (Linien, Punkte, beides, …)\npch: Symbol (Kreis, Dreieck, Kreuz, …)\nmain: Titel\ncol: Farbe\n\nMit der Funktion par() kann man alle relevanten Parameter abfragen bzw. global definieren (der Hilfetext ?par liefert eine Beschreibung aller möglichen Parameter). Wenn man diese Funktion ohne Argumente aufruft, erhält man die aktuell gesetzten Werte aller grafischen Parameter. Man kann auch einzelne Parameter global verändern, d.h. nach einer Änderung werden neue Grafiken immer mit den aktuellen Parametern erstellt.\nFolgendes Beispiel demonstriert die Verwendung von par(). Zuerst fragen wir den aktuellen Wert des Parameters col (also die Farbe) ab. Dies funktioniert mit der $-Schreibweise wie beim Herausgreifen von Spalten aus einem Data Frame:\n\npar()$col\n\n[1] \"black\"\n\n\nWir sehen, dass die Farbe auf schwarz gesetzt ist. Dies bestätigt auch eine kleine Beispielgrafik, die aus schwarzen Elementen besteht:\n\nplot(sin(seq(0, 2*pi, length.out=50)), type=\"o\", xlab=\"\", ylab=\"\")\n\n\n\n\n\n\n\n\nSetzen wir die Farbe global auf rot, werden alle nachfolgenden Grafiken diesen Wert verwenden:\n\npar(col=\"red\")\npar()$col\n\n[1] \"red\"\n\nplot(sin(seq(0, 2*pi, length.out=50)), type=\"o\", xlab=\"\", ylab=\"\")\n\n\n\n\n\n\n\n\nBevor wir weitere Grafiken erzeugen, setzen wir die Farbe aber wieder auf schwarz zurück:\n\npar(col=\"black\")\n\nWeitere nützliche Parameter sind lty (Linientyp) und cex.axis (Größe der Achsenbeschriftungen):\n\nplot(\n    sin(seq(0, 2*pi, length.out=50)),\n    type=\"l\",\n    xlab=\"\",\n    ylab=\"\",\n    cex.axis=0.6,  # Skalierung Achsenbeschriftung\n    lty=2,  # Linientyp\n    main=\"Sine\"\n)\n\n\n\n\n\n\n\n\nFolgende Linientypen sind als Werte für lty möglich:\n\n\n\nlty\nTyp\n\n\n\n\n0\nleer\n\n\n1\ndurchgehend (Standard)\n\n\n2\nstrichliert\n\n\n3\ngepunktet\n\n\n4\nPunkt-Strich\n\n\n5\nlange Striche\n\n\n6\nkurzer Strich, langer Strich\n\n\n\nDie Hilfe der Funktion points() listet alle verfügbaren Zeichensymbole für pch auf. Der Parameter cex.axis setzt einen Skalierungsfaktor für die Achsenbeschriftung; standardmäßig ist dieser 1 – Werte kleiner als 1 verkleinern daher die Achsenbeschriftung, Werte größer als 1 vergrößern diese."
  },
  {
    "objectID": "08/08.html#hinzufügen-von-elementen-zu-einer-grafik",
    "href": "08/08.html#hinzufügen-von-elementen-zu-einer-grafik",
    "title": "8 – Grafiken",
    "section": "Hinzufügen von Elementen zu einer Grafik",
    "text": "Hinzufügen von Elementen zu einer Grafik\nMit dem Base-Plotting-System kann man eine Grafik erstellen und danach zusätzliche grafische Elemente hinzufügen. Dazu verwendet man entsprechende Funktionen, die im Folgenden vorgestellt werden.\nEinen Titel kann man mit title() hinzufügen:\n\nwith(airquality, plot(Wind, Ozone))\ntitle(main=\"Ozone and Wind in NYC\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nIn diesem Beispiel verwenden wir außerdem die Funktion with(). Diese ermöglicht es, innerhalb der Klammern Spaltennamen aus einem Data Frame direkt zu verwenden, also z.B. statt airquality$Ozone kann man direkt Ozone schreiben. D.h. die obige erste Zeile könnte man ohne with() auch vollkommen äquivalent als plot(airquality$Wind, airquality$Ozone) schreiben. Beachten Sie allerdings, wie die standardmäßige Achsenbeschriftung davon abhängt, wie man die x- bzw. y-Daten übergibt!\n\n\nMit points() kann man Punkte hinzufügen. Dies kann man beispielsweise verwenden, um Gruppen von Daten in unterschiedlichen Farben darzustellen. Wir beginnen mit einem leeren Plot (type=\"n\") und fügen diesem dann Punkte mit unterschiedlichen Farben und Symbolen hinzu. Die Funktion legend() fügt eine Legende hinzu.\n\nwith(airquality, plot(Wind, Ozone, main=\"\", type=\"n\"))\nwith(subset(airquality, Month != 5), points(Wind, Ozone, col=\"red\", pch=20))\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col=\"blue\", pch=17))\nlegend(\n    \"topright\",\n    pch=c(17, 20),\n    col=c(\"blue\", \"red\"),\n    legend=c(\"May\", \"Other Months\")\n)\n\n\n\n\n\n\n\n\nEine Regressionsgerade kann man mit der Funktion abline() erstellen:\n\nwith(airquality, plot(Wind, Ozone, main=\"\", pch=20))\nmodel = lm(Ozone ~ Wind, airquality)\nabline(model, lwd=2, col=\"blue\")\n\n\n\n\n\n\n\n\nHier übergibt man die Linie in Form einer Geradengleichung, die man mit der Funktion lm() berechnen kann – mehr zu dieser Funktion werden wir später bei den Regressionsmodellen kennenlernen. Im Moment reicht es zu wissen, dass diese Funktion eine Formel der Form y ~ x erwartet, d.h. man gibt die diesen Variablen entsprechenden Spaltennamen der y- bzw. x-Achsen an.\nText und Pfeile kann man mit text() bzw. arrows() hinzufügen:\n\nwith(airquality, plot(Wind, Ozone, main=\"\", pch=20))\ntext(15, 100, \"Label\")\narrows(14.5, 90, 14, 75, length=0.1)"
  },
  {
    "objectID": "08/08.html#darstellung-von-rohdaten",
    "href": "08/08.html#darstellung-von-rohdaten",
    "title": "8 – Grafiken",
    "section": "Darstellung von Rohdaten",
    "text": "Darstellung von Rohdaten\nEs gibt viele Möglichkeiten, die Verteilung einer (numerischen) Variable zu visualisieren (z.B. Histogramme und Boxplots). Prinzipiell sollte man zusätzlich zu zusammenfassenden Größen (wie Mittelwert, Median, Streuung, etc.) immer die Rohdaten in der Grafik darstellen. Dies ist mit der Funktion stripchart() möglich.\nSehen wir uns als Beispiel die Spalte airquality$Ozone an. Man könnte in einer Balkengrafik nur deren Mittelwert darstellen, was aber nicht sehr informativ ist:\n\nbarplot(mean(airquality$Ozone, na.rm=TRUE))\n\n\n\n\n\n\n\n\nEtwas besser wäre ein Boxplot:\n\nboxplot(airquality$Ozone)\n\n\n\n\n\n\n\n\nNoch besser ist es, wenn man zusätzlich die Rohdaten in die Grafik aufnimmt (dies wird mit der Funktion stripchart() bewerkstelligt):\n\nboxplot(airquality$Ozone)\nstripchart(\n    airquality$Ozone,\n    vertical=TRUE,\n    add=TRUE,\n    pch=19,\n    col=rgb(0, 0, 0, 0.25)\n)\n\n\n\n\n\n\n\n\nBeachten Sie, dass add=TRUE übergeben werden muss, wenn man die Punkte von stripchart() zu einer existierenden Grafik hinzufügen möchte (sonst wird eine neue Grafik erzeugt). Man könnte hier auch noch weitere Verbesserungen (wie z.B. method=\"jitter\") vornehmen.\nEin komplettes Beispiel der Boxplots der einzelnen Monate würde wie folgt aussehen:\n\nwith(airquality, boxplot(Ozone ~ Month))\nstripchart(\n    airquality$Ozone ~ airquality$Month,\n    vertical=TRUE,\n    add=TRUE,\n    pch=19,\n    col=rgb(0, 0, 0, 0.25)\n)"
  },
  {
    "objectID": "08/08.html#mehrere-plots-in-einer-grafik",
    "href": "08/08.html#mehrere-plots-in-einer-grafik",
    "title": "8 – Grafiken",
    "section": "Mehrere Plots in einer Grafik",
    "text": "Mehrere Plots in einer Grafik\nMehrere Plots nebeneinander bzw. untereinander kann man mit dem globalen Parameter mfrow bzw. mfcol erzeugen. Hier setzt man den Parameter auf einen Vektor mit zwei Elementen, welcher die Anzahl an Zeilen und Spalten enthält. Beispielsweise entspricht also mfrow=c(3, 2) drei Zeilen und zwei Spalten. Danach erzeugt man die entsprechende Anzahl an Plots mit den diversen Funktionen wie z.B. plot(), hist(), boxplot(), usw. Man setzt mfrow, wenn man die Grafik zeilenweise befüllen will oder alternativ mfcol, wenn man spaltenweisen befüllen will.\n\npar(mfrow=c(1, 2))  # 1 Zeile, 2 Spalten\nwith(airquality, plot(Wind, Ozone, main=\"Ozone and Wind\", pch=20))  # Grafik 1\nwith(airquality, plot(Solar.R, Ozone, main=\"Ozone and Solar Radiation\", pch=20))  # Grafik 2\n\n\n\n\n\n\n\n\nWenn die Grafik fertig ist sollte man den globalen Parameter wieder zurücksetzen, sodass die nächste Grafik wieder aus einer einzigen Darstellung besteht:\n\npar(mfrow=c(1, 1))\n\nNoch flexibler ist die Funktion layout(). Hier spezifiziert man eine Matrix, welche die Nummern der darzustellenden Plots beinhaltet. Möchte man z.B. drei Plots in zwei Zeilen und zwei Spalten darstellen, wobei sich der erste Plot in der ersten Zeile über beide Spalten erstrecken soll, definiert man dies wie folgt:\n\n(grid = matrix(c(1, 1, 2, 3), nrow=2, ncol=2, byrow=TRUE))\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    3\n\n\nNun setzt man das Layout der Grafik auf grid und erstellt hintereinander die drei Plots:\n\nlayout(grid)\nplot(\n    airquality$date,\n    airquality$Temp,\n    type=\"l\",\n    xlab=\"\",\n    ylab=\"Temperature (°F)\",\n    main=\"Temperature\"\n)\nplot(\n    factor(airquality$Month),\n    main=\"Measurements per month\",\n    xlab=\"Month\",\n    ylab=\"Count\",\n    col=\"lightblue\"\n)\nplot(\n    airquality$Temp,\n    airquality$Ozone,\n    type=\"n\",\n    xlab=\"Temperature (°F)\",\n    ylab=\"Ozone (ppb)\",\n    main=\"Ozone vs. Temperature\"\n)\nabline(lm(Ozone ~ Temp, airquality), col=\"orange\", lwd=2)\npoints(airquality$Temp, airquality$Ozone, pch=16, col=rgb(0, 0, 0, 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Angabe der Farbe col=rgb(0, 0, 0, 0.5) im letzten Beispiel definiert die Farbe schwarz über die ersten drei Werte (RGB, also rot, grün und blau) sowie die Transparenz über den vierten Wert (1 bedeutet gar nicht transparent und 0 bedeutet vollkommen transparent – 0.5 ist also halbtransparent).\n\n\nNach der Erstellung der Grafik sollte man auch hier wieder den Parameter zurücksetzen, entweder wie oben mit par(mfrow=c(1, 1)) oder mit:\n\nlayout(1)"
  },
  {
    "objectID": "08/08.html#übungen",
    "href": "08/08.html#übungen",
    "title": "8 – Grafiken",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nLaden Sie den Datensatz penguins aus dem Paket palmerpenguins und erstellen Sie einen Scatterplot der Spalten bill_length_mm auf der x-Achse und bill_depth_mm auf der y-Achse. Beschriften Sie die Achsen mit aussagekräftigen Bezeichnungen.\n\n\nÜbung 2\nErstellen Sie den Scatterplot aus Übung 1 noch einmal, aber stellen Sie diesmal die Punkte der drei Spezies in unterschiedlichen Farben dar. Fügen Sie außerdem eine entsprechende Legende hinzu. Sie können z.B. zuerst einen leeren Plot mit dem Argument type=\"n\" erstellen und dann mit points() nacheinander die Punkte der drei Spezies in unterschiedlichen Farben hinzufügen.\n\n\nÜbung 3\nSehen Sie sich den Datensatz ToothGrowth an (lesen Sie auch die Hilfe dazu) und erstellen Sie damit einen aussagekräftigen Plot. Verwenden Sie Funktionen, die wir in dieser Veranstaltung kennengelernt haben (also plot(), hist() oder boxplot()) – es sind natürlich auch mehrere Plots pro Grafik erlaubt (mittels par(mfrow) bzw. layout())!\n\n\nÜbung 4\nVerwenden Sie den Datensatz mtcars und erstellen Sie einen Boxplot der Variable mpg in Abhängigkeit von cyl. Welche Aussage können Sie aufgrund dieser Grafik machen (welche Fahrzeuge verbrauchen mehr bzw. weniger Kraftstoff)? Achten Sie auf die korrekte Interpretation des Kraftstoffverbrauchs in MPG (miles per gallon)!\n\n\nÜbung 5\nErstellen Sie mit dem Datensatz mtcars die folgenden drei Grafiken in einem Plot:\n\nScatterplot mpg gegen drat\nBoxplot von mpg in Abhängigkeit von cyl (siehe Übung 4)\nHistogramm von mpg\n\nVerwenden Sie eine geeignete Anordnung der drei Grafiken (z.B. mittels layout())!"
  },
  {
    "objectID": "10/10.html",
    "href": "10/10.html",
    "title": "10 – Lineare Regression (1)",
    "section": "",
    "text": "Wir haben gesehen, dass man über die Korrelation die Beziehung zwischen zwei Variablen beschreiben kann. Man kann nun einen Schritt weiter gehen und versuchen, eine Variable durch die andere “vorherzusagen”. Eine weit verbreitete Methode dafür ist die lineare Regression, welche eine abhängige Variable durch eine unabhängige Variable (einfache Regression) bzw. durch mehrere unabhängige Variablen (multiple Regression) zu erklären versucht.\nEin allgemeines statistisches Modell kann man generell wie folgt aufstellen:\n\\[\\mathrm{outcome}_i = \\mathrm{model}_i + \\mathrm{error}_i\\]\nMan beschreibt also die gemessenen Daten durch ein Modell, welches im Allgemeinen aber immer Fehler machen wird (d.h. es beschreibt die gemessenen Daten nicht perfekt). Um diese Formel kürzer anschreiben zu können, setzt man \\(\\mathrm{outcome}_i = y_i\\), \\(\\mathrm{model}_i = \\hat{y}_i\\) und \\(\\mathrm{error}_i = \\varepsilon_i\\):\n\\[y_i = \\hat{y}_i + \\varepsilon_i\\]\nIm Fall der linearen Regression ist das Modell \\(\\hat{y}_i\\) linear, also eine Gerade. Die Gleichung kann daher wie folgt angeschrieben werden:\n\\[y_i = \\underbrace{\\left(b_0 + b_1 x_i\\right)}_{\\hat{y}_i} + \\varepsilon_i\\]\n\n\n\n\n\n\nHinweis\n\n\n\nEine Geradengleichung kann in der Form \\(y = k \\cdot x + d\\) angeschrieben werden. Die zwei Parameter \\(k\\) und \\(d\\) sind wie folgt definiert:\n\nDer Schnittpunkt mit der y-Achse \\(d\\) entspricht dem Wert von \\(y\\) an der Stelle \\(x=0\\).\nDie Steigung \\(k\\) ist das Verhältnis der Änderung von \\(y\\) (wird als \\(\\Delta y\\) geschrieben) zur Änderung von \\(x\\) (wird als \\(\\Delta x\\) geschrieben):\n\\[k = \\frac{\\Delta y}{\\Delta x}\\]\n\n\n\nDie Parameter \\(b_0\\) und \\(b_1\\) beschreiben daher den Schnittpunkt der Geraden mit der y-Achse (Intercept) bzw. die Steigung der Geraden und werden als Regressionskoeffizienten bezeichnet. Der Term \\(\\varepsilon_i\\) beschreibt den Fehler zwischen den vom Modell vorhergesagten Wert \\(\\hat{y}_i\\) und dem tatsächlich gemessenen Wert \\(y_i\\).\nDas tiefgestellte \\(i\\) steht für den \\(i\\)-ten Datenpunkt (von insgesamt \\(N\\)). Das Modell gilt also für alle Datenpunkte \\(i = 1, 2, \\ldots, N\\).\nDie Datenpunkte \\(x_i\\) können als Vektor \\(\\mathbf{x}\\) betrachtet werden; dieser wird als unabhängige Variable, Prädiktor oder Treatment bezeichnet. Die Datenpunkte \\(y_i\\) können ebenfalls als Vektor \\(\\mathbf{y}\\) betrachtet werden; dieser wird als abhängige Variable oder Outcome bezeichnet. Die Fehler \\(\\varepsilon_i\\) werden als Residuen bezeichnet.\n\n\n\n\n\n\nTipp\n\n\n\nZur Veranschaulichung sind in der folgenden Abbildung drei Geraden mit gleichen Intercepts aber unterschiedlichen Steigungen (links) sowie unterschiedlichen Intercepts aber gleichen Steigungen (rechts) abgebildet.\n\n\n\n\n\n\n\n\n\nIn der linken Abbildung haben alle drei Geraden denselben Intercept \\(b_0 = 50\\) (sie schneiden die y-Achse an der Stelle \\(y = 50\\)). Die Steigung der ersten Geraden beträgt \\(b_1 = 1\\). Dies ist aus der Grafik ersichtlich, denn wenn man \\(x\\) beispielsweise um 25 erhöht, dann erhöht sich \\(y\\) ebenfalls um 25. Die Steigung beträgt also \\(b_1 = \\frac{\\Delta y}{\\Delta x} = \\frac{25}{25} = 1\\). Die Steigung der zweiten Geraden beträgt \\(b_1 = -\\frac{1}{3}\\), denn wenn man \\(x\\) um 75 erhöht, dann erniedrigt sich \\(y\\) um 25. Die Steigung beträgt also \\(b_1 = \\frac{-25}{75} = -\\frac{1}{3}\\). Die Steigung der dritten Geraden ist entsprechend \\(b_1 = \\frac{-50}{75} = -\\frac{2}{3}\\).\nIn der rechten Abbildung haben alle drei Geraden die Steigung \\(b_1 = \\frac{25}{50} = \\frac{1}{2}\\). Die erste Gerade schneidet die y-Achse bei \\(y = 70\\), die zweite bei \\(y = 50\\) und die dritte bei \\(y = 20\\), was den drei Intercepts entspricht.\n\n\nDie Frage ist also nun, welche konkrete Gerade (also welches konkrete lineare Regressionsmodell) verwendet werden soll. Das Modell soll die Datenpunkte möglichst gut beschreiben, was meist durch die Methode der kleinsten Fehlerquadrate (Least Squares) erreicht wird. Diese Methode findet jene Gerade, welche die quadrierten Unterschiede zwischen den vom Modell (der Geraden) vorhergesagten Werten \\(\\hat{y}_i\\) und den tatsächlich gemessenen Datenpunkten \\(y_i\\) minimiert. Die Unterschiede (Fehler) werden hier als Residuen bezeichnet.\nIn der folgenden Grafik sind die Daten als schwarze Punkte, das Modell als schwarze Gerade und die Residuen als rote vertikale Linien dargestellt. Beachten Sie, dass es sowohl positive als auch negative Residuen gibt (in der Grafik sind beispielhaft zwei Werte zu sehen). Damit sich die positiven und negativen Terme nicht aufheben, werden die einzelnen Residuen zuerst quadriert und erst dann summiert – und diese Quadratsumme wird minimiert. Es wird also jene Gerade gesucht, welche die kleinste Quadratsumme ergibt. Die von dieser Methode gefundene Gerade hat also die kleinste Fehlerquadratsumme unter allen möglichen Geraden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nWir werden nicht darauf eingehen, wie die Methode der kleinsten Fehlerquadrate im Detail funktioniert. Es ist ausreichend zu wissen, dass diese Methode die optimale Gerade findet, also die Regressionskoeffizienten \\(b_0\\) und \\(b_1\\) so bestimmt, dass die Fehlerquadratsumme minimiert wird."
  },
  {
    "objectID": "10/10.html#einfache-lineare-regression",
    "href": "10/10.html#einfache-lineare-regression",
    "title": "10 – Lineare Regression (1)",
    "section": "",
    "text": "Wir haben gesehen, dass man über die Korrelation die Beziehung zwischen zwei Variablen beschreiben kann. Man kann nun einen Schritt weiter gehen und versuchen, eine Variable durch die andere “vorherzusagen”. Eine weit verbreitete Methode dafür ist die lineare Regression, welche eine abhängige Variable durch eine unabhängige Variable (einfache Regression) bzw. durch mehrere unabhängige Variablen (multiple Regression) zu erklären versucht.\nEin allgemeines statistisches Modell kann man generell wie folgt aufstellen:\n\\[\\mathrm{outcome}_i = \\mathrm{model}_i + \\mathrm{error}_i\\]\nMan beschreibt also die gemessenen Daten durch ein Modell, welches im Allgemeinen aber immer Fehler machen wird (d.h. es beschreibt die gemessenen Daten nicht perfekt). Um diese Formel kürzer anschreiben zu können, setzt man \\(\\mathrm{outcome}_i = y_i\\), \\(\\mathrm{model}_i = \\hat{y}_i\\) und \\(\\mathrm{error}_i = \\varepsilon_i\\):\n\\[y_i = \\hat{y}_i + \\varepsilon_i\\]\nIm Fall der linearen Regression ist das Modell \\(\\hat{y}_i\\) linear, also eine Gerade. Die Gleichung kann daher wie folgt angeschrieben werden:\n\\[y_i = \\underbrace{\\left(b_0 + b_1 x_i\\right)}_{\\hat{y}_i} + \\varepsilon_i\\]\n\n\n\n\n\n\nHinweis\n\n\n\nEine Geradengleichung kann in der Form \\(y = k \\cdot x + d\\) angeschrieben werden. Die zwei Parameter \\(k\\) und \\(d\\) sind wie folgt definiert:\n\nDer Schnittpunkt mit der y-Achse \\(d\\) entspricht dem Wert von \\(y\\) an der Stelle \\(x=0\\).\nDie Steigung \\(k\\) ist das Verhältnis der Änderung von \\(y\\) (wird als \\(\\Delta y\\) geschrieben) zur Änderung von \\(x\\) (wird als \\(\\Delta x\\) geschrieben):\n\\[k = \\frac{\\Delta y}{\\Delta x}\\]\n\n\n\nDie Parameter \\(b_0\\) und \\(b_1\\) beschreiben daher den Schnittpunkt der Geraden mit der y-Achse (Intercept) bzw. die Steigung der Geraden und werden als Regressionskoeffizienten bezeichnet. Der Term \\(\\varepsilon_i\\) beschreibt den Fehler zwischen den vom Modell vorhergesagten Wert \\(\\hat{y}_i\\) und dem tatsächlich gemessenen Wert \\(y_i\\).\nDas tiefgestellte \\(i\\) steht für den \\(i\\)-ten Datenpunkt (von insgesamt \\(N\\)). Das Modell gilt also für alle Datenpunkte \\(i = 1, 2, \\ldots, N\\).\nDie Datenpunkte \\(x_i\\) können als Vektor \\(\\mathbf{x}\\) betrachtet werden; dieser wird als unabhängige Variable, Prädiktor oder Treatment bezeichnet. Die Datenpunkte \\(y_i\\) können ebenfalls als Vektor \\(\\mathbf{y}\\) betrachtet werden; dieser wird als abhängige Variable oder Outcome bezeichnet. Die Fehler \\(\\varepsilon_i\\) werden als Residuen bezeichnet.\n\n\n\n\n\n\nTipp\n\n\n\nZur Veranschaulichung sind in der folgenden Abbildung drei Geraden mit gleichen Intercepts aber unterschiedlichen Steigungen (links) sowie unterschiedlichen Intercepts aber gleichen Steigungen (rechts) abgebildet.\n\n\n\n\n\n\n\n\n\nIn der linken Abbildung haben alle drei Geraden denselben Intercept \\(b_0 = 50\\) (sie schneiden die y-Achse an der Stelle \\(y = 50\\)). Die Steigung der ersten Geraden beträgt \\(b_1 = 1\\). Dies ist aus der Grafik ersichtlich, denn wenn man \\(x\\) beispielsweise um 25 erhöht, dann erhöht sich \\(y\\) ebenfalls um 25. Die Steigung beträgt also \\(b_1 = \\frac{\\Delta y}{\\Delta x} = \\frac{25}{25} = 1\\). Die Steigung der zweiten Geraden beträgt \\(b_1 = -\\frac{1}{3}\\), denn wenn man \\(x\\) um 75 erhöht, dann erniedrigt sich \\(y\\) um 25. Die Steigung beträgt also \\(b_1 = \\frac{-25}{75} = -\\frac{1}{3}\\). Die Steigung der dritten Geraden ist entsprechend \\(b_1 = \\frac{-50}{75} = -\\frac{2}{3}\\).\nIn der rechten Abbildung haben alle drei Geraden die Steigung \\(b_1 = \\frac{25}{50} = \\frac{1}{2}\\). Die erste Gerade schneidet die y-Achse bei \\(y = 70\\), die zweite bei \\(y = 50\\) und die dritte bei \\(y = 20\\), was den drei Intercepts entspricht.\n\n\nDie Frage ist also nun, welche konkrete Gerade (also welches konkrete lineare Regressionsmodell) verwendet werden soll. Das Modell soll die Datenpunkte möglichst gut beschreiben, was meist durch die Methode der kleinsten Fehlerquadrate (Least Squares) erreicht wird. Diese Methode findet jene Gerade, welche die quadrierten Unterschiede zwischen den vom Modell (der Geraden) vorhergesagten Werten \\(\\hat{y}_i\\) und den tatsächlich gemessenen Datenpunkten \\(y_i\\) minimiert. Die Unterschiede (Fehler) werden hier als Residuen bezeichnet.\nIn der folgenden Grafik sind die Daten als schwarze Punkte, das Modell als schwarze Gerade und die Residuen als rote vertikale Linien dargestellt. Beachten Sie, dass es sowohl positive als auch negative Residuen gibt (in der Grafik sind beispielhaft zwei Werte zu sehen). Damit sich die positiven und negativen Terme nicht aufheben, werden die einzelnen Residuen zuerst quadriert und erst dann summiert – und diese Quadratsumme wird minimiert. Es wird also jene Gerade gesucht, welche die kleinste Quadratsumme ergibt. Die von dieser Methode gefundene Gerade hat also die kleinste Fehlerquadratsumme unter allen möglichen Geraden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nWir werden nicht darauf eingehen, wie die Methode der kleinsten Fehlerquadrate im Detail funktioniert. Es ist ausreichend zu wissen, dass diese Methode die optimale Gerade findet, also die Regressionskoeffizienten \\(b_0\\) und \\(b_1\\) so bestimmt, dass die Fehlerquadratsumme minimiert wird."
  },
  {
    "objectID": "10/10.html#quadratsummen",
    "href": "10/10.html#quadratsummen",
    "title": "10 – Lineare Regression (1)",
    "section": "Quadratsummen",
    "text": "Quadratsummen\nDie Fehlerquadratsumme ist ein (relatives) Maß für die Modellgüte (Model Fit). Für die gefundene Gerade ist diese Fehlerquadratsumme zwar immer minimal (unter allen möglichen Geraden), es ist aber trotzdem nicht klar, wie gut sich die Daten überhaupt mit einer Geraden beschreiben lassen. Deswegen vergleicht man das gefundene lineare Modell mit dem einfachsten linearen Modell, welches die Prädiktorvariable vollkommen ignoriert – dem Mittelwert über alle Datenpunkte \\(y_i\\) (das entspricht einer waagrechten Geraden). Bei diesem Modell ist der Wert der Prädiktorvariable \\(x_i\\) also vollkommen egal, da komplett unabhängig davon immer derselbe Wert für die abhängige Variable \\(y_i\\) vorhergesagt wird. Das gefundene lineare Modell ist dann ein guter Fit, wenn es signifikant besser als dieses einfachste Modell ist.\nDie Summe der quadratischen Abweichungen vom einfachsten Modell (Mittelwert) wird auch als SST bezeichnet (totale Quadratsumme). Die Summe der quadratischen Abweichungen vom linearen Modell wird als SSR (Residuenquadratsumme) bezeichnet. Schließlich gibt es noch die quadratischen Abweichungen des linearen Modells vom einfachsten Modell, welche man als SSM (Modellquadratsumme) bezeichnet.\n\n\n\n\n\n\n\n\n\nDie vorangegangenen drei Abbildungen veranschaulichen diese Quadratsummen. SST ist hier nichts anderes als die (nicht normierte) Varianz der Daten. SSM sollte möglichst groß sein, denn dann ist das lineare Modell wesentlich besser als die horizontale Gerade. SSR sollte möglichst klein sein bei einem guten Modell – an SSR sieht man die übrig gebliebene Varianz, die das Modell nicht erklären kann.\nIn mathematischer Notation kann man die Quadratsummen wie folgt anschreiben:\n\\[\\mathrm{SST} = \\sum_{i=1}^N (y_i - \\bar{y})^2\\]\n\\[\\mathrm{SSR} = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\\]\n\\[\\mathrm{SSM} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2\\]\nHier sind \\(y_i\\) die einzelnen Messwerte, \\(\\bar{y}\\) ist der arithmetische Mittelwert und \\(\\hat{y}_i\\) sind die einzelnen vom Modell vorhergesagten Werte (also die auf der Geraden liegenden Werte). \\(N\\) ist die Anzahl der Datenpunkte. In Formeln kann man \\(\\bar{y}\\) und \\(\\hat{y}_i\\) so schreiben:\n\\[\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i\\]\n\\[\\hat{y}_i = b_0 + b_1 x_i\\]\nEs gilt:\n\\[\\mathrm{SST} = \\mathrm{SSM} + \\mathrm{SSR}\\]\nDie totale Quadratsumme wird also in die vom Modell erklärte Quadratsumme und die nicht erklärte Quadratsumme zerlegt."
  },
  {
    "objectID": "10/10.html#modellgüte",
    "href": "10/10.html#modellgüte",
    "title": "10 – Lineare Regression (1)",
    "section": "Modellgüte",
    "text": "Modellgüte\n\nBestimmtheitsmaß \\(R^2\\)\nEin Maß für die Modellgüte ist das Verhältnis von SSM zu SST, welches als Bestimmtheitsmaß \\(R^2\\) bezeichnet wird. Es ist der Anteil an Varianz der Daten, die vom Modell erklärt werden kann:\n\\[R^2 = \\frac{\\mathrm{SSM}}{\\mathrm{SST}}\\]\nDies ist derselbe Wert, den wir auch schon bei der Korrelation kennengelernt haben. Um die Pearson-Korrelation zwischen den beiden Variablen zu bekommen, muss man also nur die Wurzel aus \\(R^2\\) ziehen. Diese Beziehung gilt in dieser Form allerdings nur für die einfache lineare Regression, also nur bei einer einzigen unabhängigen Variable.\n\n\n\\(F\\)-Wert\nEine weitere wichtige Anwendung der Quadratsummen ist die Berechnung des \\(F\\)-Wertes. Dieser entspricht dem Verhältnis der systematischen Varianz (also der vom Modell erklärten Varianz) zur unsystematischen Varianz (also der Varianz, die nicht vom Modell erklärt werden kann). Für den \\(F\\)-Wert verwendet man aber nicht direkt SSM und SSR, sondern die mittleren Quadratsummen. Hier dividiert man die absoluten Quadratsummen durch die jeweiligen Freiheitsgrade und erhält so MSM und MSR. Die Anzahl der Freiheitsgrade von MSM entspricht der Anzahl der geschätzten Modellparameter \\(p\\) minus 1. Die Anzahl der Freiheitsgrade von MSR entspricht der Anzahl der Messwerte \\(N\\) minus der Anzahl der geschätzten Modellparameter \\(p\\).\n\\[\\mathrm{MSM} = \\frac{\\mathrm{SSM}}{\\mathrm{dfM}} = \\frac{\\mathrm{SSM}}{p - 1}\\]\n\\[\\mathrm{MSR} = \\frac{\\mathrm{SSR}}{\\mathrm{dfR}} = \\frac{\\mathrm{SSR}}{N - p}\\]\nFür die einfache Regression gibt es genau zwei Modellparameter \\(b_0\\) und \\(b_1\\), daher ist \\(p = 2\\). Für die Freiheitsgrade der Modellquadratsumme gilt daher:\n\\[\\mathrm{dfM} = p - 1 = 2 - 1 = 1\\]\nDie Freiheitsgrade der Residuenquadratsumme sind in diesem Fall dann\n\\[\\mathrm{dfR} = N - p = N - 2.\\]\nDer \\(F\\)-Wert kann dann wie folgt berechnet werden: \\[F=\\frac{\\mathrm{MSM}}{\\mathrm{MSR}}\\]\nDieser Wert ist ebenso wie \\(R^2\\) ein Maß für die Güte des Modells. Er gibt an, wie viel Varianz das Modell erklärt gegenüber wie viel Varianz das Modell nicht erklärt. D.h. ein Wert von 1 entspricht dem Fall, dass die erklärte Varianz gleich groß ist wie die nicht erklärte – also ein schlechtes Modell. Für ein gutes Modell sollte daher \\(F \\gg 1\\) gelten."
  },
  {
    "objectID": "10/10.html#koeffizienten",
    "href": "10/10.html#koeffizienten",
    "title": "10 – Lineare Regression (1)",
    "section": "Koeffizienten",
    "text": "Koeffizienten\nIn der einfachen linearen Regression entspricht der Regressionskoeffizient \\(b_1\\) der Steigung der Geraden. Er entspricht der Änderung der abhängigen Variable (AV) auf der y-Achse relativ zu einer Änderung der unabhängigen Variable (UV) auf der x-Achse. Ein schlechtes Modell (wie das einfachste horizontale Modell) sagt immer denselben Wert für die AV vorher, unabhängig vom Wert der UV. Die Steigung \\(b_1\\) ist für so ein Modell also Null. Wenn aber die UV den Wert der AV vorhersagen kann, muss die Steigung signifikant von Null verschieden sein. Diese Hypothese kann man mit dem sogenannten \\(t\\)-Test überprüfen. Ein \\(t\\)-Test kann also in der linearen Regression eingesetzt werden, um zu beurteilen, ob eine UV ein signifikanter Prädiktor für die AV ist.\nDie \\(t\\)-Statistik vergleicht das Modell mit seinem Fehler; konkret möchte man hier wissen, ob der beobachtete Wert des Regressionskoeffizienten groß gegenüber seinem Standardfehler ist:\n\\[t = \\frac{b}{\\mathrm{SE}_b}\\]\nDie Freiheitsgrade dieser t-Statistik sind \\(N-p\\), also im Fall der einfachen linearen Regression \\(N-2\\)."
  },
  {
    "objectID": "10/10.html#beispiel",
    "href": "10/10.html#beispiel",
    "title": "10 – Lineare Regression (1)",
    "section": "Beispiel",
    "text": "Beispiel\nWie man eine Regressionsanalyse in R durchführt lässt sich am besten anhand eines Beispiels zeigen. Importieren wir dazu einen Datensatz sales1.dat, welcher Daten über Musikalbenverkäufe (Spalte sales) und die Höhe des Werbebudgets (Spalte adverts) enthält:\n\nlibrary(readr)\n(album = read_tsv(\"sales1.dat\"))\n\n# A tibble: 200 × 2\n   adverts sales\n     &lt;dbl&gt; &lt;dbl&gt;\n 1    10.3   330\n 2   986.    120\n 3  1446.    360\n 4  1188.    270\n 5   575.    220\n 6   569.    170\n 7   472.     70\n 8   537.    210\n 9   514.    200\n10   174.    300\n# ℹ 190 more rows\n\n\nEs ist anzunehmen, dass ein höheres Werbebudget zu höheren Verkaufszahlen führt. Neben der Berechnung diverser deskriptiver Statistiken (wird hier nicht durchgeführt) ist es sinnvoll, die Daten vor einer Regressionsanalyse grafisch darzustellen. Hier bietet sich ein Scatterplot mit überlagerter Regressionsgeraden an (das Argument von abline(), nämlich lm(sales ~ adverts, data=album), ist das Regressionsmodell und wird gleich erklärt):\n\nplot(\n    x=album$adverts,\n    y=album$sales,\n    pch=21,\n    bg=rgb(0, 0, 0, 0.5),\n    xlab=\"Adverts (1000 EUR)\",\n    ylab=\"Sales (1000)\"\n)\nabline(lm(sales ~ adverts, data=album), col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nEs ist klar, dass eine positive Beziehung zwischen den beiden Variablen besteht (je größer das Werbebudget desto mehr Albenverkäufe). Außerdem ist die Steigung der Regressionsgeraden stark verschieden von Null, d.h. es ist zu erwarten, dass das Regressionsmodell signifikant ist.\nIn R kann man mit der Funktion lm() (steht für “linear model”) eine Regressionsanalyse durchführen:\n\nmodel = lm(sales ~ adverts, data=album)\n\nDas erste Argument sales ~ adverts ist eine Formel (welche durch eine Tilde ~ gekennzeichnet ist). Diese Formel kann man als “sales wird vorhergesagt durch adverts” lesen. Im Allgemeinen nimmt die Formel die Form AV ~ UV an. Mit dem Argument data=album teilt man der Funktion mit, dass sich die Namen in der Formel auf Spaltennamen des Data Frames album beziehen.\nDas Ergebnis der Regressionsanalyse weisen wir hier der Variablen model zu. Eine kompakte Darstellung des Ergebnisses kann man sich ausgeben lassen, indem man sich eine Zusammenfassung des Modells mittels summary() ansieht:\n\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ adverts, data = album)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-152.949  -43.796   -0.393   37.040  211.866 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nBeginnen wir mit der vorletzten Zeile:\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313\nHier sehen wir \\(R^2\\), also das Verhältnis von SSM zu SST. Daraus können wir schließen, dass die Werbeausgaben ca. 33.5% der Varianz der Albenverkäufe erklären können. Dies bedeutet natürlich, dass ca. 66.5% der Varianz unerklärt ist, d.h. es muss andere relevante Faktoren dafür geben, die wir nicht im Modell berücksichtigt haben.\nIm Fall der einfachen linearen Regression können wir auch sofort die Pearson-Korrelation zwischen den beiden Variablen berechnen, indem wir die Wurzel aus \\(R^2\\) ziehen:\n\nsqrt(summary(model)$r.squared)\n\n[1] 0.5784877\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDas Objekt summary(model) ist eine Liste, welche diverse Informationen über das Modell enthält. Man kann die einzelnen Elemente der Liste mit dem $-Operator (oder mit [[ ]]) extrahieren, genau wie bei einem Data Frame. Eine Übersicht über die Elemente der Liste erhält man mit names(summary(model)):\n\nnames(summary(model))\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\"  \"aliased\"       \"sigma\"         \"df\"           \n [8] \"r.squared\"     \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\n\n\nDiesen Wert könnte man jetzt vergleichen mit jenem aus der direkten Berechnung der Korrelation (z.B. mit cor() oder cor.test()) – das Ergebnis ist identisch:\n\ncor(album$adverts, album$sales)\n\n[1] 0.5784877\n\n\nDie letzte Zeile der Modellzusammenfassung zeigt die \\(F\\)-Statistik und deren Signifikanz:\nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\nDie \\(F\\)-Statistik ist das Verhältnis von MSM zu MSR. Der Wert liegt bei 99.59, was bei Freiheitsgraden 1 und 198 einer Signifikanz von \\(p&lt;0.001\\) entspricht. Dies bedeutet, dass die Wahrscheinlichkeit kleiner als 0.1% ist, diesen \\(F\\)-Wert (oder einen noch größeren) unter der Nullhypothese (das Modell unterscheidet sich nicht vom einfachen Mittelwertsmodell) zu erhalten. Wir können also daraus schließen, dass das lineare Modell signifikant besser als das einfachste Modell ist.\nDer \\(F\\)-Wert bedeutet also, dass das Modell insgesamt ein guter Fit der Daten ist (verglichen mit dem globalen Mittelwert). Es wird aber eigentlich keine Aussage über die individuellen Prädiktoren getroffen (wobei man im Fall der einfachen Regression natürlich darauf schließen kann, dass die Steigung \\(b_1\\) dann ebenfalls ein guter Prädiktor ist). In der Zusammenfassung sind die beiden Regressionskoeffizienten wie folgt dargestellt:\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nIn der Zeile (Intercept) enthält die Spalte Estimate den Wert für \\(b_0\\), also die Albenverkäufe wenn das Werbebudget gleich 0 ist (Schnittpunkt mit der y-Achse). Dieser Wert beträgt 1.341e+02, also 134.1, was 134100 Verkäufen entspricht (weil die Einheit der sales-Variable 1000 Verkäufe ist).\nDer Wert für \\(b_1\\), also die Steigung der Geraden, ist in der Zeile adverts mit 9.612e-02, also 0.09612 abzulesen. Das bedeutet, wenn die Prädiktorvariable adverts sich um eine Einheit ändert, dann ändert sich die Ergebnisvariable sales um 0.09612 Einheiten. Wenn man das Werbebudget also um 1000€ erhöht, verkauft man 96 Alben mehr.\nWeiters sieht man die Standardfehler der Koeffizienten sowie deren \\(t\\)-Werte. Die letzte Spalte Pr(&gt;|t|) gibt die \\(p\\)-Werte an inklusive Codes für signifikante Ergebnisse. Beide Koeffizienten sind signifikant mit \\(p&lt;0.001\\), wobei uns hier eigentlich nur die Steigung der Geraden interessiert und es für uns nicht wichtig ist, ob der Intercept signifikant von Null verschieden ist."
  },
  {
    "objectID": "10/10.html#werte-vorhersagen",
    "href": "10/10.html#werte-vorhersagen",
    "title": "10 – Lineare Regression (1)",
    "section": "Werte vorhersagen",
    "text": "Werte vorhersagen\nDas Objekt model kann man nun verwenden, um neue Werte für sales in Abhängigkeit von adverts vorherzusagen. Hierfür muss man einfach die berechneten Werte von \\(b_0\\) und \\(b_1\\) in das lineare Modell einsetzen. Die exakten Koeffizienten erhält man mit model$coefficients oder coefficients(model).\n\\[\\hat{y} = b_0 + b_1 x = 134.1 + 0.09612 \\cdot x\\]\nNun kann man berechnen, wie hoch die Verkäufe \\(y\\) wären, wenn man ein Werbebudget von \\(x=100\\) hätte:\n\\[\\hat{y} = 134.1 + 0.09612 \\cdot 100 = 143.75\\]\nD.h. bei einem Werbebudget von 100000 EUR würde man 143750 Alben verkaufen.\nEinfacher und allgemeiner kann man aber die Funktion predict() verwenden. Als Argumente gibt man das Modell und die neuen Daten an (welche als Data Frame übergeben werden müssen):\n\npredict(model, data.frame(adverts=100))\n\n       1 \n143.7524 \n\n\nSo kann man auch gleich Vorhersagen für mehrere Werte gleichzeitig berechnen:\n\npredict(model, data.frame(adverts=c(0, 10, 100, 2000)))\n\n       1        2        3        4 \n134.1399 135.1012 143.7524 326.3889"
  },
  {
    "objectID": "10/10.html#übungen",
    "href": "10/10.html#übungen",
    "title": "10 – Lineare Regression (1)",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nDie Datei cars.csv enthält mehrere Messwerte für den Bremsweg (dist) von Autos die bei einer bestimmten Geschwindigkeit (speed) bremsen. Wir wollen untersuchen, ob es einen (linearen) Zusammenhang zwischen der Geschwindigkeit und dem Bremsweg gibt. Stellen Sie zunächst beide Variablen in einem Scatterplot dar (speed auf der x-Achse und dist auf der y-Achse).\n\n\nÜbung 2\nFühren Sie mit den Daten aus Übung 1 eine lineare Regressionsanalyse durch und stellen Sie die Ergebnisse zusammengefasst dar. Erwähnen Sie die wichtigsten Eckpunkte Ihres Regressionsmodells.\n\n\nÜbung 3\nBerechnen Sie die Pearson-Korrelation zwischen den beiden Variablen (ohne Verwendung von \\(R^2\\) aus dem Modell) und überprüfen Sie, ob dieser Wert mit \\(R^2\\) aus dem Regressionsmodell übereinstimmt.\n\n\nÜbung 4\nWie lautet die Gleichung der Regressionsgeraden? Welche Bremswege sagt das Modell für Geschwindigkeiten von 5 bzw. 65 voraus? Verwenden Sie für die Vorhersage sowohl die Geradengleichung als auch die Funktion predict()."
  },
  {
    "objectID": "01/01-solutions.html",
    "href": "01/01-solutions.html",
    "title": "1 – Lösungen",
    "section": "",
    "text": "Die R-Version wird in der Console als Einleitungstext direkt nach dem Starten angezeigt. Die gesamte Versionsinformation lautet beispielsweise:\nR version 4.5.1 (2025-06-13) -- \"Great Square Root\"\nAußerdem ist die R-Version immer im Titel der Console ersichtlich.\nDie RStudio-Version lautet z.B. 2025.09.0 Build 387 – dies kann man im Hilfefenster, welches mit “Help” – “About RStudio” aufgerufen werden kann, ablesen."
  },
  {
    "objectID": "01/01-solutions.html#übung-1",
    "href": "01/01-solutions.html#übung-1",
    "title": "1 – Lösungen",
    "section": "",
    "text": "Die R-Version wird in der Console als Einleitungstext direkt nach dem Starten angezeigt. Die gesamte Versionsinformation lautet beispielsweise:\nR version 4.5.1 (2025-06-13) -- \"Great Square Root\"\nAußerdem ist die R-Version immer im Titel der Console ersichtlich.\nDie RStudio-Version lautet z.B. 2025.09.0 Build 387 – dies kann man im Hilfefenster, welches mit “Help” – “About RStudio” aufgerufen werden kann, ablesen."
  },
  {
    "objectID": "01/01-solutions.html#übung-2",
    "href": "01/01-solutions.html#übung-2",
    "title": "1 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\n4 * pi * 6371^2\n\n[1] 510064472\n\n\nDie Erdoberfläche beträgt ungefähr 510 Millionen km²."
  },
  {
    "objectID": "01/01-solutions.html#übung-3",
    "href": "01/01-solutions.html#übung-3",
    "title": "1 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\n(11 + 27 + 15 + 10 + 33 + 18 + 25 + 22 + 39 + 11) / 10\n\n[1] 21.1\n\n(11 * 27 * 15 * 10 * 33 * 18 * 25 * 22 * 39 * 11)^(1/10)\n\n[1] 19.03467"
  },
  {
    "objectID": "01/01-solutions.html#übung-4",
    "href": "01/01-solutions.html#übung-4",
    "title": "1 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\nsqrt(2) * (5^5 - pi) * 18 / ((7/5 + 13.2) * 7^(2/3))\n\n[1] 1487.473"
  },
  {
    "objectID": "01/01-solutions.html#übung-5",
    "href": "01/01-solutions.html#übung-5",
    "title": "1 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\nDer Code funktioniert nicht, weil es das Objekt PI nicht gibt (Error: object 'PI' not found). Groß-/Kleinschreibung ist relevant, und der korrekte Name von \\(\\pi\\) lautet pi (in Kleinbuchstaben):\n\n2 * 1 * pi\n\n[1] 6.283185"
  },
  {
    "objectID": "a1/a1-solutions.html",
    "href": "a1/a1-solutions.html",
    "title": "Lösungen",
    "section": "",
    "text": "library(palmerpenguins)\n\n# a.\nsummary(penguins[[\"body_mass_g\"]])  # 2 fehlende Werte\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\ndf = penguins[!is.na(penguins[[\"body_mass_g\"]]),]\n\n# b.\nby(df[[\"body_mass_g\"]], df[[\"species\"]], mean)\n\ndf[[\"species\"]]: Adelie\n[1] 3700.662\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Chinstrap\n[1] 3733.088\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Gentoo\n[1] 5076.016\n\nby(df[[\"body_mass_g\"]], df[[\"species\"]], sd)\n\ndf[[\"species\"]]: Adelie\n[1] 458.5661\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Chinstrap\n[1] 384.3351\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Gentoo\n[1] 504.1162\n\n# c.\ndf[[\"id\"]] = 1:nrow(df)\n\n# d.\nlibrary(afex)\n\naov_ez(id=\"id\", dv=\"body_mass_g\", data=df, between=\"species\")\n\nAnova Table (Type 3 tests)\n\nResponse: body_mass_g\n   Effect     df       MSE          F  ges p.value\n1 species 2, 339 213697.59 343.63 *** .670   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nJa, die Spezies unterscheiden sich signifikant in ihrer Körpermasse (\\(F(2, 339) = 213697.59\\), \\(p &lt; .001\\))."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-1",
    "href": "a1/a1-solutions.html#übung-1",
    "title": "Lösungen",
    "section": "",
    "text": "library(palmerpenguins)\n\n# a.\nsummary(penguins[[\"body_mass_g\"]])  # 2 fehlende Werte\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\ndf = penguins[!is.na(penguins[[\"body_mass_g\"]]),]\n\n# b.\nby(df[[\"body_mass_g\"]], df[[\"species\"]], mean)\n\ndf[[\"species\"]]: Adelie\n[1] 3700.662\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Chinstrap\n[1] 3733.088\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Gentoo\n[1] 5076.016\n\nby(df[[\"body_mass_g\"]], df[[\"species\"]], sd)\n\ndf[[\"species\"]]: Adelie\n[1] 458.5661\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Chinstrap\n[1] 384.3351\n------------------------------------------------------------------------------------------ \ndf[[\"species\"]]: Gentoo\n[1] 504.1162\n\n# c.\ndf[[\"id\"]] = 1:nrow(df)\n\n# d.\nlibrary(afex)\n\naov_ez(id=\"id\", dv=\"body_mass_g\", data=df, between=\"species\")\n\nAnova Table (Type 3 tests)\n\nResponse: body_mass_g\n   Effect     df       MSE          F  ges p.value\n1 species 2, 339 213697.59 343.63 *** .670   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nJa, die Spezies unterscheiden sich signifikant in ihrer Körpermasse (\\(F(2, 339) = 213697.59\\), \\(p &lt; .001\\))."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-2",
    "href": "a1/a1-solutions.html#übung-2",
    "title": "Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nset.seed(123)\ndf = data.frame(\n    x = rnorm(1000, mean=10, sd=2),\n    y = sample(LETTERS, 1000, replace=TRUE)\n)\ndf[[\"z\"]] = df[[\"x\"]] ** 2\n\n# a.\ncolMeans(df[, -2])\n\n        x         z \n 10.03226 104.57606 \n\n# b.\nnrow(df[df[[\"y\"]] == \"A\",])\n\n[1] 31\n\n# c.\nnrow(df[df[[\"z\"]] &gt; 100,])\n\n[1] 505\n\n# d.\nnrow(df[df[[\"y\"]] == \"A\" & df[[\"z\"]] &gt; 100,])\n\n[1] 14"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-3",
    "href": "a1/a1-solutions.html#übung-3",
    "title": "Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nsummary(penguins)  # 165 female, 168 male, 11 NAs\n\n      species          island    bill_length_mm  bill_depth_mm   flipper_length_mm  body_mass_g       sex     \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10   Min.   :172.0     Min.   :2700   female:165  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60   1st Qu.:190.0     1st Qu.:3550   male  :168  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30   Median :197.0     Median :4050   NA's  : 11  \n                                 Mean   :43.92   Mean   :17.15   Mean   :200.9     Mean   :4202               \n                                 3rd Qu.:48.50   3rd Qu.:18.70   3rd Qu.:213.0     3rd Qu.:4750               \n                                 Max.   :59.60   Max.   :21.50   Max.   :231.0     Max.   :6300               \n                                 NA's   :2       NA's   :2       NA's   :2         NA's   :2                  \n      year     \n Min.   :2007  \n 1st Qu.:2007  \n Median :2008  \n Mean   :2008  \n 3rd Qu.:2009  \n Max.   :2009  \n               \n\nsummary(penguins[[\"sex\"]])\n\nfemale   male   NA's \n   165    168     11 \n\npenguins_f = penguins[penguins[[\"sex\"]] == \"female\",]\npenguins_m = penguins[penguins[[\"sex\"]] == \"male\",]\n\n# Alternative mit subset():\npenguins_f = subset(penguins, sex == \"female\")\npenguins_m = subset(penguins, sex == \"male\")"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-4",
    "href": "a1/a1-solutions.html#übung-4",
    "title": "Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\nx = seq(1, 100, 2)\nsummary(x)  # enthält alles außer sd\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    25.5    50.0    50.0    74.5    99.0 \n\nmean(x)\n\n[1] 50\n\nmedian(x)\n\n[1] 50\n\nsd(x)\n\n[1] 29.15476\n\nquantile(x)\n\n  0%  25%  50%  75% 100% \n 1.0 25.5 50.0 74.5 99.0"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-5",
    "href": "a1/a1-solutions.html#übung-5",
    "title": "Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\n(m1 = matrix(rep(1:5, each=25), nrow=25, ncol=5))\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1    2    3    4    5\n [2,]    1    2    3    4    5\n [3,]    1    2    3    4    5\n [4,]    1    2    3    4    5\n [5,]    1    2    3    4    5\n [6,]    1    2    3    4    5\n [7,]    1    2    3    4    5\n [8,]    1    2    3    4    5\n [9,]    1    2    3    4    5\n[10,]    1    2    3    4    5\n[11,]    1    2    3    4    5\n[12,]    1    2    3    4    5\n[13,]    1    2    3    4    5\n[14,]    1    2    3    4    5\n[15,]    1    2    3    4    5\n[16,]    1    2    3    4    5\n[17,]    1    2    3    4    5\n[18,]    1    2    3    4    5\n[19,]    1    2    3    4    5\n[20,]    1    2    3    4    5\n[21,]    1    2    3    4    5\n[22,]    1    2    3    4    5\n[23,]    1    2    3    4    5\n[24,]    1    2    3    4    5\n[25,]    1    2    3    4    5\n\n(m2 = matrix(rep(1:25, each=5), nrow=25, ncol=5, byrow=TRUE))\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1    1    1    1    1\n [2,]    2    2    2    2    2\n [3,]    3    3    3    3    3\n [4,]    4    4    4    4    4\n [5,]    5    5    5    5    5\n [6,]    6    6    6    6    6\n [7,]    7    7    7    7    7\n [8,]    8    8    8    8    8\n [9,]    9    9    9    9    9\n[10,]   10   10   10   10   10\n[11,]   11   11   11   11   11\n[12,]   12   12   12   12   12\n[13,]   13   13   13   13   13\n[14,]   14   14   14   14   14\n[15,]   15   15   15   15   15\n[16,]   16   16   16   16   16\n[17,]   17   17   17   17   17\n[18,]   18   18   18   18   18\n[19,]   19   19   19   19   19\n[20,]   20   20   20   20   20\n[21,]   21   21   21   21   21\n[22,]   22   22   22   22   22\n[23,]   23   23   23   23   23\n[24,]   24   24   24   24   24\n[25,]   25   25   25   25   25"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-6",
    "href": "a1/a1-solutions.html#übung-6",
    "title": "Lösungen",
    "section": "Übung 6",
    "text": "Übung 6\n\nlibrary(readr)\n\ndf = read_csv2(\"thg-emissionen_1990-2022_nach_crf_long.csv\")\n\n# a.\nunique(df[[\"Schadstoff\"]])  # 8 verschiedene Schadstoffe\n\n[1] \"THG\" \"CO2\" \"CH4\" \"N2O\" \"HFC\" \"PFC\" \"SF6\" \"NF3\"\n\n# b.\nlength(unique(df[[\"CRF_Code\"]]))  # 83 verschiedene Codes\n\n[1] 83\n\n# c.\ndf = df[df[[\"CRF_Code\"]] == \"0\" & df[[\"Schadstoff\"]] == \"CO2\",]\ndf\n\n# A tibble: 33 × 9\n   Region Schadstoff Einheit CRF_Code CRF_Sektor             Quelle               Datenstand  Jahr     Werte\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1990 62183904.\n 2 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1991 65782394.\n 3 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1992 60282904.\n 4 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1993 60708862.\n 5 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1994 61083354.\n 6 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1995 64060755.\n 7 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1996 67405241.\n 8 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1997 67301823.\n 9 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1998 66936726.\n10 AT     CO2        t CO2   0        Total (without LULUCF) OLI 2023 (1990-2022)   15012024  1999 65667804.\n# ℹ 23 more rows"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-7",
    "href": "a1/a1-solutions.html#übung-7",
    "title": "Lösungen",
    "section": "Übung 7",
    "text": "Übung 7\n\nby(InsectSprays$count, InsectSprays$spray, mean)\n\nInsectSprays$spray: A\n[1] 14.5\n------------------------------------------------------------------------------------------ \nInsectSprays$spray: B\n[1] 15.33333\n------------------------------------------------------------------------------------------ \nInsectSprays$spray: C\n[1] 2.083333\n------------------------------------------------------------------------------------------ \nInsectSprays$spray: D\n[1] 4.916667\n------------------------------------------------------------------------------------------ \nInsectSprays$spray: E\n[1] 3.5\n------------------------------------------------------------------------------------------ \nInsectSprays$spray: F\n[1] 16.66667\n\nboxplot(\n    count ~ spray,\n    data=InsectSprays,\n    xlab=\"Type of spray\",\n    ylab=\"Insect count\",\n    main=\"Insect Sprays Effectiveness\"\n)"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-8",
    "href": "a1/a1-solutions.html#übung-8",
    "title": "Lösungen",
    "section": "Übung 8",
    "text": "Übung 8\n\nlibrary(readr)\n\ndf = read_delim(\"temperature.csv\")\ndf[[\"Average\"]] = rowMeans(subset(df, select=-YEAR))\n\nwith(\n    df,\n    plot(\n        x=YEAR,\n        y=Average,\n        type=\"l\",\n        main=\"Innsbruck\",\n        xlab=\"Year\",\n        ylab=\"Average Temperature (°C)\"\n    )\n)"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-9",
    "href": "a1/a1-solutions.html#übung-9",
    "title": "Lösungen",
    "section": "Übung 9",
    "text": "Übung 9\n\n(mtcars1 = subset(mtcars, mpg &gt;= 20 & hp &gt;= 110 & cyl == 6))\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-10",
    "href": "a1/a1-solutions.html#übung-10",
    "title": "Lösungen",
    "section": "Übung 10",
    "text": "Übung 10\n\na = seq(100, 0, by=-4)\nb = seq(2.5, 10, by=0.3)\nc = seq(14, 15, length.out=35)\nd = rep(c(\"A\", \"B\", \"C\"), each=50)\ne = seq(4, 96, by=2)\nf = seq(3, 97, by=2)"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-11",
    "href": "a1/a1-solutions.html#übung-11",
    "title": "Lösungen",
    "section": "Übung 11",
    "text": "Übung 11\n\nlibrary(lm.beta)\nstr(attitude)\n\n'data.frame':   30 obs. of  7 variables:\n $ rating    : num  43 63 71 61 81 43 58 71 72 67 ...\n $ complaints: num  51 64 70 63 78 55 67 75 82 61 ...\n $ privileges: num  30 51 68 45 56 49 42 50 72 45 ...\n $ learning  : num  39 54 69 47 66 44 56 55 67 47 ...\n $ raises    : num  61 63 76 54 71 54 66 70 71 62 ...\n $ critical  : num  92 73 86 84 83 49 68 66 83 80 ...\n $ advance   : num  45 47 48 35 47 34 35 41 31 41 ...\n\nmodel = lm(rating ~ complaints + privileges + learning + raises + critical + advance, data=attitude)\nsummary(model)\n\n\nCall:\nlm(formula = rating ~ complaints + privileges + learning + raises + \n    critical + advance, data = attitude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \ncomplaints   0.61319    0.16098   3.809 0.000903 ***\nprivileges  -0.07305    0.13572  -0.538 0.595594    \nlearning     0.32033    0.16852   1.901 0.069925 .  \nraises       0.08173    0.22148   0.369 0.715480    \ncritical     0.03838    0.14700   0.261 0.796334    \nadvance     -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n\nAus der Modellzusammenfassung ergeben sich folgende Antworten:\n\nDer einzige signifikante Prädiktor ist complaints mit \\(b = 0.61319\\)\n\\(R^2\\) beträgt 0.7326\nDas Modell ist signifikant mit \\(F(6, 23) = 10.5\\), \\(p &lt; .001\\)\nStandardisierte Regressionskoeffizienten erhält man mit der Funktion lm.beta():\n\n\nlm.beta(model)\n\n\nCall:\nlm(formula = rating ~ complaints + privileges + learning + raises + \n    critical + advance, data = attitude)\n\nStandardized Coefficients::\n(Intercept)  complaints  privileges    learning      raises    critical     advance \n         NA  0.67072520 -0.07342743  0.30887024  0.06981172  0.03119975 -0.18346445"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-12",
    "href": "a1/a1-solutions.html#übung-12",
    "title": "Lösungen",
    "section": "Übung 12",
    "text": "Übung 12\n\nx = 1:100\nlog(sqrt(mean(x^2 + 3, trim=0.25)), base=2)\n\n[1] 5.715618\n\n(x^2 + 3) |&gt;\n    mean(trim=0.25) |&gt;\n    sqrt() |&gt;\n    log(base=2)\n\n[1] 5.715618"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-13",
    "href": "a1/a1-solutions.html#übung-13",
    "title": "Lösungen",
    "section": "Übung 13",
    "text": "Übung 13\n\nset.seed(1)\ndf = data.frame(\n    a=sample(0:1000, 20, replace=TRUE),\n    b=sample(0:1000, 20, replace=FALSE)\n)\n\ncolMeans(df)\n\n     a      b \n585.30 628.75 \n\ncolMeans(subset(df, a %% 2 == 1))\n\n      a       b \n706.750 634.625"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-14",
    "href": "a1/a1-solutions.html#übung-14",
    "title": "Lösungen",
    "section": "Übung 14",
    "text": "Übung 14\n\n(x = c(\"1\", \"9\", \"X\", \"13\", \"Y\", \"8\", \"27\"))\n\n[1] \"1\"  \"9\"  \"X\"  \"13\" \"Y\"  \"8\"  \"27\"\n\n(y = as.numeric(x))\n\nWarning: NAs introduced by coercion\n\n\n[1]  1  9 NA 13 NA  8 27\n\nmean(y, na.rm=TRUE)  # 11.6\n\n[1] 11.6\n\n(df = data.frame(x, y, z=1))\n\n   x  y z\n1  1  1 1\n2  9  9 1\n3  X NA 1\n4 13 13 1\n5  Y NA 1\n6  8  8 1\n7 27 27 1"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-15",
    "href": "a1/a1-solutions.html#übung-15",
    "title": "Lösungen",
    "section": "Übung 15",
    "text": "Übung 15\n\nresult = sqrt(((25 + 7.1) / (5 * pi))^4 + (-6.2 * 2/3)^2 / (0.8 + 3^(2/7)))\nround(result, 3)\n\n[1] 5.032"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-16",
    "href": "a1/a1-solutions.html#übung-16",
    "title": "Lösungen",
    "section": "Übung 16",
    "text": "Übung 16\n\nset.seed(4)\nx = rnorm(100, mean=-4, sd=5)\ny = 0.29 * x + 2 * rnorm(100, mean=2, sd=2)\n\nmodel = lm(y ~ x)\nplot(x, y)\nabline(model, col=\"red\")\n\n\n\n\n\n\n\ncor.test(x, y)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 1.9281, df = 98, p-value = 0.05673\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.005442771  0.373569864\nsample estimates:\n      cor \n0.1911797 \n\n\nDie Ausgabe der Funktion cor.test(x, y) zeigt, dass \\(r = .191\\) und \\(p = .057\\). Da \\(p &gt; .05\\), ist die Korrelation nicht signifikant."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-17",
    "href": "a1/a1-solutions.html#übung-17",
    "title": "Lösungen",
    "section": "Übung 17",
    "text": "Übung 17\n\nlibrary(readr)\n\ndf = read_csv(\"inflation.csv\", na=\":\")\n\nplot(df$date, df$AT, type=\"l\", col=\"red\")\nlines(df$date, df$DE, type=\"l\", col=\"blue\")\nlines(df$date, df$CH, type=\"l\", col=\"green\")\nlegend(\"topleft\", col=c(\"red\", \"blue\", \"green\"), lty=1, legend=c(\"AT\", \"DE\", \"CH\"))"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-18",
    "href": "a1/a1-solutions.html#übung-18",
    "title": "Lösungen",
    "section": "Übung 18",
    "text": "Übung 18\n\nset.seed(4)\nx = sample(-10:10, 500, replace=TRUE)\n\ny = x[x %% 2 == 1]\nlength(x)\n\n[1] 500\n\nlength(y)\n\n[1] 236\n\nmean(x)  # 0.428\n\n[1] 0.428\n\nmean(y)  # 0.644\n\n[1] 0.6440678"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-19",
    "href": "a1/a1-solutions.html#übung-19",
    "title": "Lösungen",
    "section": "Übung 19",
    "text": "Übung 19\n\nset.seed(4)\nx = sample(c(-10:10, NA), 500, replace=TRUE)\n\nmean(x, na.rm=TRUE)\n\n[1] 0.3907563\n\nsd(x, na.rm=TRUE)\n\n[1] 6.241343\n\nquantile(x, probs=c(0, 0.25, 0.33, 0.5, 0.66, 0.75, 1), na.rm=TRUE)\n\n  0%  25%  33%  50%  66%  75% 100% \n -10   -5   -3    1    4    6   10"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-20",
    "href": "a1/a1-solutions.html#übung-20",
    "title": "Lösungen",
    "section": "Übung 20",
    "text": "Übung 20\n\nlibrary(psych)\nlibrary(readr)\n\nwine = read_delim(\"winequality-white.csv\")\ndescribe(wine)\n\n                     vars    n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se\nfixed acidity           1 4898   6.85  0.84   6.80    6.82  0.74 3.80  14.20  10.40 0.65     2.17 0.01\nvolatile acidity        2 4898   0.28  0.10   0.26    0.27  0.09 0.08   1.10   1.02 1.58     5.08 0.00\ncitric acid             3 4898   0.33  0.12   0.32    0.33  0.09 0.00   1.66   1.66 1.28     6.16 0.00\nresidual sugar          4 4898   6.39  5.07   5.20    5.80  5.34 0.60  65.80  65.20 1.08     3.46 0.07\nchlorides               5 4898   0.05  0.02   0.04    0.04  0.01 0.01   0.35   0.34 5.02    37.51 0.00\nfree sulfur dioxide     6 4898  35.31 17.01  34.00   34.36 16.31 2.00 289.00 287.00 1.41    11.45 0.24\ntotal sulfur dioxide    7 4898 138.36 42.50 134.00  136.96 43.00 9.00 440.00 431.00 0.39     0.57 0.61\ndensity                 8 4898   0.99  0.00   0.99    0.99  0.00 0.99   1.04   0.05 0.98     9.78 0.00\npH                      9 4898   3.19  0.15   3.18    3.18  0.15 2.72   3.82   1.10 0.46     0.53 0.00\nsulphates              10 4898   0.49  0.11   0.47    0.48  0.10 0.22   1.08   0.86 0.98     1.59 0.00\nalcohol                11 4898  10.51  1.23  10.40   10.43  1.48 8.00  14.20   6.20 0.49    -0.70 0.02\nquality                12 4898   5.88  0.89   6.00    5.85  1.48 3.00   9.00   6.00 0.16     0.21 0.01\n\n# mean of \"alcohol\": 10.51\n# median of \"free sulfur dioxide\": 34.00\n# range of pH: 1.10\nby(wine$pH, wine$quality, mean)\n\nwine$quality: 3\n[1] 3.1875\n------------------------------------------------------------------------------------------ \nwine$quality: 4\n[1] 3.182883\n------------------------------------------------------------------------------------------ \nwine$quality: 5\n[1] 3.168833\n------------------------------------------------------------------------------------------ \nwine$quality: 6\n[1] 3.188599\n------------------------------------------------------------------------------------------ \nwine$quality: 7\n[1] 3.213898\n------------------------------------------------------------------------------------------ \nwine$quality: 8\n[1] 3.218686\n------------------------------------------------------------------------------------------ \nwine$quality: 9\n[1] 3.308"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-21",
    "href": "a1/a1-solutions.html#übung-21",
    "title": "Lösungen",
    "section": "Übung 21",
    "text": "Übung 21\n\nset.seed(4)\ndf = data.frame(\n    A=sample(-1000:1000, 500, replace=TRUE),\n    B=runif(500, -1, 10)\n)\n\nlength(df[df[[\"A\"]] &lt; 0, \"B\"])\n\n[1] 252\n\nmean(df[df[[\"A\"]] &lt; 0, \"B\"])\n\n[1] 4.227152\n\nsd(df[df[[\"A\"]] &lt; 0, \"B\"])\n\n[1] 3.24086"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-22",
    "href": "a1/a1-solutions.html#übung-22",
    "title": "Lösungen",
    "section": "Übung 22",
    "text": "Übung 22\n\nset.seed(4)\nx &lt;- sample(-10:10, 500, replace=TRUE)\n\ny1 = x[-c(20, 37)]\ny2 = x[x &gt; 4]\ny3 = x[1:25]\ny4 = x[c(2, 128, 37)]"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-23",
    "href": "a1/a1-solutions.html#übung-23",
    "title": "Lösungen",
    "section": "Übung 23",
    "text": "Übung 23\n\nset.seed(4)\nx = sample(-10:10, 500, replace=TRUE)\n\nx_odd = x[x %% 2 == 1]\nx_even = x[x %% 2 == 0]\n\nlength(x_odd)\n\n[1] 236\n\nlength(x_even)\n\n[1] 264"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-24",
    "href": "a1/a1-solutions.html#übung-24",
    "title": "Lösungen",
    "section": "Übung 24",
    "text": "Übung 24\n\nt.test(sleep[sleep$group == 1, \"extra\"], sleep[sleep$group == 2, \"extra\"], paired=TRUE)\n\n\n    Paired t-test\n\ndata:  sleep[sleep$group == 1, \"extra\"] and sleep[sleep$group == 2, \"extra\"]\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nDie Schlafdauer unterscheidet sich signifikant zwischen den beiden Medikamenten (\\(t(9) = -4.0621\\), \\(p &lt; 0.01\\))."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-25",
    "href": "a1/a1-solutions.html#übung-25",
    "title": "Lösungen",
    "section": "Übung 25",
    "text": "Übung 25\n\nx = 1:100\nsum(log(exp(quantile(x, probs=0.4)), base=10))\n\n[1] 17.63236\n\nx |&gt;\n    quantile(probs=0.4) |&gt;\n    exp() |&gt;\n    log(base=10) |&gt;\n    sum()\n\n[1] 17.63236"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-26",
    "href": "a1/a1-solutions.html#übung-26",
    "title": "Lösungen",
    "section": "Übung 26",
    "text": "Übung 26\n\nlibrary(palmerpenguins)\n\ndf = penguins |&gt;\n    subset(species == \"Gentoo\") |&gt;\n    transform(mass = body_mass_g / 1000) |&gt;\n    subset(select=c(island, sex, mass))"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-27",
    "href": "a1/a1-solutions.html#übung-27",
    "title": "Lösungen",
    "section": "Übung 27",
    "text": "Übung 27\n\nset.seed(1)\ndf = data.frame(\n    A=sample(0:1000, 20, replace=TRUE),\n    B=sample(0:1000, 20, replace=FALSE)\n)\n\ncolMeans(df)\n\n     A      B \n585.30 628.75 \n\ncolMeans(subset(df, B &gt;= 250 & B &lt;= 750))\n\n    A     B \n582.9 539.0 \n\ncolMeans(subset(df, A &gt; B))\n\n    A     B \n762.6 498.9"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-28",
    "href": "a1/a1-solutions.html#übung-28",
    "title": "Lösungen",
    "section": "Übung 28",
    "text": "Übung 28\n\nlibrary(readxl)\n\ndf1 = read_excel(\"luis-daten.xls\", skip=3, sheet=1)\ndf2 = read_excel(\"luis-daten.xls\", skip=3, sheet=2)\n\ndf1$Datum = as.Date(df1$Datum, format=\"%d.%m.%y\")\ndf2$Datum = as.Date(df2$Datum, format=\"%d.%m.%y\")\n\nwith(df1, plot(Datum, Wert, type=\"l\", col=\"blue\", xlab=\"Date\", ylab=\"PM10\"))\nwith(df2, lines(Datum, Wert, col=\"red\"))\nlegend(\"topright\", col=c(\"blue\", \"red\"), lty=1, legend=c(\"Graz (Don Bosco)\", \"Graz (Süd)\"))\n\n\n\n\n\n\n\ncor.test(df1$Wert, df2$Wert)\n\n\n    Pearson's product-moment correlation\n\ndata:  df1$Wert and df2$Wert\nt = 88.167, df = 228, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9814249 0.9889188\nsample estimates:\n      cor \n0.9856496"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-29",
    "href": "a1/a1-solutions.html#übung-29",
    "title": "Lösungen",
    "section": "Übung 29",
    "text": "Übung 29\n\nlibrary(readr)\n\ndf = read_delim(\"temperature-graz.csv\", na=\"999.90\")\n\nplot(\n    df$YEAR,\n    df$metANN,\n    type=\"b\",\n    pch=20,\n    xlab=\"Year\",\n    ylab=\"Mean annual temperature (°C)\",\n    main=\"Graz, Austria\"\n)\n\n\n\n\n\n\n\nmin(df$metANN, na.rm=TRUE)  # 6.82°C\n\n[1] 6.82\n\nmax(df$metANN, na.rm=TRUE)  # 11.02°C\n\n[1] 11.02"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-30",
    "href": "a1/a1-solutions.html#übung-30",
    "title": "Lösungen",
    "section": "Übung 30",
    "text": "Übung 30\n\ndf = read_csv(\"divorce_margarine.csv\")\n\nmodel = lm(divorce_rate_maine ~ margarine_consumption_per_capita, data=df)\nplot(df$margarine_consumption_per_capita, df$divorce_rate_maine)\nabline(model, col=\"blue\")\n\n\n\n\n\n\n\ncor.test(df$divorce_rate_maine, df$margarine_consumption_per_capita)  # r = .932, p &lt; .001 (signifikant)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$divorce_rate_maine and df$margarine_consumption_per_capita\nt = 7.2493, df = 8, p-value = 8.811e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7304040 0.9840332\nsample estimates:\n      cor \n0.9316032"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-31",
    "href": "a1/a1-solutions.html#übung-31",
    "title": "Lösungen",
    "section": "Übung 31",
    "text": "Übung 31\n\nset.seed(1)\ndf = data.frame(\n    A=rep(c(\"A\", \"B\", \"C\"), each=10),\n    B=sample(0:100, 30, replace=TRUE),\n    X=rep(1:10, 3)\n)\n\nlibrary(tidyr)\npivot_wider(df, names_from=A, values_from=B)\n\n# A tibble: 10 × 4\n       X     A     B     C\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1    67    96    88\n 2     2    38    84   100\n 3     3     0    20    36\n 4     4    33    53    33\n 5     5    86    73    88\n 6     6    42     6    43\n 7     7    13    72    78\n 8     8    81    78    32\n 9     9    58    84    83\n10    10    50    36    34"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-32",
    "href": "a1/a1-solutions.html#übung-32",
    "title": "Lösungen",
    "section": "Übung 32",
    "text": "Übung 32\n\nlibrary(dplyr)\n\ndf = subset(starwars, mass &lt; 1000)\nmodel = lm(height ~ mass, df)\n\nplot(df$mass, df$height, main=\"Starwars\", xlab=\"Masse (kg)\", ylab=\"Größe (cm)\")\nabline(model, col=\"blue\", lwd=2)\n\n\n\n\n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = height ~ mass, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.448  -7.234   1.784  13.259  44.028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.8056     8.7549  11.971  &lt; 2e-16 ***\nmass          0.9201     0.1082   8.508 1.14e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.89 on 56 degrees of freedom\nMultiple R-squared:  0.5638,    Adjusted R-squared:  0.556 \nF-statistic: 72.38 on 1 and 56 DF,  p-value: 1.138e-11\n\nlayout(matrix(1:4, nrow=2))\nplot(model)\n\n\n\n\n\n\n\n\nDie Steigung von 0.9201 ist signifikant mit \\(p = 1.14 \\cdot 10^{-11}\\). Das Regressionsmodell ist ebenfalls signifikant (\\(F(1, 56) = 72.38\\), \\(p = 1.14 \\cdot 10^{-11}\\)). Aus der Grafik “Residuals vs Fitted” sieht man, dass die Linearitätsannahme vermutlich verletzt ist (die rote Linie ist keine Gerade um Null). Außerdem sind die Residuen nicht normalverteilt, was aus dem QQ-Plot ersichtlich ist (vor allem die kleineren Werte weichen sehr stark von einer Normalverteilung ab)."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-33",
    "href": "a1/a1-solutions.html#übung-33",
    "title": "Lösungen",
    "section": "Übung 33",
    "text": "Übung 33\n\nset.seed(4)\nx = rnorm(500, mean=-4, sd=5)\ny = rnorm(500, mean=-2, sd=3) + 0.05 * x\n\ncor.test(x, y, method=\"spearman\")  # rho = 0.1049, nicht signifikant da p &gt; 0.01\n\n\n    Spearman's rank correlation rho\n\ndata:  x and y\nS = 18647836, p-value = 0.019\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1049003"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-34",
    "href": "a1/a1-solutions.html#übung-34",
    "title": "Lösungen",
    "section": "Übung 34",
    "text": "Übung 34\n\nresult = (exp(2.74) + sin(0.55 * pi + 1.23)) / sqrt(1 + 8.3^1.4)\nround(result, 3)\n\n[1] 3.473"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-35",
    "href": "a1/a1-solutions.html#übung-35",
    "title": "Lösungen",
    "section": "Übung 35",
    "text": "Übung 35\n\nset.seed(4)\nx = sample(c(-10:10, NA), 500, replace=TRUE)\n\nmean(x, na.rm=TRUE)\n\n[1] 0.3907563\n\nsd(x, na.rm=TRUE)\n\n[1] 6.241343\n\nquantile(x, probs=c(0, 0.25, 0.33, 0.5, 0.66, 0.75, 1), na.rm=TRUE)\n\n  0%  25%  33%  50%  66%  75% 100% \n -10   -5   -3    1    4    6   10"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-36",
    "href": "a1/a1-solutions.html#übung-36",
    "title": "Lösungen",
    "section": "Übung 36",
    "text": "Übung 36\n\nlibrary(readr)\nlibrary(lm.beta)\n\nwine = read_csv(\"wine.csv\")\n\nRows: 27 Columns: 7\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): Year, Price, WinterRain, AGST, HarvestRain, Age, FrancePop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npairs(wine)  # Year und Age sind perfekt korreliert -&gt; entferne Year\n\n\n\n\n\n\n\nmodel = lm(Price ~ WinterRain + AGST + HarvestRain + Age + FrancePop, data=wine[, -1])\nsummary(model)\n\n\nCall:\nlm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + \n    FrancePop, data = wine[, -1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46541 -0.24133  0.00413  0.18974  0.52495 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.343e+00  7.697e+00  -0.304  0.76384    \nWinterRain   1.153e-03  4.991e-04   2.311  0.03109 *  \nAGST         6.144e-01  9.799e-02   6.270 3.22e-06 ***\nHarvestRain -3.837e-03  8.366e-04  -4.587  0.00016 ***\nAge          1.377e-02  5.821e-02   0.237  0.81531    \nFrancePop   -2.213e-05  1.268e-04  -0.175  0.86313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.293 on 21 degrees of freedom\nMultiple R-squared:  0.8278,    Adjusted R-squared:  0.7868 \nF-statistic: 20.19 on 5 and 21 DF,  p-value: 2.232e-07\n\n# Signifikante Prädiktoren: WinterRain, AGST, HarvestRain\n# R² = 0.8278\n# Das Modell ist insgesamt statistisch signifikant (p = 2.232e-07)\nlm.beta(model)\n\n\nCall:\nlm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + \n    FrancePop, data = wine[, -1])\n\nStandardized Coefficients::\n(Intercept)  WinterRain        AGST HarvestRain         Age   FrancePop \n         NA   0.2344909   0.6382419  -0.4418232   0.1789302  -0.1322963 \n\n# Den größten Einfluss hat AGST, gefolgt von HarvestRain, gefolgt von WinterRain."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-37",
    "href": "a1/a1-solutions.html#übung-37",
    "title": "Lösungen",
    "section": "Übung 37",
    "text": "Übung 37\n\nx = seq(-76, 120, by=2)\nlength(x)\n\n[1] 99\n\nmean(x)\n\n[1] 22\n\nsd(x)\n\n[1] 57.44563\n\nmedian(x)\n\n[1] 22\n\nquantile(x, c(0.25, 0.75))\n\n25% 75% \n-27  71 \n\nlength(x[x &gt; mean(x)])\n\n[1] 49"
  },
  {
    "objectID": "a1/a1-solutions.html#übung-38",
    "href": "a1/a1-solutions.html#übung-38",
    "title": "Lösungen",
    "section": "Übung 38",
    "text": "Übung 38\n\ndf = read_tsv(\"imdb-ratings.tsv.zip\")\n\nm_g = mean(df$averageRating)\nm = quantile(df$numVotes, 0.9)\ndf$weightedRating = (df$averageRating * df$numVotes + m_g * m) / (df$numVotes + m)\n\ndf[order(-df$weightedRating), ]\n\n# A tibble: 1,628,304 × 4\n   tconst     averageRating numVotes weightedRating\n   &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 tt2301451           10     282095           9.99\n 2 tt4283088            9.9   251299           9.89\n 3 tt4283094            9.9   181028           9.89\n 4 tt2301455            9.9   179032           9.89\n 5 tt2178784            9.9   139417           9.89\n 6 tt1683088            9.9    97718           9.89\n 7 tt12187040           9.9    73261           9.88\n 8 tt38504411           9.9    63904           9.88\n 9 tt8084176            9.9    42499           9.87\n10 tt38504412           9.9    39322           9.86\n# ℹ 1,628,294 more rows\n\n\nDer Film mit der höchsten gewichteten Bewertung von 9.99 ist “tt2301451”. Dabei handelt es sich um die Folge “Ozymandias” aus Breaking Bad (S5E14).\n\n\n\n\n\n\nHinweis\n\n\n\nDer in dieser Aufgabe verwendete Datensatz kann, neben anderen, auf IMDb heruntergeladen werden."
  },
  {
    "objectID": "a1/a1-solutions.html#übung-39",
    "href": "a1/a1-solutions.html#übung-39",
    "title": "Lösungen",
    "section": "Übung 39",
    "text": "Übung 39\n\nset.seed(123)\nx = sample(-100:120, size=100, replace=TRUE)\nx |&gt; unique() |&gt; mean(trim=0.25) |&gt; round(2)\n\n[1] 2.31\n\nround(mean(unique(x), trim=0.25), 2)\n\n[1] 2.31"
  },
  {
    "objectID": "07/07.html",
    "href": "07/07.html",
    "title": "7 – Datenaufbereitung",
    "section": "",
    "text": "Es gibt in der Praxis (mindestens) zwei verschiedene Möglichkeiten, ein und dieselben Daten in einer Tabelle darzustellen, und zwar entweder im Wide-Format oder im Long-Format. Zur Berechnung deskriptiver Statistiken eignet sich das Wide-Format besser, das Long-Format ist aber bei anderen Aufgabenstellungen notwendig. Deswegen ist es sinnvoll, wenn man weiß, wie man die eine Darstellung in die andere umwandeln kann.\nBei Daten im Wide-Format gibt es für jede Variable eine eigene Spalte. Daten im Long-Format haben nur eine Spalte, die alle Werte beinhaltet, und eine oder mehrere Spalte(n) mit Indikator-Variablen, welche den Kontext der Werte definieren. Die folgende Tabelle zeigt Beispieldaten im Wide-Format:\n\n\n\nPerson\nAge\nWeight\nHeight\n\n\n\n\nBob\n32\n98\n188\n\n\nAl\n24\n61\n176\n\n\nSue\n64\n87\n174\n\n\n\nMan sieht, dass es drei Wertespalten gibt (Age, Weight und Height), sowie eine Spalte, welche die Person identifiziert (Person). Dieselben Daten sehen im Long-Format so aus (man beachte, dass es nur eine einzige Werte-Spalte namens Value gibt):\n\n\n\nPerson\nVariable\nValue\n\n\n\n\nBob\nAge\n32\n\n\nBob\nWeight\n98\n\n\nBob\nHeight\n188\n\n\nAl\nAge\n24\n\n\nAl\nWeight\n61\n\n\nAl\nHeight\n176\n\n\nSue\nAge\n64\n\n\nSue\nWeight\n87\n\n\nSue\nHeight\n174\n\n\n\nIn R kann man mit dem Paket tidyr zwischen den beiden Formaten hin- und herwechseln, d.h. wenn die Daten in einem Format vorliegen, kann man relativ einfach das andere Format produzieren. Die Daten aus dem Beispiel oben können wir zunächst einmal im Wide-Format erzeugen:\n\nlibrary(tibble)\n\n(df = tibble(\n    Person=c(\"Bob\", \"Al\", \"Sue\"),\n    Age=c(32, 24, 64),\n    Weight=c(98, 61, 87),\n    Height=c(188, 176, 174)\n))\n\n# A tibble: 3 × 4\n  Person   Age Weight Height\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bob       32     98    188\n2 Al        24     61    176\n3 Sue       64     87    174\n\n\nDas Paket tidyr aus dem Tidyverse beinhaltet die Funktionen pivot_longer() und pivot_wider(), welche Data Frames von wide nach long bzw. von long nach wide umwandeln können. Das Data Frame df (welches im Wide-Format vorliegt) können wir also wie folgt ins Long-Format konvertieren:\n\nlibrary(tidyr)\n\n(df_long = pivot_longer(\n    df,\n    Age:Height,\n    names_to=\"Variable\",\n    values_to=\"Value\"\n))\n\n# A tibble: 9 × 3\n  Person Variable Value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Bob    Age         32\n2 Bob    Weight      98\n3 Bob    Height     188\n4 Al     Age         24\n5 Al     Weight      61\n6 Al     Height     176\n7 Sue    Age         64\n8 Sue    Weight      87\n9 Sue    Height     174\n\n\nHier übergibt man zuerst die Daten im Wide-Format (df), gefolgt von einer Auswahl der (Werte-)Spalten, die man ins Long-Format bringen möchte – in unserem Beispiel sind das die Spalten von Age bis Height. Für diese Spaltenauswahl kann man die Spaltennamen ohne Anführungszeichen schreiben, und auch der Doppelpunkt funktioniert wie bei Zahlensequenzen. Man könnte hier auch die Indizes der Spalten verwenden, also 2:4 statt Age:Height. Schließlich gibt man mit names_to den gewünschten Namen der Indikatorspalte an (im Beispiel soll diese also Variable heißen) und mit values_to den Namen der neuen Werte-Spalte (Value).\nDer umgekehrte Weg wird mit pivot_wider() beschritten; diese Funktion kann eine Variable auf mehrere Spalten aufteilen, d.h. vom Long-Format ins Wide-Format konvertieren:\n\n(df_wide = pivot_wider(\n    df_long,\n    id_cols=Person,\n    names_from=Variable,\n    values_from=Value\n))\n\n# A tibble: 3 × 4\n  Person   Age Weight Height\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bob       32     98    188\n2 Al        24     61    176\n3 Sue       64     87    174\n\n\nHier gibt man zunächst die Daten im Long-Format an (df_long). Danach folgen mit id_cols die Spaltennamen, welche die einzelnen Fälle identifizieren (in unserem Beispiel ist das nur eine einzige Spalte namens Person). Danach gibt man mit names_from die Spalte an, welche die Variablennamen beinhaltet (also die Indikatorspalte Variable). Schließlich definiert man mit values_from die Spalte, welche die Werte beinhaltet (Value).\nDiese beiden Funktionen können sehr viel komplexere Strukturen in die jeweiligen Formate konvertieren – dies ist alles in der Hilfe nachzulesen, inklusive vieler Beispiele, die die Verwendung demonstrieren.\n\n\n\n\n\n\nHinweis\n\n\n\nDas Tidyverse basiert auf einer Darstellung von Daten, die man als “tidy” bezeichnet. Im Wesentlichen entspricht dies dem Wide-Format, bei dem jede Variable einer eigenen Spalte sowie jeder Datenpunkt einer eigenen Zeile entspricht (mehr Details dazu sind in diesem Artikel zu finden)."
  },
  {
    "objectID": "07/07.html#daten-umformen",
    "href": "07/07.html#daten-umformen",
    "title": "7 – Datenaufbereitung",
    "section": "",
    "text": "Es gibt in der Praxis (mindestens) zwei verschiedene Möglichkeiten, ein und dieselben Daten in einer Tabelle darzustellen, und zwar entweder im Wide-Format oder im Long-Format. Zur Berechnung deskriptiver Statistiken eignet sich das Wide-Format besser, das Long-Format ist aber bei anderen Aufgabenstellungen notwendig. Deswegen ist es sinnvoll, wenn man weiß, wie man die eine Darstellung in die andere umwandeln kann.\nBei Daten im Wide-Format gibt es für jede Variable eine eigene Spalte. Daten im Long-Format haben nur eine Spalte, die alle Werte beinhaltet, und eine oder mehrere Spalte(n) mit Indikator-Variablen, welche den Kontext der Werte definieren. Die folgende Tabelle zeigt Beispieldaten im Wide-Format:\n\n\n\nPerson\nAge\nWeight\nHeight\n\n\n\n\nBob\n32\n98\n188\n\n\nAl\n24\n61\n176\n\n\nSue\n64\n87\n174\n\n\n\nMan sieht, dass es drei Wertespalten gibt (Age, Weight und Height), sowie eine Spalte, welche die Person identifiziert (Person). Dieselben Daten sehen im Long-Format so aus (man beachte, dass es nur eine einzige Werte-Spalte namens Value gibt):\n\n\n\nPerson\nVariable\nValue\n\n\n\n\nBob\nAge\n32\n\n\nBob\nWeight\n98\n\n\nBob\nHeight\n188\n\n\nAl\nAge\n24\n\n\nAl\nWeight\n61\n\n\nAl\nHeight\n176\n\n\nSue\nAge\n64\n\n\nSue\nWeight\n87\n\n\nSue\nHeight\n174\n\n\n\nIn R kann man mit dem Paket tidyr zwischen den beiden Formaten hin- und herwechseln, d.h. wenn die Daten in einem Format vorliegen, kann man relativ einfach das andere Format produzieren. Die Daten aus dem Beispiel oben können wir zunächst einmal im Wide-Format erzeugen:\n\nlibrary(tibble)\n\n(df = tibble(\n    Person=c(\"Bob\", \"Al\", \"Sue\"),\n    Age=c(32, 24, 64),\n    Weight=c(98, 61, 87),\n    Height=c(188, 176, 174)\n))\n\n# A tibble: 3 × 4\n  Person   Age Weight Height\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bob       32     98    188\n2 Al        24     61    176\n3 Sue       64     87    174\n\n\nDas Paket tidyr aus dem Tidyverse beinhaltet die Funktionen pivot_longer() und pivot_wider(), welche Data Frames von wide nach long bzw. von long nach wide umwandeln können. Das Data Frame df (welches im Wide-Format vorliegt) können wir also wie folgt ins Long-Format konvertieren:\n\nlibrary(tidyr)\n\n(df_long = pivot_longer(\n    df,\n    Age:Height,\n    names_to=\"Variable\",\n    values_to=\"Value\"\n))\n\n# A tibble: 9 × 3\n  Person Variable Value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Bob    Age         32\n2 Bob    Weight      98\n3 Bob    Height     188\n4 Al     Age         24\n5 Al     Weight      61\n6 Al     Height     176\n7 Sue    Age         64\n8 Sue    Weight      87\n9 Sue    Height     174\n\n\nHier übergibt man zuerst die Daten im Wide-Format (df), gefolgt von einer Auswahl der (Werte-)Spalten, die man ins Long-Format bringen möchte – in unserem Beispiel sind das die Spalten von Age bis Height. Für diese Spaltenauswahl kann man die Spaltennamen ohne Anführungszeichen schreiben, und auch der Doppelpunkt funktioniert wie bei Zahlensequenzen. Man könnte hier auch die Indizes der Spalten verwenden, also 2:4 statt Age:Height. Schließlich gibt man mit names_to den gewünschten Namen der Indikatorspalte an (im Beispiel soll diese also Variable heißen) und mit values_to den Namen der neuen Werte-Spalte (Value).\nDer umgekehrte Weg wird mit pivot_wider() beschritten; diese Funktion kann eine Variable auf mehrere Spalten aufteilen, d.h. vom Long-Format ins Wide-Format konvertieren:\n\n(df_wide = pivot_wider(\n    df_long,\n    id_cols=Person,\n    names_from=Variable,\n    values_from=Value\n))\n\n# A tibble: 3 × 4\n  Person   Age Weight Height\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bob       32     98    188\n2 Al        24     61    176\n3 Sue       64     87    174\n\n\nHier gibt man zunächst die Daten im Long-Format an (df_long). Danach folgen mit id_cols die Spaltennamen, welche die einzelnen Fälle identifizieren (in unserem Beispiel ist das nur eine einzige Spalte namens Person). Danach gibt man mit names_from die Spalte an, welche die Variablennamen beinhaltet (also die Indikatorspalte Variable). Schließlich definiert man mit values_from die Spalte, welche die Werte beinhaltet (Value).\nDiese beiden Funktionen können sehr viel komplexere Strukturen in die jeweiligen Formate konvertieren – dies ist alles in der Hilfe nachzulesen, inklusive vieler Beispiele, die die Verwendung demonstrieren.\n\n\n\n\n\n\nHinweis\n\n\n\nDas Tidyverse basiert auf einer Darstellung von Daten, die man als “tidy” bezeichnet. Im Wesentlichen entspricht dies dem Wide-Format, bei dem jede Variable einer eigenen Spalte sowie jeder Datenpunkt einer eigenen Zeile entspricht (mehr Details dazu sind in diesem Artikel zu finden)."
  },
  {
    "objectID": "07/07.html#zeilen-filtern-mit-subset",
    "href": "07/07.html#zeilen-filtern-mit-subset",
    "title": "7 – Datenaufbereitung",
    "section": "Zeilen filtern mit subset()",
    "text": "Zeilen filtern mit subset()\nOft ist es wünschenswert, nur gewisse Zeilen aus einem vorhandenen Data Frame zu verwenden. Beispielsweise könnte es für eine Datenanalyse notwendig sein, männliche und weibliche Versuchspersonen getrennt auszuwerten. Dieses Merkmal ist im folgenden Beispiel in einer Spalte sex mit den Ausprägungen m und f vorhanden:\n\n(df = tibble(\n    name=c(\"Bob\", \"Al\", \"Sue\", \"John\", \"Mary\", \"Ann\"),\n    age=c(32, 24, 64, 44, 21, 75),\n    weight=c(98, 61, 87, 82, 73, 66),\n    height=c(188, 176, 174, 182, 181, 159),\n    sex=factor(c(\"m\", \"m\", \"f\", \"m\", \"f\", \"f\"))\n))\n\n# A tibble: 6 × 5\n  name    age weight height sex  \n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1 Bob      32     98    188 m    \n2 Al       24     61    176 m    \n3 Sue      64     87    174 f    \n4 John     44     82    182 m    \n5 Mary     21     73    181 f    \n6 Ann      75     66    159 f    \n\n\nNun haben wir bereits gelernt, dass wir spezifische Zeilen mittels Indizieren herausfiltern können. Mit folgendem Befehl erhalten wir ein neues Data Frame mit allen weiblichen Versuchspersonen:\n\ndf[df$sex == \"f\",]\n\n# A tibble: 3 × 5\n  name    age weight height sex  \n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1 Sue      64     87    174 f    \n2 Mary     21     73    181 f    \n3 Ann      75     66    159 f    \n\n\nDiese Schreibweise ist für Data Frames aber relativ unübersichtlich, vor allem weil man das zugrundeliegende Data Frame df zwei Mal schreiben muss. Glücklicherweise gibt es aber eine intuitivere Alternative durch die Funktion subset(). Diese Funktion kann, wie der Name bereits andeutet, Untermengen (Subsets) von bestehenden Vektoren bzw. Data Frames erzeugen. Im Falle eines Data Frames bedeutet das, dass man eine Untermenge der Zeilen und/oder Spalten auswählen kann.\nBeginnen wir mit der Auswahl von Zeilen (man bezeichnet diese Operation auch als “filtern”). Hier übergibt man der Funktion subset() als erstes Argument das ursprüngliche Data Frame. Das zweite Argument (welches subset heißt, nicht zu verwechseln mit dem Funktionsnamen) bestimmt dann, welche Zeilen ausgewählt (gefiltert) werden sollen. Das obige Beispiel kann damit wie folgt angeschrieben werden:\n\nsubset(df, sex == \"f\")\n\n# A tibble: 3 × 5\n  name    age weight height sex  \n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1 Sue      64     87    174 f    \n2 Mary     21     73    181 f    \n3 Ann      75     66    159 f    \n\n\nBeachten Sie, dass man den Spaltennamen direkt anschreiben kann ohne df$ voranstellen zu müssen.\nDamit kann man auch komplexere Filter-Operationen durchführen, indem man einen entsprechend komplexen Vergleich für das subset-Argument übergibt, z.B. durch Verknüpfen mehrerer Vergleiche:\n\nsubset(df, age &gt; 40 & weight &lt;= 73)\n\n# A tibble: 1 × 5\n  name    age weight height sex  \n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1 Ann      75     66    159 f    \n\n\nHier kann man also alle möglichen Vergleiche bzw. Verknüpfungsoperatoren (& und, | oder, xor() exklusives oder) verwenden."
  },
  {
    "objectID": "07/07.html#spalten-selektieren-mit-subset",
    "href": "07/07.html#spalten-selektieren-mit-subset",
    "title": "7 – Datenaufbereitung",
    "section": "Spalten selektieren mit subset()",
    "text": "Spalten selektieren mit subset()\nManchmal ist es auch gewünscht, nur spezifische Spalten aus einem Data Frame weiterzuverwenden. Klassisch kann man dies wieder über Indizieren lösen, z.B. wenn man im Beispiel nur die Spalten name, age und sex benötigt:\n\ndf[, c(\"name\", \"age\", \"sex\")]\n\n# A tibble: 6 × 3\n  name    age sex  \n  &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;\n1 Bob      32 m    \n2 Al       24 m    \n3 Sue      64 f    \n4 John     44 m    \n5 Mary     21 f    \n6 Ann      75 f    \n\n\nAuch hier kann man alternativ die subset()-Funktion verwenden, und zwar unter Verwendung des dritten Arguments (namens select):\n\nsubset(df, select=c(name, age, sex))\n\n# A tibble: 6 × 3\n  name    age sex  \n  &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;\n1 Bob      32 m    \n2 Al       24 m    \n3 Sue      64 f    \n4 John     44 m    \n5 Mary     21 f    \n6 Ann      75 f    \n\n\nBeachten Sie, dass auch hier die Spaltennamen ohne Anführungszeichen angegeben werden können. Es ist sogar möglich, einen Bereich mit einem : wie folgt anzuschreiben (so wie wir es bereits bei pivot_longer() bzw. pivot_wider() gesehen haben):\n\nsubset(df, select=name:weight)\n\n# A tibble: 6 × 3\n  name    age weight\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Bob      32     98\n2 Al       24     61\n3 Sue      64     87\n4 John     44     82\n5 Mary     21     73\n6 Ann      75     66\n\n\nHier werden also alle Spalten von name bis weight selektiert.\nSelbstverständlich kann man beide Operationen (also Zeilenauswahl und Spaltenauswahl) auch mit einem Funktionsaufruf durchführen:\n\nsubset(df, subset=age &gt; 30, select=c(name, age, sex))\n\n# A tibble: 4 × 3\n  name    age sex  \n  &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;\n1 Bob      32 m    \n2 Sue      64 f    \n3 John     44 m    \n4 Ann      75 f"
  },
  {
    "objectID": "07/07.html#spalten-transformieren-mit-transform",
    "href": "07/07.html#spalten-transformieren-mit-transform",
    "title": "7 – Datenaufbereitung",
    "section": "Spalten transformieren mit transform()",
    "text": "Spalten transformieren mit transform()\nEine weitere wichtige Aufgabe in der Datenanalyse ist es, neue Spalten zu einem bestehenden Data Frame hinzuzufügen. Wenn die Werte dieser neuen Spalten dabei auf vorhandenen Spalten basieren, spricht man von einer Transformation. Betrachten wir dazu das in R vorhandene Data Frame airquality:\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nDie Temperaturen in der Temp-Spalte sind in Fahrenheit angegeben. Wir können nun eine neue Spalte namens celsius hinzufügen, indem wir die vorhandene Temp-Spalte wie folgt transformieren:\n\naq = transform(airquality, celsius=(Temp - 32) * (5/9))\nhead(aq)\n\n  Ozone Solar.R Wind Temp Month Day  celsius\n1    41     190  7.4   67     5   1 19.44444\n2    36     118  8.0   72     5   2 22.22222\n3    12     149 12.6   74     5   3 23.33333\n4    18     313 11.5   62     5   4 16.66667\n5    NA      NA 14.3   56     5   5 13.33333\n6    28      NA 14.9   66     5   6 18.88889\n\n\nBeachten Sie, dass wir in der Transformation vorhandene Spalten wieder direkt verwenden können (im Beispiel Temp), ohne Anführungszeichen oder airquality$ verwenden zu müssen. Außerdem wird immer ein neues Data Frame erzeugt; Sie könnten dieses aber dem ursprünglichen Namen zuweisen, wenn Sie möchten (also airquality in obigem Beispiel statt aq).\n\n\n\n\n\n\nWichtig\n\n\n\nDas Ergebnis der Funktion transform() ist ein Data Frame, auch wenn man ein Tibble als Ausgangsdatensatz verwendet. Falls das Ergebnis ein Tibble sein soll, muss man den Rückgabewert von transform() mit tibble::as_tibble() explizit in ein Tibble konvertieren."
  },
  {
    "objectID": "07/07.html#der-pipe-operator",
    "href": "07/07.html#der-pipe-operator",
    "title": "7 – Datenaufbereitung",
    "section": "Der Pipe-Operator |>",
    "text": "Der Pipe-Operator |&gt;\nSeit R 4.1 gibt es einen sogenannten Pipe-Operator. Dieser wird als |&gt; angeschrieben. Das Prinzip dahinter ist so einfach wie genial. Obwohl damit keinerlei neue Funktionalität hinzugefügt wird (d.h. man kann alle Operationen auch ohne den Pipe-Operator umsetzen), werden viele Operationen dadurch einfacher bzw. intuitiver.\nGrundsätzlich kann man mit dem Pipe-Operator einen Funktionsaufruf f(x) als x |&gt; f() anschreiben. Das ist eine reine syntaktische Alternative, d.h. beide Varianten tun genau das gleiche. Beispiel:\n\nx = 1:10\nmean(x)  # klassisch\n\n[1] 5.5\n\nx |&gt; mean()  # Pipe\n\n[1] 5.5\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDer Pipe-Operator zieht also das erste Argument vor den Funktionsaufruf hinaus.\n\n\nDas macht in diesem Beispiel natürlich wenig Sinn, denn hier ist mean(x) wesentlich kürzer und einfacher lesbar. Interessant wird es aber dann, wenn man das Ergebnis eines Funktionsaufrufes direkt als Argument für einen weiteren Funktionsaufruf verwenden will. Klassisch würde man dies so anschreiben:\ng(f(x))\nHier ist es schon schwieriger zu sehen, in welcher Reihenfolge die Berechnung eigentlich durchgeführt wird: zuerst wird f(x) berechnet, und dessen Ergebnis wird als Argument für die Funktion g() übergeben. R führt die Berechnungen also von innen nach außen durch. Mit dem Pipe-Operator würde dieselbe Operation wie folgt aussehen:\nx |&gt; f() |&gt; g()\nHier ist sofort klar, in welcher Reihenfolge die Berechnung durchgeführt wird: x wird zuerst der Funktion f() übergeben, und das Ergebnis davon wird der Funktion g() übergeben.\nDas folgende Beispiel berechnet zuerst den Mittelwert von einem Vektor x und gleich danach den Logarithmus dieses Mittelwerts:\n\nlog(mean(x))\n\n[1] 1.704748\n\nx |&gt; mean() |&gt; log()\n\n[1] 1.704748\n\n\nDie Variante mit dem Pipe-Operator ist meistens intuitiver. Noch übersichtlicher wird es, wenn jeder Schritt in der Pipeline in eine eigene Zeile geschrieben wird (das ist in R ja bei allen Befehlen prinzipiell möglich):\n\nx |&gt;\n    mean() |&gt;\n    log()\n\n[1] 1.704748"
  },
  {
    "objectID": "07/07.html#data-wrangling",
    "href": "07/07.html#data-wrangling",
    "title": "7 – Datenaufbereitung",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nKombinieren wir nun alles, was wir in diesem Kapitel gelernt haben (nämlich subset(), transform() und |&gt;), dann erhalten wir ein Werkzeug, welches das Umformen eines Data Frames (das sogenannte Data Wrangling) sehr intuitiv ermöglicht. Sehen wir uns das anhand des airquality-Datensatzes an.\nNehmen wir an, wir möchten die Temperaturen (in °C) im Monat Juli untersuchen. Folgende Pipeline könnten wir dazu verwenden:\n\nlibrary(tibble)\n\nairquality |&gt;\n    transform(celsius=(Temp - 32) * (5/9)) |&gt;\n    subset(Month == 7) |&gt;\n    subset(select=-c(Month, Day)) |&gt;\n    as_tibble()\n\n# A tibble: 31 × 5\n   Ozone Solar.R  Wind  Temp celsius\n   &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1   135     269   4.1    84    28.9\n 2    49     248   9.2    85    29.4\n 3    32     236   9.2    81    27.2\n 4    NA     101  10.9    84    28.9\n 5    64     175   4.6    83    28.3\n 6    40     314  10.9    83    28.3\n 7    77     276   5.1    88    31.1\n 8    97     267   6.3    92    33.3\n 9    97     272   5.7    92    33.3\n10    85     175   7.4    89    31.7\n# ℹ 21 more rows\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nIm vorigen Beispiel wird mit as_tibble() am Ende alles in ein Tibble konvertiert. Ansonsten hätte man ein Data Frame, weil wir die Funktion transform() verwendet haben.\n\n\nUm mit diesem Data Frame weiterzuarbeiten, sollte man das Ergebnis einer (neuen) Variablen zuweisen, z.B.:\n\naq = airquality |&gt;\n    transform(celsius=(Temp - 32) * (5/9)) |&gt;\n    subset(Month == 7) |&gt;\n    subset(select=-c(Month, Day))\n\nmean(aq$celsius)  # mittlere Temperatur (in °C) im Juli\n\n[1] 28.83513\n\n\nAbschließend ist noch anzumerken, dass das Tidyverse wesentlich mehr Funktionen bietet, die mit dem Pipe-Operator elegant miteinander verbunden werden können. Insbesonders gruppierte zusammenfassende Statistiken (z.B. die mittleren Temperaturen für alle Monate) werden dadurch ähnlich intuitiv möglich wie in den hier gezeigten Beispielen. Mit Base-R ist dies zwar auch möglich, aber nicht in dieser einheitlichen Pipeline-Form."
  },
  {
    "objectID": "07/07.html#übungen",
    "href": "07/07.html#übungen",
    "title": "7 – Datenaufbereitung",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nInstallieren und aktivieren Sie das Paket tidyr. Darin enthalten ist der Datensatz table2. Erzeugen Sie daraus ein neues Data Frame, welches aus den Spalten type und count die Werte in zwei Spalten cases und population enthält.\n\n\nÜbung 2\nIm Paket tidyr ist auch ein Datensatz table4a enthalten. Fassen Sie die beiden Spalten 1999 und 2000 zu einer Wertespalte namens count und einer Indikatorspalte namens year zusammen.\n\n\n\n\n\n\nHinweis\n\n\n\nDie beiden Spaltennamen muss man mit Backticks (`) umschließen, da Namen die mit Ziffern starten und R diese sonst als Zahlen interpretieren würde, d.h. `1999` bzw. `2000`.\n\n\n\n\nÜbung 3\nErstellen Sie aus dem in R vorhandenen Data Frame mtcars ein neues Data Frame namens mtcars1, welches nur aus jenen Zeilen besteht in denen die Spalte mpg Werte größer als 25 aufweist. Aus wie vielen Zeilen bzw. Spalten bestehen mtcars bzw. mtcars1? Verwenden Sie dafür die Funktion subset()!\n\n\nÜbung 4\nInstallieren und aktivieren Sie das Paket nycflights13. Wir verwenden den Datensatz flights aus diesem Paket. Lesen Sie sich zuerst den Hilfetext zu flights durch. Führen Sie danach folgende Aufgaben durch (unter Verwendung des Pipe-Operators und den Funktionen subset() bzw. transform()):\n\nErstellen Sie einen neuen Datensatz, welcher alle Flüge am 1.1.2013 enthält. Außerdem sollen nur die Spalten year, month, day, dep_time, arr_time und tailnum vorhanden sein. Wie viele Flüge sind das?\nErzeugen Sie zwei neue Spalten hours (die Flugzeit in Stunden; die Spalte air_time beinhaltet die Flugzeit in Minuten) und speed (die Fluggeschwindigkeit in km/h; die Spalte distance beinhaltet den zurückgelegten Weg in Meilen, hier wäre also eine zusätzliche Spalte km hilfreich). Geben Sie das neue Data Frame nur mit den Spalten month, day, carrier, tailnum und speed aus!\nErstellen Sie einen neuen Datensatz, welcher alle Frühflüge beinhaltet (Flüge, die vor 6:00 gestartet sind).\nErstellen Sie einen neuen Datensatz, welcher nur jene Flüge beinhaltet, die schneller als geplant unterwegs waren (wo also die Verspätung bei der Ankunft kleiner war als beim Abflug)."
  },
  {
    "objectID": "05/05-solutions.html",
    "href": "05/05-solutions.html",
    "title": "5 – Lösungen",
    "section": "",
    "text": "library(readr)\n\n?read_delim\nFolgende Argumente werden verwendet:\n\nSpaltentrennzeichen: delim, dieses Argument folgt nach dem Dateinamen; wird es nicht angegeben, versucht die Funktion das Spaltentrennzeichen automatisch zu erkennen\nDezimaltrennzeichen: locale (z.B. locale=locale(decimal_mark=\",\") für deutsche Zahlenschreibweise)\nFehlende Werte: na"
  },
  {
    "objectID": "05/05-solutions.html#übung-1",
    "href": "05/05-solutions.html#übung-1",
    "title": "5 – Lösungen",
    "section": "",
    "text": "library(readr)\n\n?read_delim\nFolgende Argumente werden verwendet:\n\nSpaltentrennzeichen: delim, dieses Argument folgt nach dem Dateinamen; wird es nicht angegeben, versucht die Funktion das Spaltentrennzeichen automatisch zu erkennen\nDezimaltrennzeichen: locale (z.B. locale=locale(decimal_mark=\",\") für deutsche Zahlenschreibweise)\nFehlende Werte: na"
  },
  {
    "objectID": "05/05-solutions.html#übung-2",
    "href": "05/05-solutions.html#übung-2",
    "title": "5 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\n(df = read_csv2(\"homework.csv\"))\n\n# A tibble: 4 × 4\n  category temperature   age height\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 A               23.5    33   76.1\n2 A               20.1    17   65.2\n3 X               19.6    65   99.0\n4 B               13.3    54   87.2\n\n\nDie Datentypen der vier Spalten sind &lt;chr&gt;, &lt;dbl&gt;, &lt;dbl&gt;, &lt;dbl&gt; – d.h. das deutsche Zahlenformat wurde korrekt importiert."
  },
  {
    "objectID": "05/05-solutions.html#übung-3",
    "href": "05/05-solutions.html#übung-3",
    "title": "5 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nwahl16 = read_csv(\"wahl16.csv\")\nround(colSums(wahl16[, -1]) / sum(wahl16[, -1]) * 100, 1)\n\n      griss       hofer hundstorfer        khol      lugner         vdb \n       18.9        35.1        11.3        11.1         2.3        21.3"
  },
  {
    "objectID": "05/05-solutions.html#übung-4",
    "href": "05/05-solutions.html#übung-4",
    "title": "5 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\ncovid = read_csv2(\"covid19.csv\")\ncovid[[\"Time\"]] = as.Date(covid[[\"Time\"]], format=\"%d.%m.%Y\")\ncovid\n\n# A tibble: 12,170 × 12\n   Time       Bundesland BundeslandID AnzEinwohner AnzahlFaelle AnzahlFaelleSum AnzahlFaelle7Tage SiebenTageInzidenzFa…¹\n   &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1 2020-02-26 Burgenland            1       299235            0               0                 0                 0     \n 2 2020-02-26 Kärnten               2       566166            0               0                 0                 0     \n 3 2020-02-26 Niederöst…            3      1708140            0               0                 0                 0     \n 4 2020-02-26 Oberöster…            4      1512226            0               0                 0                 0     \n 5 2020-02-26 Salzburg              5       564293            0               0                 0                 0     \n 6 2020-02-26 Steiermark            6      1256890            0               0                 0                 0     \n 7 2020-02-26 Tirol                 7       766492            0               0                 0                 0     \n 8 2020-02-26 Vorarlberg            8       403203            0               0                 0                 0     \n 9 2020-02-26 Wien                  9      1951354            1               1                 1                 0.0512\n10 2020-02-26 Österreich           10      9027999            1               1                 1                 0.0111\n# ℹ 12,160 more rows\n# ℹ abbreviated name: ¹​SiebenTageInzidenzFaelle\n# ℹ 4 more variables: AnzahlTotTaeglich &lt;dbl&gt;, AnzahlTotSum &lt;dbl&gt;, AnzahlGeheiltTaeglich &lt;dbl&gt;, AnzahlGeheiltSum &lt;dbl&gt;\n\ndim(covid)\n\n[1] 12170    12\n\n\nMan könnte die gewünschten Spaltentypen auch direkt beim Importieren mittels col_types angeben:\ncovid = read_csv2(\"covid19.csv\", col_types=cols(Time=col_date(\"%d.%m.%Y %H:%M:%S\")))"
  },
  {
    "objectID": "05/05-solutions.html#übung-5",
    "href": "05/05-solutions.html#übung-5",
    "title": "5 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\n(df = read_delim(\"household_power_consumption.zip\", delim=\";\", na=c(\"?\", \"\")))\n\n# A tibble: 2,075,259 × 9\n   Date       Time   Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2\n   &lt;chr&gt;      &lt;time&gt;               &lt;dbl&gt;                 &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 16/12/2006 17:24                 4.22                 0.418    235.             18.4              0              1\n 2 16/12/2006 17:25                 5.36                 0.436    234.             23                0              1\n 3 16/12/2006 17:26                 5.37                 0.498    233.             23                0              2\n 4 16/12/2006 17:27                 5.39                 0.502    234.             23                0              1\n 5 16/12/2006 17:28                 3.67                 0.528    236.             15.8              0              1\n 6 16/12/2006 17:29                 3.52                 0.522    235.             15                0              2\n 7 16/12/2006 17:30                 3.70                 0.52     235.             15.8              0              1\n 8 16/12/2006 17:31                 3.7                  0.52     235.             15.8              0              1\n 9 16/12/2006 17:32                 3.67                 0.51     234.             15.8              0              1\n10 16/12/2006 17:33                 3.66                 0.51     234.             15.8              0              2\n# ℹ 2,075,249 more rows\n# ℹ 1 more variable: Sub_metering_3 &lt;dbl&gt;\n\ndim(df)  # 2.075.259 Zeilen und 9 Spalten\n\n[1] 2075259       9\n\n\nDie Spalte Date könnte man noch in einen passenderen Typ konvertieren (ist jetzt character)."
  },
  {
    "objectID": "05/05-solutions.html#übung-6",
    "href": "05/05-solutions.html#übung-6",
    "title": "5 – Lösungen",
    "section": "Übung 6",
    "text": "Übung 6\n\nx = c(1, 2, 3)\nfactor(x, levels=c(1, 2), labels=c(\"one\", \"two\"))\n\n[1] one  two  &lt;NA&gt;\nLevels: one two\n\n\nMan erkennt, dass alle Stufen, die nicht in levels enthalten sind, auch nicht im resultierenden Faktor vorkommen. Daher werden solche Werte auf NA gesetzt."
  },
  {
    "objectID": "06/06-solutions.html",
    "href": "06/06-solutions.html",
    "title": "6 – Lösungen",
    "section": "",
    "text": "library(readr)\nlibrary(psych)\nlibrary(pastecs)\nlibrary(car)\n\ndf = read_delim(\"household_power_consumption.zip\", delim=\";\", na=c(\"\", \"?\"))\n\nsapply(df[, 3:6], mean, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n            1.0916150             0.1237145           240.8398580             4.6277593 \n\nsapply(df[, 3:6], median, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n                0.602                 0.100               241.010                 2.600 \n\nsapply(df[, 3:6], min, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n                0.076                 0.000               223.200                 0.200 \n\nsapply(df[, 3:6], max, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n               11.122                 1.390               254.150                48.400 \n\nsummary(df[, 3:6])\n\n Global_active_power Global_reactive_power    Voltage      Global_intensity\n Min.   : 0.076      Min.   :0.0000        Min.   :223.2   Min.   : 0.200  \n 1st Qu.: 0.308      1st Qu.:0.0480        1st Qu.:239.0   1st Qu.: 1.400  \n Median : 0.602      Median :0.1000        Median :241.0   Median : 2.600  \n Mean   : 1.092      Mean   :0.1237        Mean   :240.8   Mean   : 4.628  \n 3rd Qu.: 1.528      3rd Qu.:0.1940        3rd Qu.:242.9   3rd Qu.: 6.400  \n Max.   :11.122      Max.   :1.3900        Max.   :254.2   Max.   :48.400  \n NA's   :25979       NA's   :25979         NA's   :25979   NA's   :25979   \n\ndescribe(df[, 3:6])\n\n                      vars       n   mean   sd median trimmed  mad    min    max range  skew kurtosis se\nGlobal_active_power      1 2049280   1.09 1.06   0.60    0.90 0.59   0.08  11.12 11.05  1.79     4.22  0\nGlobal_reactive_power    2 2049280   0.12 0.11   0.10    0.11 0.13   0.00   1.39  1.39  1.26     2.61  0\nVoltage                  3 2049280 240.84 3.24 241.01  240.93 2.88 223.20 254.15 30.95 -0.33     0.72  0\nGlobal_intensity         4 2049280   4.63 4.44   2.60    3.83 2.67   0.20  48.40 48.20  1.85     4.60  0\n\nround(stat.desc(df[, 3:6]), 1)\n\n             Global_active_power Global_reactive_power     Voltage Global_intensity\nnbr.val                2049280.0             2049280.0   2049280.0        2049280.0\nnbr.null                     0.0              481561.0         0.0              0.0\nnbr.na                   25979.0               25979.0     25979.0          25979.0\nmin                          0.1                   0.0       223.2              0.2\nmax                         11.1                   1.4       254.2             48.4\nrange                       11.0                   1.4        31.0             48.2\nsum                    2237024.9              253525.6 493548304.1        9483574.6\nmedian                       0.6                   0.1       241.0              2.6\nmean                         1.1                   0.1       240.8              4.6\nSE.mean                      0.0                   0.0         0.0              0.0\nCI.mean.0.95                 0.0                   0.0         0.0              0.0\nvar                          1.1                   0.0        10.5             19.8\nstd.dev                      1.1                   0.1         3.2              4.4\ncoef.var                     1.0                   0.9         0.0              1.0\n\n\nAus den Ausgaben der zusammenfassenden Statistiken kann abgelesen werden, dass die mittlere Spannung 240.84 und der Median der globale Wirkleistung 0.602 beträgt. Die Funktion summary() zeigt außerdem an, dass es pro Spalte 25979 fehlende Werte gibt (unter NA's)."
  },
  {
    "objectID": "06/06-solutions.html#übung-1",
    "href": "06/06-solutions.html#übung-1",
    "title": "6 – Lösungen",
    "section": "",
    "text": "library(readr)\nlibrary(psych)\nlibrary(pastecs)\nlibrary(car)\n\ndf = read_delim(\"household_power_consumption.zip\", delim=\";\", na=c(\"\", \"?\"))\n\nsapply(df[, 3:6], mean, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n            1.0916150             0.1237145           240.8398580             4.6277593 \n\nsapply(df[, 3:6], median, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n                0.602                 0.100               241.010                 2.600 \n\nsapply(df[, 3:6], min, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n                0.076                 0.000               223.200                 0.200 \n\nsapply(df[, 3:6], max, na.rm=TRUE)\n\n  Global_active_power Global_reactive_power               Voltage      Global_intensity \n               11.122                 1.390               254.150                48.400 \n\nsummary(df[, 3:6])\n\n Global_active_power Global_reactive_power    Voltage      Global_intensity\n Min.   : 0.076      Min.   :0.0000        Min.   :223.2   Min.   : 0.200  \n 1st Qu.: 0.308      1st Qu.:0.0480        1st Qu.:239.0   1st Qu.: 1.400  \n Median : 0.602      Median :0.1000        Median :241.0   Median : 2.600  \n Mean   : 1.092      Mean   :0.1237        Mean   :240.8   Mean   : 4.628  \n 3rd Qu.: 1.528      3rd Qu.:0.1940        3rd Qu.:242.9   3rd Qu.: 6.400  \n Max.   :11.122      Max.   :1.3900        Max.   :254.2   Max.   :48.400  \n NA's   :25979       NA's   :25979         NA's   :25979   NA's   :25979   \n\ndescribe(df[, 3:6])\n\n                      vars       n   mean   sd median trimmed  mad    min    max range  skew kurtosis se\nGlobal_active_power      1 2049280   1.09 1.06   0.60    0.90 0.59   0.08  11.12 11.05  1.79     4.22  0\nGlobal_reactive_power    2 2049280   0.12 0.11   0.10    0.11 0.13   0.00   1.39  1.39  1.26     2.61  0\nVoltage                  3 2049280 240.84 3.24 241.01  240.93 2.88 223.20 254.15 30.95 -0.33     0.72  0\nGlobal_intensity         4 2049280   4.63 4.44   2.60    3.83 2.67   0.20  48.40 48.20  1.85     4.60  0\n\nround(stat.desc(df[, 3:6]), 1)\n\n             Global_active_power Global_reactive_power     Voltage Global_intensity\nnbr.val                2049280.0             2049280.0   2049280.0        2049280.0\nnbr.null                     0.0              481561.0         0.0              0.0\nnbr.na                   25979.0               25979.0     25979.0          25979.0\nmin                          0.1                   0.0       223.2              0.2\nmax                         11.1                   1.4       254.2             48.4\nrange                       11.0                   1.4        31.0             48.2\nsum                    2237024.9              253525.6 493548304.1        9483574.6\nmedian                       0.6                   0.1       241.0              2.6\nmean                         1.1                   0.1       240.8              4.6\nSE.mean                      0.0                   0.0         0.0              0.0\nCI.mean.0.95                 0.0                   0.0         0.0              0.0\nvar                          1.1                   0.0        10.5             19.8\nstd.dev                      1.1                   0.1         3.2              4.4\ncoef.var                     1.0                   0.9         0.0              1.0\n\n\nAus den Ausgaben der zusammenfassenden Statistiken kann abgelesen werden, dass die mittlere Spannung 240.84 und der Median der globale Wirkleistung 0.602 beträgt. Die Funktion summary() zeigt außerdem an, dass es pro Spalte 25979 fehlende Werte gibt (unter NA's)."
  },
  {
    "objectID": "06/06-solutions.html#übung-2",
    "href": "06/06-solutions.html#übung-2",
    "title": "6 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\ndim(penguins)  # 344 Zeilen, 8 Spalten\n\n[1] 344   8\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm   flipper_length_mm  body_mass_g       sex     \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10   Min.   :172.0     Min.   :2700   female:165  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60   1st Qu.:190.0     1st Qu.:3550   male  :168  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30   Median :197.0     Median :4050   NA's  : 11  \n                                 Mean   :43.92   Mean   :17.15   Mean   :200.9     Mean   :4202               \n                                 3rd Qu.:48.50   3rd Qu.:18.70   3rd Qu.:213.0     3rd Qu.:4750               \n                                 Max.   :59.60   Max.   :21.50   Max.   :231.0     Max.   :6300               \n                                 NA's   :2       NA's   :2       NA's   :2         NA's   :2                  \n      year     \n Min.   :2007  \n 1st Qu.:2007  \n Median :2008  \n Mean   :2008  \n 3rd Qu.:2009  \n Max.   :2009  \n               \n\nby(penguins[, 3:5], penguins$species, colMeans, na.rm=TRUE)\n\npenguins$species: Adelie\n   bill_length_mm     bill_depth_mm flipper_length_mm \n         38.79139          18.34636         189.95364 \n------------------------------------------------------------------------------------------ \npenguins$species: Chinstrap\n   bill_length_mm     bill_depth_mm flipper_length_mm \n         48.83382          18.42059         195.82353 \n------------------------------------------------------------------------------------------ \npenguins$species: Gentoo\n   bill_length_mm     bill_depth_mm flipper_length_mm \n         47.50488          14.98211         217.18699 \n\n\nDie Faktorspalten species und island haben jeweils drei Stufen, die Spalte sex hat zwei Stufen (wobei hier auch fehlende Werte vorkommen). Laut summary(penguins) gibt es in den Spalten 3–6 jeweils 2 fehlende Werte und in der 7. Spalte 11 fehlende Werte."
  },
  {
    "objectID": "06/06-solutions.html#übung-3",
    "href": "06/06-solutions.html#übung-3",
    "title": "6 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\ndescribe(mtcars)  # min, max, mean, median\n\n     vars  n   mean     sd median trimmed    mad   min    max  range  skew kurtosis    se\nmpg     1 32  20.09   6.03  19.20   19.70   5.41 10.40  33.90  23.50  0.61    -0.37  1.07\ncyl     2 32   6.19   1.79   6.00    6.23   2.97  4.00   8.00   4.00 -0.17    -1.76  0.32\ndisp    3 32 230.72 123.94 196.30  222.52 140.48 71.10 472.00 400.90  0.38    -1.21 21.91\nhp      4 32 146.69  68.56 123.00  141.19  77.10 52.00 335.00 283.00  0.73    -0.14 12.12\ndrat    5 32   3.60   0.53   3.70    3.58   0.70  2.76   4.93   2.17  0.27    -0.71  0.09\nwt      6 32   3.22   0.98   3.33    3.15   0.77  1.51   5.42   3.91  0.42    -0.02  0.17\nqsec    7 32  17.85   1.79  17.71   17.83   1.42 14.50  22.90   8.40  0.37     0.34  0.32\nvs      8 32   0.44   0.50   0.00    0.42   0.00  0.00   1.00   1.00  0.24    -2.00  0.09\nam      9 32   0.41   0.50   0.00    0.38   0.00  0.00   1.00   1.00  0.36    -1.92  0.09\ngear   10 32   3.69   0.74   4.00    3.62   1.48  3.00   5.00   2.00  0.53    -1.07  0.13\ncarb   11 32   2.81   1.62   2.00    2.65   1.48  1.00   8.00   7.00  1.05     1.26  0.29\n\nshapiro.test(mtcars$mpg)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$mpg\nW = 0.94756, p-value = 0.1229\n\n\nDie Nullhypothese der Normalverteilung kann nicht verworfen werden (\\(p = 0.123\\)).\n\nleveneTest(mtcars$mpg, mtcars$cyl)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)   \ngroup  2  5.5071 0.00939 **\n      29                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDie Nullhypothese der Varianzhomogenität kann verworfen werden (\\(p = 0.009\\))."
  },
  {
    "objectID": "06/06-solutions.html#übung-4",
    "href": "06/06-solutions.html#übung-4",
    "title": "6 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\nlibrary(readr)\n\ndf = read_tsv(\"lecturer.dat\")\n\nVersuchen wir zunächst, die Mittelwerte der numerischen Spalten mit der Funktion mean() zu berechnen:\n\nby(df[, -(1:3)], df$job, mean)\n\nWarning in mean.default(data[x, , drop = FALSE], ...): argument is not numeric or logical: returning NA\nWarning in mean.default(data[x, , drop = FALSE], ...): argument is not numeric or logical: returning NA\n\n\ndf$job: 1\n[1] NA\n------------------------------------------------------------------------------------------ \ndf$job: 2\n[1] NA\n\n\nDies führt zu Warnungen sowie NA für die Ergebnisse beider Gruppen. Der Grund ist, dass die gruppierten Daten nach wie vor als Data Frame mit vier numerischen Spalten vorliegen. Die Funktion mean() funktioniert aber nur mit einem Vektor (bzw. einer einzigen Spalte eines Data Frames). Daher müssen wir eine andere Aggregierungsfunktion verwenden, die mit Data Frames umgehen kann. Eine Möglichkeit ist die Funktion colMeans():\n\nby(df[, -(1:3)], df$job, colMeans)\n\ndf$job: 1\n friends  alcohol   income neurotic \n     2.4     16.0  33400.0     15.0 \n------------------------------------------------------------------------------------------ \ndf$job: 2\n friends  alcohol   income neurotic \n    13.2     19.2   3622.0     11.2"
  },
  {
    "objectID": "06/06-solutions.html#übung-5",
    "href": "06/06-solutions.html#übung-5",
    "title": "6 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\nset.seed(4)  # stellt sicher, dass die Zufallszahlen reproduzierbar sind\nx = rnorm(5001)\nshapiro.test(x)\n\nError in shapiro.test(x): sample size must be between 3 and 5000\n\n\nDie Funktion shapiro.test() funktioniert nur bis zu einer Stichprobengröße von 5000, darüber hinaus wird ein Fehler ausgegeben. Für dieses Verhalten gibt es zwei Gründe:\n\nEs ist generell nicht sinnvoll, eine sehr große Stichprobe auf Normalverteilung zu testen, da die Teststatistik selbst bei geringsten Abweichungen von der Normalverteilung sehr wahrscheinlich signifikant wird.\nTheoretisch könnte der Shapiro-Wilk-Test auch für Stichproben größer als 5000 gerechnet werden, dies wurde in der ursprünglichen Publikation des Tests sowie dessen Implementierung in der Programmiersprache FORTRAN jedoch nicht berücksichtigt. Die Funktion shapiro.wilk() in R basiert auf dieser FORTRAN-Implementierung und ist daher ebenfalls auf Stichproben bis zu einer Größe von 5000 beschränkt.\n\nEine detaillierte Erklärung von Ben Bolker ist hier zu finden."
  },
  {
    "objectID": "04/04-solutions.html",
    "href": "04/04-solutions.html",
    "title": "4 – Lösungen",
    "section": "",
    "text": "u = seq(98, 50, -2)\nv = seq(0, 48, 2)\nlength(u)  # 25 Elemente -&gt; 5 Zeilen bedingen 5 Spalten\n\n[1] 25\n\nlength(v)  # 25 Elemente -&gt; 5 Zeilen bedingen 5 Spalten\n\n[1] 25\n\ndim(u) = c(5, 5)\ndim(v) = c(5, 5)\n(r = cbind(u, v))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]   98   88   78   68   58    0   10   20   30    40\n[2,]   96   86   76   66   56    2   12   22   32    42\n[3,]   94   84   74   64   54    4   14   24   34    44\n[4,]   92   82   72   62   52    6   16   26   36    46\n[5,]   90   80   70   60   50    8   18   28   38    48\n\nclass(r)  # matrix\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "04/04-solutions.html#übung-1",
    "href": "04/04-solutions.html#übung-1",
    "title": "4 – Lösungen",
    "section": "",
    "text": "u = seq(98, 50, -2)\nv = seq(0, 48, 2)\nlength(u)  # 25 Elemente -&gt; 5 Zeilen bedingen 5 Spalten\n\n[1] 25\n\nlength(v)  # 25 Elemente -&gt; 5 Zeilen bedingen 5 Spalten\n\n[1] 25\n\ndim(u) = c(5, 5)\ndim(v) = c(5, 5)\n(r = cbind(u, v))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]   98   88   78   68   58    0   10   20   30    40\n[2,]   96   86   76   66   56    2   12   22   32    42\n[3,]   94   84   74   64   54    4   14   24   34    44\n[4,]   92   82   72   62   52    6   16   26   36    46\n[5,]   90   80   70   60   50    8   18   28   38    48\n\nclass(r)  # matrix\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "04/04-solutions.html#übung-2",
    "href": "04/04-solutions.html#übung-2",
    "title": "4 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\ndim(r)  # 5 Zeilen, 10 Spalten\n\n[1]  5 10\n\nlength(r)  # 50 Elemente\n\n[1] 50\n\nr[4, 6]  # 6\n\n[1] 6\n\nrowMeans(r)\n\n[1] 49 49 49 49 49\n\ncolMeans(r)\n\n [1] 94 84 74 64 54  4 14 24 34 44\n\nmean(r[3:5, 1:2])  # 87\n\n[1] 87"
  },
  {
    "objectID": "04/04-solutions.html#übung-3",
    "href": "04/04-solutions.html#übung-3",
    "title": "4 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\ndf = data.frame(\n    name=c(\n        \"Ben\", \"Emma\", \"Luis\", \"Mia\", \"Paul\",\n        \"Hanna\", \"Lukas\", \"Sophia\", \"Jonas\", \"Emilia\"\n    ),\n    gender=rep(c(\"m\", \"f\"), 5),\n    value=c(11, 76, 42, 8, 32, 96, 88, 65, 14, 50)\n)\ndf\n\n     name gender value\n1     Ben      m    11\n2    Emma      f    76\n3    Luis      m    42\n4     Mia      f     8\n5    Paul      m    32\n6   Hanna      f    96\n7   Lukas      m    88\n8  Sophia      f    65\n9   Jonas      m    14\n10 Emilia      f    50\n\nstr(df)  # character, character, numeric\n\n'data.frame':   10 obs. of  3 variables:\n $ name  : chr  \"Ben\" \"Emma\" \"Luis\" \"Mia\" ...\n $ gender: chr  \"m\" \"f\" \"m\" \"f\" ...\n $ value : num  11 76 42 8 32 96 88 65 14 50"
  },
  {
    "objectID": "04/04-solutions.html#übung-4",
    "href": "04/04-solutions.html#übung-4",
    "title": "4 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\ndf_f = df[df[, \"gender\"] == \"f\", ]\ndf_f\n\n     name gender value\n2    Emma      f    76\n4     Mia      f     8\n6   Hanna      f    96\n8  Sophia      f    65\n10 Emilia      f    50\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nZur Erstellung von df_f gibt es noch weitere Varianten:\n\ndf_f = df[df$gender == \"f\", ]\ndf_f = df[df[[\"gender\"]] == \"f\",]\ndf_f = df[df[[2]] == \"f\",]\ndf_f = df[df[, 2] == \"f\",]\n\n\n\nFünf Möglichkeiten, um auf die erste Spalte zuzugreifen:\n\ndf_f[, 1]  # Spalte 1\n\n[1] \"Emma\"   \"Mia\"    \"Hanna\"  \"Sophia\" \"Emilia\"\n\ndf_f[, \"name\"]  # Spalte \"name\"\n\n[1] \"Emma\"   \"Mia\"    \"Hanna\"  \"Sophia\" \"Emilia\"\n\ndf_f$name  # Spalte \"name\"\n\n[1] \"Emma\"   \"Mia\"    \"Hanna\"  \"Sophia\" \"Emilia\"\n\ndf_f[[\"name\"]]  # Spalte \"name\"\n\n[1] \"Emma\"   \"Mia\"    \"Hanna\"  \"Sophia\" \"Emilia\"\n\ndf_f[[1]]  # Spalte 1\n\n[1] \"Emma\"   \"Mia\"    \"Hanna\"  \"Sophia\" \"Emilia\""
  },
  {
    "objectID": "04/04-solutions.html#übung-5",
    "href": "04/04-solutions.html#übung-5",
    "title": "4 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\ndim(mtcars)  # 32 Zeilen, 11 Spalten\n\n[1] 32 11\n\nmtcars1 = mtcars[mtcars$mpg &gt; 25, ]\ndim(mtcars1)  # 6 Zeilen, 11 Spalten\n\n[1]  6 11"
  },
  {
    "objectID": "04/04-solutions.html#übung-6",
    "href": "04/04-solutions.html#übung-6",
    "title": "4 – Lösungen",
    "section": "Übung 6",
    "text": "Übung 6\n\nlibrary(tibble)\n\nair = as_tibble(airquality)\n\n\nairquality[, 1]\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\nair[, 1]\n\n# A tibble: 153 × 1\n   Ozone\n   &lt;int&gt;\n 1    41\n 2    36\n 3    12\n 4    18\n 5    NA\n 6    28\n 7    23\n 8    19\n 9     8\n10    NA\n# ℹ 143 more rows\n\nairquality[, \"Ozone\"]\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\nair[, \"Ozone\"]\n\n# A tibble: 153 × 1\n   Ozone\n   &lt;int&gt;\n 1    41\n 2    36\n 3    12\n 4    18\n 5    NA\n 6    28\n 7    23\n 8    19\n 9     8\n10    NA\n# ℹ 143 more rows\n\n\nBeim Indizieren einer Spalte mit eckigen Klammern erhält man bei einem Data Frame einen Vektor, bei einem Tibble erhält man jedoch ein Tibble.\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\nair$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\nairquality[[\"Ozone\"]]\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\nair[[\"Ozone\"]]\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6  30  11   1  11   4  32  NA  NA  NA  23\n [29]  45 115  37  NA  NA  NA  NA  NA  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA  NA  NA\n [57]  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA  10  27  NA   7  48  35  61  79  63  16  NA  NA\n [85]  80 108  20  52  82  50  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22  59  23  31  44\n[113]  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73  91  47  32  20  23  21  24  44  21  28   9  13  46  18\n[141]  13  24  16  13  23  36   7  14  30  NA  14  18  20\n\n\nBei diesen Varianten erhält man die Daten in der Spalte immer als Vektor.\nTibbles sind beim Indizieren mit eckigen Klammern konsistenter, weil das Ergebnis immer ein Tibble ist. Bei Data Frames ist das Ergebnis manchmal ein Vektor (wie in den obigen Beispielen), manchmal aber auch ein Data Frame (z.B. wenn man zwei Spalten herausgreift):\n\nairquality[, 1:2]\n\n    Ozone Solar.R\n1      41     190\n2      36     118\n3      12     149\n4      18     313\n5      NA      NA\n6      28      NA\n7      23     299\n8      19      99\n9       8      19\n10     NA     194\n11      7      NA\n12     16     256\n13     11     290\n14     14     274\n15     18      65\n16     14     334\n17     34     307\n18      6      78\n19     30     322\n20     11      44\n21      1       8\n22     11     320\n23      4      25\n24     32      92\n25     NA      66\n26     NA     266\n27     NA      NA\n28     23      13\n29     45     252\n30    115     223\n31     37     279\n32     NA     286\n33     NA     287\n34     NA     242\n35     NA     186\n36     NA     220\n37     NA     264\n38     29     127\n39     NA     273\n40     71     291\n41     39     323\n42     NA     259\n43     NA     250\n44     23     148\n45     NA     332\n46     NA     322\n47     21     191\n48     37     284\n49     20      37\n50     12     120\n51     13     137\n52     NA     150\n53     NA      59\n54     NA      91\n55     NA     250\n56     NA     135\n57     NA     127\n58     NA      47\n59     NA      98\n60     NA      31\n61     NA     138\n62    135     269\n63     49     248\n64     32     236\n65     NA     101\n66     64     175\n67     40     314\n68     77     276\n69     97     267\n70     97     272\n71     85     175\n72     NA     139\n73     10     264\n74     27     175\n75     NA     291\n76      7      48\n77     48     260\n78     35     274\n79     61     285\n80     79     187\n81     63     220\n82     16       7\n83     NA     258\n84     NA     295\n85     80     294\n86    108     223\n87     20      81\n88     52      82\n89     82     213\n90     50     275\n91     64     253\n92     59     254\n93     39      83\n94      9      24\n95     16      77\n96     78      NA\n97     35      NA\n98     66      NA\n99    122     255\n100    89     229\n101   110     207\n102    NA     222\n103    NA     137\n104    44     192\n105    28     273\n106    65     157\n107    NA      64\n108    22      71\n109    59      51\n110    23     115\n111    31     244\n112    44     190\n113    21     259\n114     9      36\n115    NA     255\n116    45     212\n117   168     238\n118    73     215\n119    NA     153\n120    76     203\n121   118     225\n122    84     237\n123    85     188\n124    96     167\n125    78     197\n126    73     183\n127    91     189\n128    47      95\n129    32      92\n130    20     252\n131    23     220\n132    21     230\n133    24     259\n134    44     236\n135    21     259\n136    28     238\n137     9      24\n138    13     112\n139    46     237\n140    18     224\n141    13      27\n142    24     238\n143    16     201\n144    13     238\n145    23      14\n146    36     139\n147     7      49\n148    14      20\n149    30     193\n150    NA     145\n151    14     191\n152    18     131\n153    20     223\n\nair[, 1:2]\n\n# A tibble: 153 × 2\n   Ozone Solar.R\n   &lt;int&gt;   &lt;int&gt;\n 1    41     190\n 2    36     118\n 3    12     149\n 4    18     313\n 5    NA      NA\n 6    28      NA\n 7    23     299\n 8    19      99\n 9     8      19\n10    NA     194\n# ℹ 143 more rows\n\n\nMöchte man für eine einzige Spalte einen Vektor, kann man bei beiden Datentypen entweder $ oder [[]] verwenden (letzteres ist zu empfehlen)."
  },
  {
    "objectID": "12/12-solutions.html",
    "href": "12/12-solutions.html",
    "title": "12 – Lösungen",
    "section": "",
    "text": "library(dplyr)\n\nt.test(height ~ gender, data=starwars, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  height by gender\nt = -1.2507, df = 31.676, p-value = 0.1101\nalternative hypothesis: true difference in means between group feminine and group masculine is less than 0\n95 percent confidence interval:\n     -Inf 3.547265\nsample estimates:\n mean in group feminine mean in group masculine \n               166.5333                176.5323 \n\n\nWeibliche Charaktere sind nicht signifikant kleiner als männliche Charaktere (\\(t=-1.25\\), \\(p = .110\\)).\n\n\n\n\nt.test(mass ~ gender, data=starwars, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  mass by gender\nt = -1.8706, df = 46.968, p-value = 0.03382\nalternative hypothesis: true difference in means between group feminine and group masculine is less than 0\n95 percent confidence interval:\n      -Inf -5.336999\nsample estimates:\n mean in group feminine mean in group masculine \n               54.68889               106.51489 \n\n\nWeibliche Charaktere sind signifikant leichter als männliche Charaktere (\\(t=-1.87\\), \\(p = .034\\))."
  },
  {
    "objectID": "12/12-solutions.html#übung-1",
    "href": "12/12-solutions.html#übung-1",
    "title": "12 – Lösungen",
    "section": "",
    "text": "library(dplyr)\n\nt.test(height ~ gender, data=starwars, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  height by gender\nt = -1.2507, df = 31.676, p-value = 0.1101\nalternative hypothesis: true difference in means between group feminine and group masculine is less than 0\n95 percent confidence interval:\n     -Inf 3.547265\nsample estimates:\n mean in group feminine mean in group masculine \n               166.5333                176.5323 \n\n\nWeibliche Charaktere sind nicht signifikant kleiner als männliche Charaktere (\\(t=-1.25\\), \\(p = .110\\)).\n\n\n\n\nt.test(mass ~ gender, data=starwars, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  mass by gender\nt = -1.8706, df = 46.968, p-value = 0.03382\nalternative hypothesis: true difference in means between group feminine and group masculine is less than 0\n95 percent confidence interval:\n      -Inf -5.336999\nsample estimates:\n mean in group feminine mean in group masculine \n               54.68889               106.51489 \n\n\nWeibliche Charaktere sind signifikant leichter als männliche Charaktere (\\(t=-1.87\\), \\(p = .034\\))."
  },
  {
    "objectID": "12/12-solutions.html#übung-2",
    "href": "12/12-solutions.html#übung-2",
    "title": "12 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nlibrary(palmerpenguins)\n\n\nSchnabellänge\n\nwith(\n    subset(penguins, species == \"Adelie\" | species == \"Chinstrap\"),\n    t.test(bill_length_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -21.865, df = 106.97, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -10.952948  -9.131917\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               38.79139                48.83382 \n\nwith(\n    subset(penguins, species == \"Adelie\" | species == \"Gentoo\"),\n    t.test(bill_length_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -24.725, df = 242.58, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -9.407672 -8.019303\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            38.79139             47.50488 \n\nwith(\n    subset(penguins, species == \"Gentoo\" | species == \"Chinstrap\"),\n    t.test(bill_length_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = 2.706, df = 129.22, p-value = 0.00773\nalternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n95 percent confidence interval:\n 0.3572698 2.3006212\nsample estimates:\nmean in group Chinstrap    mean in group Gentoo \n               48.83382                47.50488 \n\n\n\n\nSchnabeltiefe\n\nwith(\n    subset(penguins, species == \"Adelie\" | species == \"Chinstrap\"),\n    t.test(bill_depth_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_depth_mm by species\nt = -0.43771, df = 137.75, p-value = 0.6623\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -0.4095657  0.2611044\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               18.34636                18.42059 \n\nwith(\n    subset(penguins, species == \"Adelie\" | species == \"Gentoo\"),\n    t.test(bill_depth_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_depth_mm by species\nt = 25.337, df = 271.98, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n 3.102837 3.625651\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            18.34636             14.98211 \n\nwith(\n    subset(penguins, species == \"Gentoo\" | species == \"Chinstrap\"),\n    t.test(bill_depth_mm ~ species)\n)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_depth_mm by species\nt = 21.01, df = 122.3, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n95 percent confidence interval:\n 3.114497 3.762452\nsample estimates:\nmean in group Chinstrap    mean in group Gentoo \n               18.42059                14.98211 \n\n\n\n\nVerwendung von pairwise.t.test()\n\npairwise.t.test(penguins$bill_length_mm, penguins$species)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  penguins$bill_length_mm and penguins$species \n\n          Adelie Chinstrap\nChinstrap &lt;2e-16 -        \nGentoo    &lt;2e-16 0.0032   \n\nP value adjustment method: holm \n\npairwise.t.test(penguins$bill_depth_mm, penguins$species)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  penguins$bill_depth_mm and penguins$species \n\n          Adelie Chinstrap\nChinstrap 0.65   -        \nGentoo    &lt;2e-16 &lt;2e-16   \n\nP value adjustment method: holm"
  },
  {
    "objectID": "12/12-solutions.html#übung-3",
    "href": "12/12-solutions.html#übung-3",
    "title": "12 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\nFür einen gepaarten t-Test müssen wir die Daten zunächst ins Wide-Format konvertieren:\n\nlibrary(tidyr)\n\nsleep_wide = pivot_wider(\n    data=sleep,\n    id_cols=ID,\n    names_from=group,\n    names_prefix=\"g\",\n    values_from=extra\n)\n\nJetzt können wir den Test durchführen:\n\nt.test(sleep_wide$g1, sleep_wide$g2, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  sleep_wide$g1 and sleep_wide$g2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nDer mittlere Unterschied beträgt −1.58, welcher signifikant von Null verschieden ist (\\(p = .003\\)). Das 95%-Konfidenzintervall reicht von −2.46 bis −0.70."
  },
  {
    "objectID": "02/02.html",
    "href": "02/02.html",
    "title": "2 – Die R-Umgebung",
    "section": "",
    "text": "Bereits in der vorigen Einheit haben wir die grafische Benutzeroberfläche RStudio installiert und verwendet. RStudio setzt eine funktionierende Installation von R voraus. Das Programmfenster von RStudio ist in der Standardeinstellung in vier Bereiche unterteilt (wobei oft nur drei Bereiche sichtbar sind):\n\nLinks unten befindet sich die Console (die anderen Tabs sind für uns nicht relevant). Hier wartet R auf unsere Eingaben.\nLinks oben befindet sich der Editor (falls zumindest eine Datei geöffnet ist). Ansonsten nimmt die Console diesen Platz ein.\nRechts oben gibt es eine Übersicht aller geladenen Objekte (Environment) und eine Liste aller jemals eingegebenen Befehle (History) (die anderen Tabs sind für uns hier nicht von Bedeutung). Auf die History kann man übrigens auch in der Console mit den Tasten ↑ bzw. ↓ zugreifen und vor der erneuten Bestätigung mit der Eingabetaste auch editieren.\nRechts unten werden wahlweise der Inhalt des aktuellen Verzeichnisses (Files), grafische Ausgaben (Plots), eine Paketverwaltung (Packages) oder ein Hilfefenster (Help) angezeigt (die anderen Tabs sind für uns nicht relevant).\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDie Aufteilung der Bereiche kann in den Einstellungen umfangreich angepasst werden. Wenn Sie z.B. die Console gerne rechts unten haben möchten, können Sie das dort festlegen.\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nDie meisten Einstellungen von RStudio haben vernünftige Standardwerte, aber die folgenden zwei Punkte sollten Sie unbedingt ändern. Gehen Sie dazu in die Einstellungen (Tools – Global Options) und ändern Sie diese beiden Optionen:\n\n“Restore .RData into workspace at startup” sollte deaktiviert sein.\n“Save workspace to .RData on exit” sollte auf Never gesetzt sein.\n\n\nDies bewirkt, dass jeder Neustart von RStudio auch eine komplett frische R-Sitzung generiert, was für die Reproduzierbarkeit von Analysen unerlässlich ist."
  },
  {
    "objectID": "02/02.html#rstudio",
    "href": "02/02.html#rstudio",
    "title": "2 – Die R-Umgebung",
    "section": "",
    "text": "Bereits in der vorigen Einheit haben wir die grafische Benutzeroberfläche RStudio installiert und verwendet. RStudio setzt eine funktionierende Installation von R voraus. Das Programmfenster von RStudio ist in der Standardeinstellung in vier Bereiche unterteilt (wobei oft nur drei Bereiche sichtbar sind):\n\nLinks unten befindet sich die Console (die anderen Tabs sind für uns nicht relevant). Hier wartet R auf unsere Eingaben.\nLinks oben befindet sich der Editor (falls zumindest eine Datei geöffnet ist). Ansonsten nimmt die Console diesen Platz ein.\nRechts oben gibt es eine Übersicht aller geladenen Objekte (Environment) und eine Liste aller jemals eingegebenen Befehle (History) (die anderen Tabs sind für uns hier nicht von Bedeutung). Auf die History kann man übrigens auch in der Console mit den Tasten ↑ bzw. ↓ zugreifen und vor der erneuten Bestätigung mit der Eingabetaste auch editieren.\nRechts unten werden wahlweise der Inhalt des aktuellen Verzeichnisses (Files), grafische Ausgaben (Plots), eine Paketverwaltung (Packages) oder ein Hilfefenster (Help) angezeigt (die anderen Tabs sind für uns nicht relevant).\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDie Aufteilung der Bereiche kann in den Einstellungen umfangreich angepasst werden. Wenn Sie z.B. die Console gerne rechts unten haben möchten, können Sie das dort festlegen.\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nDie meisten Einstellungen von RStudio haben vernünftige Standardwerte, aber die folgenden zwei Punkte sollten Sie unbedingt ändern. Gehen Sie dazu in die Einstellungen (Tools – Global Options) und ändern Sie diese beiden Optionen:\n\n“Restore .RData into workspace at startup” sollte deaktiviert sein.\n“Save workspace to .RData on exit” sollte auf Never gesetzt sein.\n\n\nDies bewirkt, dass jeder Neustart von RStudio auch eine komplett frische R-Sitzung generiert, was für die Reproduzierbarkeit von Analysen unerlässlich ist."
  },
  {
    "objectID": "02/02.html#pakete",
    "href": "02/02.html#pakete",
    "title": "2 – Die R-Umgebung",
    "section": "Pakete",
    "text": "Pakete\nPakete erweitern den Funktionsumfang von R. Werksmäßig wird R nur mit einer Handvoll an Paketen ausgeliefert – sobald man damit nicht mehr auskommt, kann man sehr einfach zusätzliche Pakete hinzufügen. Die meisten Zusatzpakete sind im Comprehensive R Archive Network (CRAN) verfügbar. Wenn ein Paket einmal installiert ist, kann es danach jederzeit aktiviert und verwendet werden. Das bedeutet also, dass folgende zwei voneinander abhängige Schritte auszuführen sind:\n\nInstallieren eines neuen Pakets (nur ein Mal)\nAktivieren eines installierten Pakets (vor jeder Verwendung, d.h. ein Mal pro R-Sitzung)\n\nIm CRAN gibt es tausende Pakete, und deswegen kann es mitunter schwierig sein, ein passendes bzw. das gesuchte Paket zu finden. Eine sehr praktische Übersicht gibt es auf CRAN Task Views. Hier werden ausgewählte Pakete nach Anwendungsgebieten thematisch gruppiert dargestellt. Zusätzlich ist eine Internet-Suche nach dem gewünschten Thema in Verbindung mit R sehr häufig zielführend.\nZur Paketverwaltung gibt es zwei Möglichkeiten: entweder man verwendet den Bereich Packages in RStudio, oder man benutzt R-Befehle dafür.\n\nPaketverwaltung in RStudio\nRStudio bietet im Bereich Packages (im Panel rechts unten) eine Liste aller installierten Pakete. Hier ist auch ersichtlich, welche Pakete gerade aktiviert sind (durch Setzen/Entfernen des Häkchens vor einem Paket kann dieses aktiviert/deaktiviert werden).\nIn dieser Ansicht kann man durch Klicken auf Update installierte Pakete aktualisieren. Dies sollte man von Zeit zu Zeit auch tun, und zwar unabhängig von eventuellen Updates der verwendeten R-Version.\nNeue Pakete installiert man durch Klicken auf Install. Wenn man in das Feld Packages die Anfangsbuchstaben des gesuchten Pakets eingibt, wird automatisch eine Liste aller passenden Pakete vorgeschlagen. Ein bereits installiertes Paket kann man durch Klicken auf das hellgraue X in der rechten Spalte der Paketliste wieder deinstallieren.\n\n\n\nPaketverwaltung mit R-Befehlen\nPrinzipiell interagiert man mit R über Textbefehle in der Console. Daher ist es nicht überraschend, dass auch die Paketverwaltung mit speziellen R-Befehlen funktioniert. Tatsächlich verwendet die gerade beschriebene Paketverwaltung in RStudio einfach entsprechende R-Befehle im Hintergrund (diese sind auch in der Console ersichtlich).\nEine Liste aller installierten Pakete bekommt man mit:\n\nlibrary()\n\nDiese Liste entspricht der Darstellung im Bereich Packages in RStudio. Eine Liste aller aktivierten (geladenen) Pakete erhält man mit:\n\nsearch()\n\n[1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n[6] \"package:datasets\"  \"package:methods\"   \"Autoloads\"         \"package:base\"     \n\n\nEin neues Paket aus dem CRAN (z.B. psych) installiert man mit:\ninstall.packages(\"psych\")\nZu beachten ist, dass man den Namen des zu installierenden Paketes in Anführungszeichen angeben muss. Ein bereits installiertes Paket aktiviert man mit:\nlibrary(psych)\nHier kann man die Anführungszeichen um den Paketnamen weglassen."
  },
  {
    "objectID": "02/02.html#die-programmiersprache-r",
    "href": "02/02.html#die-programmiersprache-r",
    "title": "2 – Die R-Umgebung",
    "section": "Die Programmiersprache R",
    "text": "Die Programmiersprache R\n\nScripts\nBefehle in der R-Console einzugeben ist praktisch, wenn man neue Dinge interaktiv ausprobieren möchte bzw. nur schnell Kleinigkeiten berechnen will. Möchte man eine reproduzierbare Datenanalyse durchführen, dann sollte man die dafür notwendigen Befehle in einem sogenannten R-Script abspeichern. Damit kann man die Analyse später jederzeit wiederholen. Ein R-Script ist nichts anderes als eine einfache Textdatei mit der Endung .R.\nIn jeder Zeile eines R-Scripts befindet sich (meist) genau ein R-Befehl. Wenn man das Script ausführt, werden alle Zeilen vom Anfang bis zum Ende der Reihe nach ausgeführt. In RStudio kann man ein gesamtes Script durch Klicken auf die Schaltfläche “Source” (bzw. “Source with Echo”) ausführen. Möchte man nur die aktuelle Zeile bzw. die markierten Zeilen ausführen, kann man dies durch Klicken auf die Schaltfläche “Run” tun (dafür gibt es auch das Tastenkürzel StrgEnter unter Windows/Linux bzw. ⌘Enter unter macOS). Praktischerweise springt der Cursor danach zum nächsten Befehl im Script, welcher dann auf Wunsch wieder ausgeführt werden kann.\nDie folgende Abbildung zeigt ein kurzes R-Script, welches im Editor-Bereich von RStudio geöffnet ist:\n\n\n\n\n\n\n\nTipp\n\n\n\nWenn man in einem Script Befehle aus zusätzlichen Paketen benötigt, muss man diese Pakete im Script auch mittels library(package) aktivieren. Am besten geschieht dies ganz am Anfang des Scripts.\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nInstallieren Sie Pakete niemals in einem Script (mittels install.packages(\"package\"))! Das Paket würde bei jedem Ausführen des Scripts neu installiert werden (inklusive Download aus dem Internet), was definitiv vermieden werden sollte. Die Installation von Paketen wird also immer wie oben beschrieben manuell mit RStudio oder in der Console durchgeführt.\n\n\n\n\nArbeitsverzeichnis\nDas Arbeitsverzeichnis, also jenes Verzeichnis in dem die aktuelle R-Sitzung ausgeführt wird, erhält man mit dem Funktionsaufruf:\n\ngetwd()\n\nDieses Arbeitsverzeichnis (Working Directory) ist wichtig, da R diverse Dateien (wie z.B. Scripts oder Daten) immer in diesem (oder relativ zu diesem) Verzeichnis erwartet. Alle Dateien im aktuellen Arbeitsverzeichnis kann man mit dir() ausgeben lassen (RStudio zeigt diese rechts unten im Bereich Files an).\n\n\n\n\n\n\nTipp\n\n\n\nDer Titel der R-Console in RStudio zeigt ebenfalls das aktuelle Arbeitsverzeichnis an. Der Name ~ ist eine Abkürzung für den persönlichen Ordner.\n\n\nDer Befehl setwd(\"/path/to/working/directory\") setzt das aktuelle Arbeitsverzeichnis auf den angegebenen Wert (im Beispiel das fiktive Verzeichnis /path/to/working/directory). Hierbei ist zu beachten, dass Verzeichnisse auch in Windows durch einen normalen Schrägstrich / und nicht durch einen umgekehrten Schrägstrich (Backslash) \\ voneinander zu trennen sind.\nIn RStudio hat man aber mindestens drei Alternativen, das Arbeitsverzeichnis auch mit der grafischen Oberfläche zu setzen:\n\nMenü Session – Set Working Directory – Choose Directory…\nNavigieren zum gewünschten Verzeichnis im Bereich Files (Bereich rechts unten) und dann Klick auf More – Set As Working Directory\nFalls eine Script-Datei im Editorbereich geöffnet ist, kann man auf den Dateinamen im Tab rechtsklicken und Set Working Directory auswählen (dies setzt das Arbeitsverzeichnis auf jenes Verzeichnis, in dem das Script abgespeichert ist)\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nBevor Sie ein Script ausführen, sollten Sie das Arbeitsverzeichnis korrekt setzen (üblicherweise auf das Verzeichnis, in dem das Script abgespeichert ist). Führen Sie dies allerdings nicht automatisiert im Script selbst durch, denn das Script soll auch auf anderen Umgebungen laufen, wo es Ihr spezifisches Verzeichnis vielleicht nicht gibt.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nWenn Sie ein R-Script per Doppelklick (im Windows Explorer oder macOS Finder) öffnen, wird RStudio geöffnet und das Arbeitsverzeichnis automatisch auf das Verzeichnis gesetzt, in dem sich das Script befindet.\n\n\n\n\nWorkspace\nAlle selbst erstellten bzw. geladenen Objekte (Variablen und Daten) fasst man unter dem Begriff Workspace zusammen. Man kann ihn mit folgendem Befehl anzeigen:\n\nls()\n\nIn einer frisch gestarteten R-Sitzung ist der Workspace leer (falls nicht, haben Sie vermutlich die oben erwähnten Einstellungen nicht korrekt gesetzt). Dieses Verhalten ist für die Reproduzierbarkeit von Analysen eine wichtige Voraussetzung. In RStudio wird der Workspace auch im Bereich Environment (rechts oben) angezeigt .\n\n\nSyntax\nUnter Syntax versteht man die Regeln, wie man aus einzelnen Zeichen gültige R-Befehle erstellen kann. Um die Syntax von R kennenzulernen, betrachten wir das folgende kurze Beispiel-Script:\n# compute sum of integers from 1 to 100\nn = 100\nx = 1:n\nsum(x)\nn * (n + 1) / 2  # closed-form solution\nMan erkennt bereits in diesen wenigen Zeilen die grundlegenden Regeln. Prinzipiell wird jeder Befehl in eine eigene Zeile geschrieben.\n\nKommentare\nKommentare, d.h. alle Zeichen ab # bis zum Zeilenende, werden von R nicht ausgeführt, sondern einfach komplett ignoriert. Dennoch sind Kommentare extrem wichtig, da Sie zum Verstehen des Codes beitragen können – fügen Sie daher bei komplizierteren Berechnungen immer erklärende Kommentare hinzu!\n\n\nVariablen und Objekte\nDer Zuweisungsoperator in R ist = oder &lt;-. Damit kann man Werte (oft auch als Objekte bezeichnet) Variablen zuweisen, welche man dann später wieder verwenden kann (um z.B. damit weiterzurechnen). Im Beispiel wird also mit n = 100 der Wert 100 der Variablen n zugewiesen. Im gesamten obigen Beispiel-Script werden daher zwei Variablen namens n und x erstellt.\n\n\n\n\n\n\nHinweis\n\n\n\nEs ist egal, ob man = oder &lt;- für Zuweisungen verwendet. Man sollte aber konsistent sein und immer nur einen der beiden Zuweisungsoperatoren verwenden. In diesen Unterlagen wird = benutzt, weil man weniger tippen muss und die meisten anderen Programmiersprachen ebenfalls = verwenden.\n\n\nR unterscheidet streng zwischen Groß- und Kleinschreibung, d.h. die Variable N ist nicht gleich der Variablen n. Neben Buchstaben können auch Ziffern, Unterstriche und sogar Punkte (nicht empfohlen) für Variablennamen verwendet werden.\n\n\nFunktionen\nEine Funktion ist ein Mini-Script, welches man durch Aufrufen ausführen kann. Im obigen Beispiel ist sum eine Funktion. Zum Aufrufen einer Funktion ist ein Klammernpaar () nach dem Funktionsnamen notwendig. Eventuelle Argumente (die man benötigt, wenn die Funktion zusätzliche Informationen braucht) werden innerhalb dieser Klammern übergeben. Mehrere Argumente werden mit einem , voneinander getrennt.\nMit dem Befehl sum(x) wird also die Funktion sum mit dem Argument x aufgerufen. Es gibt auch Funktionen, die keine Argumente benötigen – das runde Klammernpaar ist aber trotzdem notwendig (z.B. library() ruft die Funktion library ohne Argumente auf). Weitere Beispiele für Funktionsaufrufe ohne Argumente, die wir bereits kennengelernt haben, sind search(), getwd(), dir() und ls().\n\n\n\n\n\n\nHinweis\n\n\n\nIn diesen Unterlagen werden Funktionsnamen mit nachfolgenden Klammern () geschrieben, um deutlich zu machen, dass es sich dabei um eine Funktion handelt.\n\n\nMit Objekten und Funktionen hätten wir bereits die wichtigsten beiden Konzepte in R besprochen. Zusammenfassend kann man also sagen (aus John M. Chambers, Extending R, Chapman & Hall/CRC, 2016):\n\nAlles, was in R existiert, ist ein Objekt.\nAlles, was in R passiert, geschieht durch Aufrufen einer Funktion.\n\n\n\n\nHilfe\nR beinhaltet eine sehr gute integrierte Hilfe zu allen möglichen Themen und Befehlen. In RStudio sind im Bereich Help alle Hilfethemen gruppiert – es bietet sich an, einmal in diesem Hilfefenster zu stöbern.\nAm häufigsten benötigt man aber Hilfe zu einem konkreten Befehl. Wenn man z.B. Informationen zur Funktion mean() braucht, gibt man in der Console folgendes ein:\nhelp(mean)\nAlternativ und kürzer geht das mit:\n?mean\nDer Hilfetext für eine Funktion enthält alle notwendigen Informationen – wenn Sie eine neue Funktion erstmalig verwenden möchten, ist ein Blick in die Hilfe sehr zu empfehlen.\nDie Hilfetexte sind alle sehr ähnlich aufgebaut. Nach einer kurzen Beschreibung (Description) sieht man unter Usage wie man die Funktion verwendet (aufruft). Hier ist in unserem Beispiel zu lesen:\nmean(x, ...)\nGleich danach folgen diese beiden Zeilen mit mehr Details:\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\nHier erkennt man, dass die Funktion ein Argument namens x erwartet. Dieses Argument ist verpflichtend, d.h. wenn man es nicht angibt, bekommt man eine Fehlermeldung (probieren Sie es in der Console aus, indem Sie mean() eintippen). Dies sieht man am Hilfetext, weil kein Standardwert für x ersichtlich ist. Im Gegensatz dazu sind die nächsten beiden Argumente trim und na.rm optional, d.h. man muss sie nicht angeben, denn in diesem Fall werden deren Standardwerte verwendet (trim hat den Wert 0 und na.rm hat den Wert FALSE).\nDie Bedeutung der Argumente wird im Abschnitt Arguments genau beschrieben. Der Wert, der von der Funktion berechnet und zurückgegeben wird, wird im Abschnitt Value beschrieben. Danach folgen Literaturhinweise, verwandte Funktionen und schließlich Beispiele. Diese Beispiele kann man auch kopieren und in der Console ausführen (oder einfach durch Klicken auf “Run examples” im Hilfe-Fenster ausführen).\n\n\nMöglichkeiten zum Aufrufen einer Funktion\nNachdem wir nun Funktionen sowie die integrierte Hilfe besprochen haben, sehen wir uns abschließend noch an, mit welchen unterschiedlichen Varianten wir eine Funktion aufrufen können.\nEiner Funktion kann man Argumente auf zwei verschiedene Arten übergeben:\n\nMan übergibt nur die Werte in der richtigen Reihenfolge (wie von der Funktion laut Hilfe erwartet), z.B. mean(1).\nMan verwendet die Namen der Argumente gemeinsam mit deren Werten, z.B. mean(x=1). Die Reihenfolge ist dann egal.\n\nSehen wir uns einige Beispiele für korrekte Aufrufe der Funktion mean() an. Vorausgesetzt ist hier, dass es eine Variable x gibt, von deren Werten wir den Mittelwert berechnen möchten (eine detaillierte Erklärung zur Funktion c() folgt in der nächsten Einheit).\nWir erstellen also zunächst die Variable (das Objekt) x:\n\nx = c(-14, 2, 3, 4, 5, 6, 7, 28, 99)\n\nJetzt sehen wir uns an, mit welchen unterschiedlichen Varianten wir die Funktion mean() aufrufen können (die Kommentare zu den Aufrufen geben jeweils die Werte aller Argumente an, auch und vor allem wenn diese nicht explizit übergeben wurden und dadurch den Standardwerten entsprechen):\n\nmean(x)  # x=x, trim=0, na.rm=FALSE\n\n[1] 15.55556\n\nmean(x, 0.1)  # x=x, trim=0.1, na.rm=FALSE\n\n[1] 15.55556\n\nmean(x, 0.2, TRUE)  # x=x, trim=0.2, na.rm=TRUE\n\n[1] 7.857143\n\nmean(x, na.rm=TRUE)  # x=x, trim=0, na.rm=TRUE\n\n[1] 15.55556\n\nmean(x, trim=0.2, na.rm=TRUE)  # x=x, trim=0.2, na.rm=TRUE\n\n[1] 7.857143\n\nmean(x=x, na.rm=TRUE, trim=0.3)  # x=x, trim=0.3, na.rm=TRUE\n\n[1] 5\n\nmean(x, 0.2, na.rm=TRUE)  # x=x, trim=0.2, na.rm=TRUE\n\n[1] 7.857143\n\n\nAnzumerken ist hier, dass der tatsächlich übergebene Wert nicht denselben Namen wie das Argument haben muss. Im obigen Beispiel ist dies zufälligerweise der Fall (sowohl der Funktionsparameter als auch der übergebene Wert heißen x), aber das ist nicht erforderlich. Im Grunde sind nur die übergebenen Werte relevant, ob diese Werte einen Namen haben oder nicht spielt keine Rolle. D.h. man könnte mean() auch wie folgt aufrufen:\n\nmean(c(-14, 2, 3, 4, 5, 6, 7, 28, 99))\n\n[1] 15.55556\n\nmean(x=c(-14, 2, 3, 4, 5, 6, 7, 28, 99))\n\n[1] 15.55556\n\ny = c(-14, 2, 3, 4, 5, 6, 7, 28, 99)\nmean(y)\n\n[1] 15.55556\n\nmean(x=y)\n\n[1] 15.55556"
  },
  {
    "objectID": "02/02.html#literatur",
    "href": "02/02.html#literatur",
    "title": "2 – Die R-Umgebung",
    "section": "Literatur",
    "text": "Literatur\n\nBücher\n\nDiscovering Statistics Using R\nOpenIntro Statistics\nLearning Statistics With R\nR for Data Science\nAdvanced R\n\n\n\nTutorials und Dokumentation\n\nlearnr\nRStudio Education\nAn Introduction to R\nQuick-R\nR Documentation\nRtips\nCookbook for R\n\n\n\nOnline-Kurse\n\nStatistics and R\nR Programming\nStatistics with R Specialization\nMastering Software Development in R\n\n\n\nArtikel\nFalls Sie sich für die historische Entwicklung von R interessieren, finden Sie hier einen spannenden Überblick von John M. Chambers, einem der Mitentwickler von R."
  },
  {
    "objectID": "02/02.html#übungen",
    "href": "02/02.html#übungen",
    "title": "2 – Die R-Umgebung",
    "section": "Übungen",
    "text": "Übungen\n\nÜbung 1\nInstallieren Sie die Pakete tidyverse, Hmisc und psych – welche R-Befehle verwenden Sie dafür? Nennen Sie die Versionsnummern dieser Pakete. Mit welchen Befehlen können Sie die installierten Pakete anschließend aktivieren?\n\n\nÜbung 2\nZeigen Sie die Hilfe zur Funktion help() an. Welche zwei Möglichkeiten haben Sie dafür?\n\n\nÜbung 3\nWie sieht ein Funktionsaufruf in R aus?\n\n\nÜbung 4\nErstellen Sie in RStudio ein einfaches Script mit dem Namen my_first_script.R. Fügen Sie folgende Elemente in dieses Script ein:\n\nEine Kommentarzeile mit dem Inhalt “Übung 4”\nAktivieren des Pakets Hmisc\nBerechnung des Mittelwerts der Zahlen 45, 66, 37, 54, 7 und 22 (nur mit Grundrechenarten)\n\nDas fertige Script sollte also aus drei Zeilen bestehen (Sie könnten zur Erhöhung der Übersichtlichkeit aber zusätzliche leere Zeilen einfügen).\nFühren Sie zur Kontrolle das gesamte Script aus – es darf dabei kein Fehler auftreten!\n\n\nÜbung 5\nLesen Sie die Hilfe zur Funktion sum() und beantworten Sie dann folgende Fragen:\n\nWie viele verpflichtende Argumente benötigt diese Funktion?\nWie viele optionale Argumente hat diese Funktion?\nWas ergibt der Aufruf sum() und warum (beachten Sie dazu auch das Ergebnis von mean())?\nErklären Sie, warum sum(1, 2, 3) das korrekte Ergebnis liefert, aber mean(1, 2, 3) nicht!\nWas passiert, wenn Sie nur sum in der Console ausführen (also ohne das Klammernpaar)?"
  },
  {
    "objectID": "09/09-solutions.html",
    "href": "09/09-solutions.html",
    "title": "9 – Lösungen",
    "section": "",
    "text": "Durch Probieren mit verschiedenen Werten für \\(N\\) erhält man \\(N = 62\\) für \\(p &lt; 0.05\\). Dies bedeutet, dass eine Korrelation von \\(r = 0.25\\) signfikant ist bei \\(N = 62\\) oder größer.\n\nr = 0.25\nN = 62\nalpha = 0.05\nz = atanh(r)\nse_z = 1 / sqrt(N - 3)\n(p = 2 * (1 - pnorm(z / se_z)))\n\n[1] 0.04977843\n\n\nWiederum durch Probieren erhält man \\(N = 1538\\) (mindestens) damit \\(p &lt; 0.05\\). Man sieht, dass selbst eine sehr kleine Korrelation ab einer gewissen Stichprobengröße signifikant wird.\n\nr = 0.05\nN = 1538\nalpha = 0.05\nz = atanh(r)\nse_z = 1 / sqrt(N - 3)\n(p = 2 * (1 - pnorm(z / se_z)))\n\n[1] 0.04992702"
  },
  {
    "objectID": "09/09-solutions.html#übung-1",
    "href": "09/09-solutions.html#übung-1",
    "title": "9 – Lösungen",
    "section": "",
    "text": "Durch Probieren mit verschiedenen Werten für \\(N\\) erhält man \\(N = 62\\) für \\(p &lt; 0.05\\). Dies bedeutet, dass eine Korrelation von \\(r = 0.25\\) signfikant ist bei \\(N = 62\\) oder größer.\n\nr = 0.25\nN = 62\nalpha = 0.05\nz = atanh(r)\nse_z = 1 / sqrt(N - 3)\n(p = 2 * (1 - pnorm(z / se_z)))\n\n[1] 0.04977843\n\n\nWiederum durch Probieren erhält man \\(N = 1538\\) (mindestens) damit \\(p &lt; 0.05\\). Man sieht, dass selbst eine sehr kleine Korrelation ab einer gewissen Stichprobengröße signifikant wird.\n\nr = 0.05\nN = 1538\nalpha = 0.05\nz = atanh(r)\nse_z = 1 / sqrt(N - 3)\n(p = 2 * (1 - pnorm(z / se_z)))\n\n[1] 0.04992702"
  },
  {
    "objectID": "09/09-solutions.html#übung-2",
    "href": "09/09-solutions.html#übung-2",
    "title": "9 – Lösungen",
    "section": "Übung 2",
    "text": "Übung 2\n\nx = c(8, 1, -4, 5, 6, 10, 9)\ny = c(-2, -5, -6, 0, 3, 7, 10)\ncor.test(x, y, conf.level=0.99)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 3.1632, df = 5, p-value = 0.02501\nalternative hypothesis: true correlation is not equal to 0\n99 percent confidence interval:\n -0.1405326  0.9847508\nsample estimates:\n      cor \n0.8165732 \n\n\nDie Pearson-Korrelation beträgt 0.8166, der zugehörige \\(p\\)-Wert ist 0.02501. Bei \\(\\alpha=0.99\\) ist dies nicht signifikant. Das 99%-Konfidenzintervall lautet \\((-0.1405326, 0.9847508)\\). Es beinhaltet den Wert 0, daher ist die Korrelation nicht signifikant."
  },
  {
    "objectID": "09/09-solutions.html#übung-3",
    "href": "09/09-solutions.html#übung-3",
    "title": "9 – Lösungen",
    "section": "Übung 3",
    "text": "Übung 3\n\ncor(mtcars[, c(\"mpg\", \"disp\", \"hp\")], method=\"pearson\")\n\n            mpg       disp         hp\nmpg   1.0000000 -0.8475514 -0.7761684\ndisp -0.8475514  1.0000000  0.7909486\nhp   -0.7761684  0.7909486  1.0000000\n\ncor(mtcars[, c(\"mpg\", \"disp\", \"hp\")], method=\"spearman\")\n\n            mpg       disp         hp\nmpg   1.0000000 -0.9088824 -0.8946646\ndisp -0.9088824  1.0000000  0.8510426\nhp   -0.8946646  0.8510426  1.0000000\n\n\n\npairs(subset(mtcars, select=c(mpg, disp, hp)))"
  },
  {
    "objectID": "09/09-solutions.html#übung-4",
    "href": "09/09-solutions.html#übung-4",
    "title": "9 – Lösungen",
    "section": "Übung 4",
    "text": "Übung 4\n\nlibrary(readr)\npm10 = read_csv(\"pm10.csv\")\npm10$Datum = as.Date(pm10$Datum, format=\"%d.%m.%y\")\n\n\nZeitlicher Verlauf von PM10 für beide Messorte\n\nplot(pm10$Datum, pm10$Petersgasse, type=\"l\", col=\"blue\", xlab=\"\", ylab=\"PM10\")\nlines(pm10$Datum, pm10$DonBosco, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nZusammenhang der beiden Messstationen\n\nwith(pm10, plot(Petersgasse, DonBosco, pch=16, col=rgb(0, 0, 0, 0.5)))\nabline(lm(pm10$DonBosco ~ pm10$Petersgasse), col=\"blue\")\n\n\n\n\n\n\n\n(r = cor.test(pm10$Petersgasse, pm10$DonBosco))\n\n\n    Pearson's product-moment correlation\n\ndata:  pm10$Petersgasse and pm10$DonBosco\nt = 37.46, df = 121, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9425599 0.9715014\nsample estimates:\n      cor \n0.9594887 \n\nr$estimate**2  # erklärte Varianz\n\n      cor \n0.9206186 \n\n# fehlende Werte (NA) müssen explizit ausgeschlossen werden\ncor(pm10$Petersgasse, pm10$DonBosco, use=\"complete.obs\")\n\n[1] 0.9594887"
  },
  {
    "objectID": "09/09-solutions.html#übung-5",
    "href": "09/09-solutions.html#übung-5",
    "title": "9 – Lösungen",
    "section": "Übung 5",
    "text": "Übung 5\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm, use=\"complete.obs\")\n\n[1] -0.2350529\n\nby(\n    penguins[, c(\"bill_length_mm\", \"bill_depth_mm\")],\n    penguins$species,\n    cor,\n    use=\"complete.obs\"\n)\n\npenguins$species: Adelie\n               bill_length_mm bill_depth_mm\nbill_length_mm      1.0000000     0.3914917\nbill_depth_mm       0.3914917     1.0000000\n------------------------------------------------------------------------------------------ \npenguins$species: Chinstrap\n               bill_length_mm bill_depth_mm\nbill_length_mm      1.0000000     0.6535362\nbill_depth_mm       0.6535362     1.0000000\n------------------------------------------------------------------------------------------ \npenguins$species: Gentoo\n               bill_length_mm bill_depth_mm\nbill_length_mm      1.0000000     0.6433839\nbill_depth_mm       0.6433839     1.0000000"
  }
]