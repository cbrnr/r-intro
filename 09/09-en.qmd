---
title: "9 – Correlation"
subtitle: "Statistical Data Analysis with R"
author: "Clemens Brunner"
date: 2024-12-05
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
---

## Introduction

It is often interesting to ask whether two variables show mutual dependence. In other words, do they behave similarly – when one quantity increases, does the other increase as well? Or is it the opposite, where one increases and the other decreases? Or is there no relationship at all? The correlation is a simple and popular measure to answer these questions.

However, it is important to note that [correlation does not imply causation]((https://en.wikipedia.org/wiki/Cum_hoc_ergo_propter_hoc)). The following examples of [spurious correlations](http://www.tylervigen.com/spurious-correlations) illustrate the danger of misinterpreting correlations. The first example shows the US spending on science, space, and technology from 1999 to 2009 (red line) alongside the number of suicides by hanging, strangulation, and suffocation (black line). Both curves exhibit a remarkably similar trend, and the correlation coefficient is extremely high at $r = 0.998$:

![](spurious1.png)

The second example shows the relationship between the number of people who drowned by falling into a pool and the number of movies featuring Nicolas Cage from 1999 to 2009. Again, a strong correlation with a correlation coefficient of $r = 0.666$ can be observed:

![](spurious2.png)


## Pearson correlation (product-moment correlation)

The product-moment correlation $r$ (also called Pearson correlation) is a measure of the degree of *linear* relationship between two interval-scaled variables. The correlation takes values between −1 and 1 and is derived from the variances and the covariance of both variables.

The variance of a variable $x$ is defined as follows:

$$\mathrm{Var}(x) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})^2$$

Here, $\bar{x}$ is the mean over all $N$ values (denoted as $x_i$), i.e.,

$$\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i.$$

Alternatively, we can write the variance by explicitly multiplying the square:

$$\mathrm{Var}(x) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})(x_i - \bar{x})$$

The variance describes how much the data points vary around their mean. Accordingly, the covariance between two variables $x$ and $y$ is defined as

$$\mathrm{Cov}(x, y) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})(y_i - \bar{y}).$$

The covariance describes how strongly the two variables vary together around their respective means. A positive covariance means that both variables vary in the same direction (i.e., if one variable is larger than its mean, the other is also larger). Conversely, a negative covariance means that both variables vary in opposite directions (if one variable is larger than its mean, the other variable is smaller).

The covariance is not a standardized measure, i.e., we cannot simply compare two covariances from different data sets, as it depends on the scaling of the two variables. The Pearson correlation standardizes the covariance with the variances of the individual variables, ensuring that the correlation lies in the range between −1 and 1:

$$r = \frac{\mathrm{Cov}(x, y)}{\sqrt{\mathrm{Var}(x) \mathrm{Var}(y)}}$$


### Example

Let's calculate the correlation for two example vectors $x$ and $y$:

$$
\begin{aligned}
x &= (8, 1, -4, 5, 6, 10, 9) \\
y &= (-2, -5, -6, 0, 3, 7, 10)
\end{aligned}
$$

We can visualize these seven data points in a scatter plot:

```{r}
#| echo: false
x = c(8, 1, -4, 5, 6, 10, 9)
y = c(-2, -5, -6, 0, 3, 7, 10)
library(ggplot2)
df = data.frame(x, y)
ggplot(df, aes(x, y)) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    geom_point(size=6) +
    geom_text(aes(label=rownames(df)), color="white") +
    theme_minimal() +
    scale_x_continuous(breaks=-4:10) +
    scale_y_continuous(breaks=-6:10)
```

First, we calculate the two means:

$$
\begin{aligned}
\bar{x} &= \frac{1}{7} \cdot (8 + 1 - 4 + 5 + 6 + 10 + 9) = 5 \\
\bar{y} &= \frac{1}{7} \cdot (-2 - 5 - 6 + 0 + 3 + 7 + 10) = 1
\end{aligned}
$$

Next, we can calculate the variances:

$$
\begin{aligned}
\text{Var}(x) = \frac{1}{6} \cdot \Bigl[ & (8 - 5)^2 + (1 - 5)^2 + (-4 - 5)^2 \\
 &+ (5 - 5)^2 + (6 - 5)^2 + (10 - 5)^2 + (9 - 5)^2 \Bigr] = 24.\dot{6} \\
\text{Var}(y) = \frac{1}{6} \cdot \Bigl[ & (-2 - 1)^2 + (-5 - 1)^2 + (-6 - 1)^2 \\
 &+ (0 - 1)^2 + (3 - 1)^2 + (7 - 1)^2 + (10 - 1)^2 \Bigr] = 36
\end{aligned}
$$

Finally, we calculate the covariance:

$$
\begin{aligned}
\text{Cov}(x, y) = \frac{1}{6} \cdot \Bigl[ & (8 - 5) \cdot (-2 -1) + (1 - 5) \cdot (-5 - 1) \\
 &+ (-4 - 5) \cdot (-6 - 1) + (5 - 5) \cdot (0 - 1) \\
 &+ (6 - 5) \cdot (3 - 1) + (10 - 5) \cdot (7 - 1) + (9 - 5) \cdot (10 - 1) \Bigr] = 24.\dot{3}
\end{aligned}
$$

To obtain the Pearson correlation, we need to process these values according to the formula:

$$r = \frac{24.\dot{3}}{\sqrt{24.\dot{6} \cdot 36}} = 0.8165732$$


### Significance testing

Usually, after calculating the correlation, a test is performed to check whether the obtained correlation significantly deviates from the null hypothesis ("there is *no* correlation, i.e., the correlation is 0"). Since the sampling distribution of the correlation is not normally distributed, the value of $r$ needs to be transformed into a normal distribution using the [Fisher transformation](https://en.wikipedia.org/wiki/Fisher_transformation). The transformed value $z_r$ is calculated as follows:

$$z_r = \frac{1}{2} \ln \frac{1 + r}{1 - r} = \text{arctanh}(r)$$

umwandeln. Der Standardfehler von $z_r$ beträgt dabei:

$$\mathrm{SE}(z_r) = \frac{1}{\sqrt{N - 3}}$$

Nun kann man den erhaltenen Wert von $z_r$ durch den Standardfehler dividieren und das Ergebnis dann mit Werten aus einer Tabelle der Standardnormalverteilung vergleichen. So erhält man dann den $p$-Wert (siehe auch [hier](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Using_the_Fisher_transformation)).

:::{.callout-note}
Der $p$-Wert gibt hier die Wahrscheinlichkeit an, dass man eine Korrelation erhält, die *mindestens* so groß ist wie die beobachtete, unter der Annahme, dass die Nullhypothese zutrifft (also dass die Korrelation 0 ist).

Ein kleiner $p$-Wert bedeutet also, dass der beobachtete Wert unter der Nullhypothese sehr unwahrscheinlich ist. Daher entscheidet man sich in solchen Situationen, die Nullhypothese zu verwerfen und die Korrelation als signifikant (unterschiedlich von 0) zu betrachten.
:::

Betrachten wir dazu ein Beispiel zur Veranschaulichung der Berechnung des $p$-Wertes sowie des Konfidenzintervalls für eine gegebene Pearson-Korrelation.


### Beispiel

Gegeben sei eine Korrelation $r = 0.25$ berechnet aus einer Stichprobe der Größe $N = 40$:

```{r}
r = 0.25
N = 40
```

Weiters geben wir ein Signifikanzniveau von $\alpha = 0.05$ vor (das ist der Schwellwert für den $p$-Wert, ab dem wir eine Korrelation als signifikant betrachten wollen):

```{r}
alpha = 0.05
```

Wir möchten nun wissen, ob die Korrelation $r = 0.25$ bei einem gegebenen Signifikanzniveau signifikant unterschiedlich von 0 ist. Dazu berechnen wir den $p$-Wert und das Konfidenzintervall. Um dies berechnen zu können, müssen wir zuerst die Fisher-Transformation von $r$ berechnen:

```{r}
(z = atanh(r))
```

Jetzt können wir den Standardfehler berechnen:
```{r}
(se_z = 1 / sqrt(N - 3))
```

Den $p$-Wert erhält man nun, indem man die Wahrscheinlichkeit berechnet, dass man in einer Standardnormalverteilung Werte größer als $z \, / \, \text{SE}(z) \approx 1.55$ erhält. Dazu sieht man entweder in einer [Tabelle der Standardnormalverteilung](https://en.wikipedia.org/wiki/Standard_normal_table#Complementary_cumulative) nach oder man berechnet diesen Wert mit entsprechenden Funktionen in R (der Faktor 2 wird benötigt, da wir keine gerichtete Hypothese haben):

```{r}
(p = 2 * (1 - pnorm(z / se_z)))
```

Dieser $p$-Wert ist größer als $\alpha = 0.05$, d.h. wir können die Nullhypothese (keine Korrelation) *nicht* verwerfen.

Das Konfidenzintervall um $z$ erhält man, indem man zum gegebenen Wert $z$ das Produkt aus dem Signifikanzniveau entsprechenden Quantil (ca. 1.96 für $\alpha = 0.05$) mit dem Standardfehler addiert bzw. subtrahiert:

```{r}
cl_z = z - qnorm(1 - alpha/2) * se_z
cu_z = z + qnorm(1 - alpha/2) * se_z
c(cl_z, z, cu_z)
```

Man beachte, dass es sich bei allen drei Werten um Fisher-transformierte Werte handelt. Möchte man ein Konfidenzintervall um die ursprüngliche Korrelation $r$ angeben, so muss man diese drei Werte noch rücktransformieren (`tanh` ist die Umkehrfunktion von `atanh`):

```{r}
cl_r = tanh(cl_z)
cu_r = tanh(cu_z)
c(cl_r, r, cu_r)
```

:::{.callout-note}
Das Konfidenzintervall gibt an, in welchem Bereich der wahre Wert der Korrelation mit einer gegebenen Sicherheit (Konfidenz) liegt. Die Konfidenz ist *nicht* die *Wahrscheinlichkeit*, dass der wahre Wert in dem berechneten Intervall liegt – denn der wahre Wert ist entweder in dem Intervall oder nicht. Wenn man viele Stichproben ziehen würde und damit jedes Mal ein Konfidenzintervall berechnen würde, dann würde im Falle eines 95%-Konfidenzintervalls in 95% der berechneten Fälle der wahre Wert in dem Intervall liegen.
:::

:::{.callout-tip}
Die Tatsache, dass die Korrelation $r=0.25$ bei einer Stichprobe von $N=40$ nicht signifikant unterschiedlich von Null ist ($p=0.12$) kann man auch daran erkennen, dass das 95%-Konfidenzintervall den Wert 0 (also die Nullhypothese) enthält. Das bedeutet also, dass wir die Nullhypothese (keine Korrelation) *nicht* verwerfen können.
:::


## Nicht-parametrische Korrelationskoeffizienten

### Spearman Rangkorrelationskoeffizient

Im Gegensatz zur Pearson-Korrelation misst der Spearman Rangkorrelationskoeffizient $\rho$ nicht nur lineare Zusammenhänge zwischen zwei Variablen, sondern der Zusammenhang kann eine beliebige monotone (nicht-lineare) Funktion sein. Die beiden Variablen müssen auch nicht intervallskaliert sein, d.h. so kann man auch ordinalskalierte Daten miteinander korrelieren.

Im Prinzip berechnet man die Spearman-Korrelation, indem man die Daten $x$ und $y$ vorher in Ränge konvertiert und dann die Pearson-Korrelation berechnet. Zur Berechnung kann folgende vereinfachte Formel verwendet werden:

$$\rho = 1 - \frac{6 \sum d_i^2}{N \cdot (N^2 - 1)}$$
Hier ist $d_i$ die Differenz der Ränge einer Beobachtung.


### Kendall Rangkorrelationskoeffizient

Bei kleinen Stichproben und relativ großer Anzahl an gleichen Rängen liefert oft der Kendall Rangkorrelationskoeffizient $\tau$ bessere Ergebnisse. Hier werden nicht die Differenzen zwischen den Rängen betrachtet (also die Abstände der Ränge zwischen beiden Variablen), sondern ob es Unterschiede in den Rängen zwischen Datenpaaren gibt oder nicht.


## Korrelationen mit R berechnen

Korrelationskoeffizienten kann man mit den folgenden drei Funktionen berechnen: `cor()`, `cor.test()` und `rcorr()`. Die ersten beiden Funktionen sind Teil von R, die Funktion `rcorr()` muss mit dem `Hmisc`-Paket geladen werden.

```{r}
#| message: false
library(Hmisc)
```

Die drei Funktionen haben unterschiedliche Features, d.h. welche Funktion man benutzt ist abhängig von den benötigten Eigenschaften, welche in nachfolgender Tabelle zusammengefasst sind ("CI" steht für Konfidenzintervall, "Mehrfach" bedeutet, dass Korrelationen zwischen mehr als zwei Variablen paarweise berechnet werden können).

|            | Pearson | Spearman | Kendall | $p$-Werte | CI | Mehrfach |
|:-----------|:-------:|:--------:|:-------:|:---------:|:--:|:--------:|
| `cor`      | x       | x        | x       |           |    | x        |
| `cor.test` | x       | x        | x       | x         | x  |          |
| `rcorr`    | x       | x        |         | x         |    | x        |


### Funktion `cor()`

Die Funktion `cor()` ruft man wie folgt auf:

```r
cor(x, y, method="pearson")
```

Hier übergibt man zwei Vektoren und spezifiziert, welche Korrelation berechnet werden soll (standardmäßig wird die Pearson-Korrelation berechnet). Wenn `x` ein Data Frame mit mindestens zwei Spalten ist, kann man `y` weglassen – dann werden automatisch die Korrelationen zwischen allen Spaltenpaaren berechnet.


## Funktion `cor.test()`

Der Aufruf der Funktion `cor.test()` ist sehr ähnlich:

```r
cor.test(x, y, alternative="t", method="pearson", conf.level=0.95)
```

Hier kann man die Form der Alternativhypothese (`"two-sided"`, `"greater"`, `"less"`) sowie das Konfidenzniveau angeben. Diese Funktion kann nur mit genau zwei Vektoren umgehen.


## Funktion `rcorr()`

Die Funktion `rcorr()` verwendet man wie folgt:

```r
rcorr(x, y, type="pearson")
```

Wie bei den anderen Funktionen sind `x` und `y` Vektoren. Weiters ist es wie bei `cor()` möglich, nur das Argument `x` anzugeben, wenn dieses eine Matrix mit mindestens zwei Spalten ist.


## Beispiel

Am besten können die drei Funktionen anhand eines Beispiels veranschaulicht werden. Dazu laden wir einen (fiktiven) Datensatz [`exam.dat`](exam.dat) über Prüfungsangst:

```{r}
#| message: false
library(readr)
(exam = read_tsv("exam.dat"))
```

Es ist hilfreich, die Daten zuerst einmal grafisch darzustellen. Für den Zusammenhang zwischen zwei Variablen bietet sich ein Scatterplot (inklusive Regressionsgeraden) an – so kann man die Korrelation direkt visualisieren. Wir können eine solche Grafik zunächst einmal separat für alle drei Kombinationen erzeugen:

```{r}
plot(exam$Revise, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Exam ~ exam$Revise), col="blue")
plot(exam$Anxiety, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Exam ~ exam$Anxiety), col="blue")
plot(exam$Revise, exam$Anxiety, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Anxiety ~ exam$Revise), col="blue")
```

:::{.callout-tip}
Bei mehreren Variablen (wie in diesem Beispiel) kann man die Funktion `pairs()` verwenden, um alle paarweisen Scatterplots gleichzeitig darzustellen:

```{r}
pairs(exam[, 2:4])
```

Möchte man auch Regressionsgeraden einzeichnen, kann man eine (anonyme) Funktion als `panel`-Argument übergeben:

```{r}
pairs(
    exam[2:4],
    panel=function(x, y) {
        points(x, y, pch=16, col=rgb(0, 0, 0, 0.5))
        abline(lm(y ~ x), col="blue")
    }
)
```
:::


### Pearson-Korrelation

Nun berechnen wir die Pearson-Korrelationen zwischen den drei Variablen `Exam`, `Anxiety` und `Revise`:

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")])
```

Man kann aus dieser Korrelationsmatrix direkt die einzelnen Koeffizienten für alle Variablenpaare ablesen. Die Diagonale beinhaltet die Korrelationen der Variablen mit sich selbst und besteht daher aus lauter Werten, die exakt gleich 1 sind. Außerdem ist es egal, ob man die Korrelationen in dem Dreieck *unter* der Diagonale oder *über* der Diagonale abliest, da die Korrelationsmatrix symmetrisch ist (die Korrelation zwischen "Exam" und "Revise" ist dieselbe wie zwischen "Revise" und "Exam" – die Korrelation misst ja keine kausalen Zusammenhänge).

Möchte man jedoch auch $p$-Werte, muss man die Funktion `rcorr()` verwenden. Diese Funktion erwartet die Daten jedoch nicht als Data Frame, sondern als Matrix. Daher müssen die Daten beim Aufruf der Funktion in eine Matrix umgewandelt werden:

```{r}
rcorr(as.matrix(exam[, c("Exam", "Anxiety", "Revise")]))
```

Zusätzlich zur Korrelationsmatrix bekommt man auch die $p$-Werte geliefert. In diesem Beispiel sind alle Korrelationen signifikant, da die $p$-Werte sehr klein sind (gerundet Null).

Wenn man auch Konfidenzintervalle benötigt, muss man die Funktion `cor.test()` verwenden. Diese Funktion unterstützt aber nur zwei Variablen, d.h. bei mehreren Variablen muss man die Funktion mehrmals aufrufen, um alle paarweisen Korrelationen zu erhalten.

```{r}
cor.test(exam$Anxiety, exam$Exam)
```


### Bestimmtheitsmaß $R^2$

Wenn man den Korrelationskoeffizienten $r$ quadriert, erhält man das Bestimmtheitsmaß $R^2$. Es gibt an, wie viel Varianz in einer Variablen von der zweiten erklärt werden kann (wobei auch hier wieder gilt, dass dies keine Aussage über Kausalität ist). Es ist üblich, die Korrelation mit dem Kleinbuchstaben $r$ zu bezeichnen, für das Bestimmtheitsmaß verwendet man aber einen Großbuchstaben $R^2$.

$$R^2 = r^2$$

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")])^2
```


### Spearman Rangkorrelationskoeffizient

Für die Beispieldaten können wir analog auch die Spearman-Korrelation bestimmen:

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")], method="spearman")
rcorr(as.matrix(exam[, c("Exam", "Anxiety", "Revise")]), type="spearman")
cor.test(exam$Revise, exam$Exam, method="spearman")
```


### Kendall Rangkorrelationskoeffizient

Die Funktion `cor.test()` gibt im vorigen Beispiel eine Warnung aus, dass der berechnete $p$-Wert nicht exakt ist, da die Daten gleiche Ränge beinhalten. In solchen Fällen ist daher der Kendall-Korrelationskoeffizient die bessere Wahl:

```{r}
cor.test(exam$Revise, exam$Exam, method="kendall")
```


## Übungen

### Übung 1

Im theoretischen Teil der Übung haben wir in einem Beispiel die Signifikanz einer Korrelation $r = 0.25$ bei einer Stichprobengröße von $N = 40$ berechnet. Diese war mit $p = 0.12$ nicht signifikant.

1. Welche Stichprobengröße $N$ müssten Sie mindestens wählen, um ein mit $\alpha = 0.05$ signifikantes Ergebnis zu erhalten?
2. Ab welcher Stichprobengröße wird sogar eine sehr kleine Korrelation von $r = 0.05$ signifikant?

Sie können dieses Beispiel durch Probieren lösen, indem Sie die Stichprobengröße schrittweise verändern (vergrößern) und sich den zugehörigen $p$-Wert ausrechnen.


### Übung 2

In dieser Einheit haben wir die Korrelation zwischen $x = (8, 1, -4, 5, 6, 10, 9)^T$ und $y = (-2, -5, -6, 0, 3, 7, 10)^T$ händisch berechnet. Berechnen Sie nun mit Hilfe von R die Pearson-Korrelation zwischen $x$ und $y$. Geben Sie außerdem das 99%-Konfidenzintervall sowie den $p$-Wert an! Ist diese Korrelation signifikant bei einem Signifikanzniveau von $\alpha = 0.01$?


### Übung 3

Verwenden Sie die Beispieldaten `mtcars` und analysieren Sie den Zusammenhang zwischen den Variablen `mpg`, `disp` und `hp`. Stellen Sie den Zusammenhang zwischen den Variablenpaaren grafisch dar, und berechnen Sie danach Pearson- und Spearman-Korrelationen!


### Übung 4

In der Datei [`pm10.csv`](pm10.csv) finden Sie die monatlichen Feinstaubwerte PM10 von zwei Messstationen in Graz im Zeitraum Februar 2006 bis Mai 2016. Führen Sie folgende Analysen durch:

- Erstellen Sie eine Grafik, in der Sie den Verlauf der PM10-Konzentration von beiden Messstationen über die Zeit darstellen.
- Erstellen Sie eine Grafik, in der Sie den Zusammenhang zwischen den beiden Messstationen darstellen.

Berechnen Sie abschließend die Pearson-Korrelation zwischen den Daten beider Messstationen inklusive Konfidenzinterval sowie $p$-Wert. Wie viel Varianz der einen Variable kann durch die andere Variable erklärt werden?

:::{.callout-note}
Wandeln Sie die Spalte `Datum` nach dem Einlesen in einen Datums-Typ um. Für die gleichzeitige Darstellung der beiden Zeitverläufe können Sie zuerst eine Liniengrafik einer Variable erzeugen und dann mit der Funktion `lines()` den zweiten Zeitverlauf mit einer anderen Farbe hinzufügen.

Wenn Sie die Funktion `cor()` zur Berechnung der Korrelation zwischen den beiden Variablen verwenden möchten, können Sie fehlende Werte in jeder der beiden Variablen mit dem Argument `use="complete.obs"` ausschließen.
:::


### Übung 5

Berechnen Sie die Korrelation der Schnabellänge (`bill_length_mm`) mit der Schnabeltiefe (`bill_depth_mm`) im Datensatz `penguins` (aus dem Paket `palmerpenguins`). Berechnen Sie außerdem diese Korrelation für jede Spezies separat! Was stellen Sie fest?
