---
title: "9 – Correlation"
subtitle: "Statistical Data Analysis with R"
author: "Clemens Brunner"
date: 2025-05-12
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
---

## Introduction

It is often interesting to ask whether two variables show mutual dependence. In other words, do they behave similarly – when one quantity increases, does the other increase as well? Or is it the opposite, where one increases and the other decreases? Or is there no relationship at all? The correlation is a simple and popular measure to answer these questions.

However, it is important to note that [correlation does not imply causation]((https://en.wikipedia.org/wiki/Cum_hoc_ergo_propter_hoc)). The following examples of [spurious correlations](http://www.tylervigen.com/spurious-correlations) illustrate the danger of misinterpreting correlations. The first example shows the US spending on science, space, and technology from 1999 to 2009 (red line) alongside the number of suicides by hanging, strangulation, and suffocation (black line). Both curves exhibit a remarkably similar trend, and the correlation coefficient is extremely high at $r = 0.998$:

![](spurious1.png)

The second example shows the relationship between the number of people who drowned by falling into a pool and the number of movies featuring Nicolas Cage from 1999 to 2009. Again, a strong correlation with a correlation coefficient of $r = 0.666$ can be observed:

![](spurious2.png)


## Pearson correlation (product-moment correlation)

The product-moment correlation $r$ (also called Pearson correlation) is a measure of the degree of *linear* relationship between two interval-scaled variables. The correlation takes values between −1 and 1 and is derived from the variances and the covariance of both variables.

The variance of a variable $x$ is defined as follows:

$$\mathrm{Var}(x) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})^2$$

Here, $\bar{x}$ is the mean over all $N$ values (denoted as $x_i$), i.e.,

$$\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i.$$

Alternatively, we can write the variance by explicitly multiplying the square:

$$\mathrm{Var}(x) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})(x_i - \bar{x})$$

The variance describes how much the data points vary around their mean. Accordingly, the covariance between two variables $x$ and $y$ is defined as

$$\mathrm{Cov}(x, y) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \bar{x})(y_i - \bar{y}).$$

The covariance describes how strongly the two variables vary together around their respective means. A positive covariance means that both variables vary in the same direction (i.e., if one variable is larger than its mean, the other is also larger). Conversely, a negative covariance means that both variables vary in opposite directions (if one variable is larger than its mean, the other variable is smaller).

The covariance is not a standardized measure, i.e., we cannot simply compare two covariances from different data sets, as it depends on the scaling of the two variables. The Pearson correlation standardizes the covariance with the variances of the individual variables, ensuring that the correlation lies in the range between −1 and 1:

$$r = \frac{\mathrm{Cov}(x, y)}{\sqrt{\mathrm{Var}(x) \mathrm{Var}(y)}}$$


### Example

Let's calculate the correlation for two example vectors $x$ and $y$:

$$
\begin{aligned}
x &= (8, 1, -4, 5, 6, 10, 9) \\
y &= (-2, -5, -6, 0, 3, 7, 10)
\end{aligned}
$$

We can visualize these seven data points in a scatter plot:

```{r}
#| echo: false
x = c(8, 1, -4, 5, 6, 10, 9)
y = c(-2, -5, -6, 0, 3, 7, 10)
library(ggplot2)
df = data.frame(x, y)
ggplot(df, aes(x, y)) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    geom_point(size=6) +
    geom_text(aes(label=rownames(df)), color="white") +
    theme_minimal() +
    scale_x_continuous(breaks=-4:10) +
    scale_y_continuous(breaks=-6:10)
```

First, we calculate the two means:

$$
\begin{aligned}
\bar{x} &= \frac{1}{7} \cdot (8 + 1 - 4 + 5 + 6 + 10 + 9) = 5 \\
\bar{y} &= \frac{1}{7} \cdot (-2 - 5 - 6 + 0 + 3 + 7 + 10) = 1
\end{aligned}
$$

Next, we can calculate the variances:

$$
\begin{aligned}
\text{Var}(x) = \frac{1}{6} \cdot \Bigl[ & (8 - 5)^2 + (1 - 5)^2 + (-4 - 5)^2 \\
 &+ (5 - 5)^2 + (6 - 5)^2 + (10 - 5)^2 + (9 - 5)^2 \Bigr] = 24.\dot{6} \\
\text{Var}(y) = \frac{1}{6} \cdot \Bigl[ & (-2 - 1)^2 + (-5 - 1)^2 + (-6 - 1)^2 \\
 &+ (0 - 1)^2 + (3 - 1)^2 + (7 - 1)^2 + (10 - 1)^2 \Bigr] = 36
\end{aligned}
$$

Finally, we calculate the covariance:

$$
\begin{aligned}
\text{Cov}(x, y) = \frac{1}{6} \cdot \Bigl[ & (8 - 5) \cdot (-2 -1) + (1 - 5) \cdot (-5 - 1) \\
 &+ (-4 - 5) \cdot (-6 - 1) + (5 - 5) \cdot (0 - 1) \\
 &+ (6 - 5) \cdot (3 - 1) + (10 - 5) \cdot (7 - 1) + (9 - 5) \cdot (10 - 1) \Bigr] = 24.\dot{3}
\end{aligned}
$$

To obtain the Pearson correlation, we need to process these values according to the formula:

$$r = \frac{24.\dot{3}}{\sqrt{24.\dot{6} \cdot 36}} = 0.8165732$$


### Significance testing

After calculating the correlation, a test can be performed to check whether the obtained correlation significantly deviates from the null hypothesis ("there is *no* correlation, i.e., the correlation is 0"). Since the sampling distribution of the correlation is not normally distributed, the value of $r$ needs to be transformed into a normal distribution using the [Fisher transformation](https://en.wikipedia.org/wiki/Fisher_transformation). The transformed value $z_r$ is calculated as follows:

$$z_r = \frac{1}{2} \ln \frac{1 + r}{1 - r} = \text{arctanh}(r)$$

The standard error of $z_r$ is given by:

$$\mathrm{SE}(z_r) = \frac{1}{\sqrt{N - 3}}$$

We can then divide $z_r$ by the standard error and compare the result with values from a table of the standard normal distribution to obtain the *p*-value (see also [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Using_the_Fisher_transformation)).

::: {.callout-important}
The *p*-value is **not** the probability that the null hypothesis is true! Instead, it corresponds to the probability of obtaining a correlation that is *at least* as extreme as the observed one, given that the null hypothesis is true (i.e., that the correlation is 0).

A small *p*-value suggests that the observed correlation is unlikely if the null hypothesis is true. In such cases, we reject the null hypothesis and conclude that the correlation is statistically significant (different from 0).
:::

Let's consider an example to illustrate the calculation of the *p*-value and the confidence interval for a given Pearson correlation.


### Example

Given a correlation $r = 0.25$ calculated based on a sample size of $N = 40$:

```{r}
r = 0.25
N = 40
```

We also specify a significance level of $\alpha = 0.05$ (this is the threshold for the *p*-value below which we consider a correlation significant):

```{r}
alpha = 0.05
```

We would like to know whether the correlation $r = 0.25$ is significantly different from 0 at a given significance level. To do this, we calculate the *p*-value and the confidence interval. First, we need to calculate the Fisher transformation of $r$:

```{r}
(z = atanh(r))
```

Now we can calculate the standard error:

```{r}
(se_z = 1 / sqrt(N - 3))
```
The *p*-value is the probability of obtaining values greater than $z / \text{SE}(z) \approx 1.55$ in a standard normal distribution. To calculate this, we can look up the value 1.55 in a [table of the standard normal distribution](https://en.wikipedia.org/wiki/Standard_normal_table#Complementary_cumulative) (the value we are looking for is located in row "z = 1.5" and column "+0.05"):

| z | +0.00 | +0.01 | +0.02 | +0.03 | +0.04 | +0.05 | +0.06 | +0.07 | +0.08 | +0.09 |
|--:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|
| 0.0 | 0.50000 | 0.49601 | 0.49202 | 0.48803 | 0.48405 | 0.48006 | 0.47608 | 0.47210 | 0.46812 | 0.46414 |
| 0.1 | 0.46017 | 0.45620 | 0.45224 | 0.44828 | 0.44433 | 0.44038 | 0.43640 | 0.43251 | 0.42858 | 0.42465 |
| 0.2 | 0.42074 | 0.41683 | 0.41294 | 0.40905 | 0.40517 | 0.40129 | 0.39743 | 0.39358 | 0.38974 | 0.38591 |
| 0.3 | 0.38209 | 0.37828 | 0.37448 | 0.37070 | 0.36693 | 0.36317 | 0.35942 | 0.35569 | 0.35197 | 0.34827 |
| 0.4 | 0.34458 | 0.34090 | 0.33724 | 0.33360 | 0.32997 | 0.32636 | 0.32276 | 0.31918 | 0.31561 | 0.31207 |
| 0.5 | 0.30854 | 0.30503 | 0.30153 | 0.29806 | 0.29460 | 0.29116 | 0.28774 | 0.28434 | 0.28096 | 0.27760 |
| 0.6 | 0.27425 | 0.27093 | 0.26763 | 0.26435 | 0.26109 | 0.25785 | 0.25463 | 0.25143 | 0.24825 | 0.24510 |
| 0.7 | 0.24196 | 0.23885 | 0.23576 | 0.23277 | 0.22965 | 0.22663 | 0.22363 | 0.22065 | 0.21770 | 0.21476 |
| 0.8 | 0.21186 | 0.20897 | 0.20611 | 0.20327 | 0.20045 | 0.19766 | 0.19489 | 0.19215 | 0.18943 | 0.18673 |
| 0.9 | 0.18406 | 0.18141 | 0.17879 | 0.17619 | 0.17361 | 0.17106 | 0.16853 | 0.16602 | 0.16354 | 0.16109 |
| 1.0 | 0.15866 | 0.15625 | 0.15386 | 0.15151 | 0.14917 | 0.14686 | 0.14457 | 0.14231 | 0.14007 | 0.13786 |
| 1.1 | 0.13567 | 0.13350 | 0.13136 | 0.12924 | 0.12714 | 0.12507 | 0.12302 | 0.12100 | 0.11900 | 0.11702 |
| 1.2 | 0.11507 | 0.11314 | 0.11123 | 0.10935 | 0.10749 | 0.10565 | 0.10383 | 0.10204 | 0.10027 | 0.09853 |
| 1.3 | 0.09680 | 0.09510 | 0.09342 | 0.09176 | 0.09012 | 0.08851 | 0.08692 | 0.08534 | 0.08379 | 0.08226 |
| 1.4 | 0.08076 | 0.07927 | 0.07778 | 0.07636 | 0.07493 | 0.07353 | 0.07215 | 0.07078 | 0.06944 | 0.06811 |
| 1.5 | 0.06681 | 0.06552 | 0.06426 | 0.06301 | 0.06178 | 0.06057 | 0.05938 | 0.05821 | 0.05705 | 0.05592 |
| 1.6 | 0.05480 | 0.05370 | 0.05262 | 0.05155 | 0.05050 | 0.04947 | 0.04846 | 0.04746 | 0.04648 | 0.04552 |
| 1.7 | 0.04457 | 0.04363 | 0.04272 | 0.04182 | 0.04093 | 0.04006 | 0.03920 | 0.03836 | 0.03754 | 0.03673 |

: {.striped .hover}

Alternatively (and even better), we can use appropriate functions in R (note that we need to multiply the obtained probability by 2, because we do not have a directional hypothesis):

```{r}
(p = 2 * (1 - pnorm(z / se_z)))
```

This *p*-value is greater than $\alpha = 0.05$, so we can *not* reject the null hypothesis (no correlation).

We can compute the confidence interval around $z$ by adding and subtracting the product of the quantile corresponding to the significance level (approximately 1.96 for $\alpha = 0.05$) and the standard error from the given value $z$:

```{r}
cl_z = z - qnorm(1 - alpha/2) * se_z
cu_z = z + qnorm(1 - alpha/2) * se_z
c(cl_z, z, cu_z)
```

Here, we use `alpha/2` because we are interested in a two-tailed test (we do not have a directional hypothesis). Note that all three values are Fisher-transformed values. If we want to provide a confidence interval around the original correlation $r$, we need to transform these three values back (the inverse function of `atanh` is `tanh`):

```{r}
cl_r = tanh(cl_z)
cu_r = tanh(cu_z)
c(cl_r, r, cu_r)
```

::: {.callout-important}
A confidence interval indicates the range in which the true value of the correlation is expected to lie, given a specified level of confidence (e.g., 95%). This confidence level is **not** the probability that the true value lies within the particular interval calculated from a single sample. Instead, if many samples were drawn and a confidence interval was calculated each time, then for a 95% confidence interval, the true value would lie within approximately 95% of these intervals.
:::

::: {.callout-tip}
The correlation of $r = 0.25$ for a sample size of $N = 40$ is not significantly different from zero ($p = 0.12$). This is also evident from the 95% confidence interval, which includes 0 (the value under the null hypothesis of no correlation). Therefore, we *cannot* reject the null hypothesis, indicating that the data do not provide strong evidence of a nonzero correlation.
:::


## Nonparametric correlation coefficients

### Spearman rank correlation coefficient

Unlike the Pearson correlation, the Spearman rank correlation coefficient $\rho$ can capture any monotonic relationship between two variables, not just linear ones. The two variables do not need to be interval-scaled; ordinal data can also be used with Spearman correlation.

In principle, the Spearman correlation entails converting the data $x$ and $y$ into ranks and then calculating the Pearson correlation. The following simplified formula can be used to calculate the Spearman correlation:

$$\rho = 1 - \frac{6 \sum d_i^2}{N \cdot (N^2 - 1)}$$

Here, $d_i$ is the difference in ranks of an observation.


### Kendall rank correlation coefficient

For small samples and a relatively large number of equal ranks, the Kendall rank correlation coefficient $\tau$ often provides better results. Here, the Kendall correlation coefficient does not consider the differences between the ranks (i.e., the distances between the ranks of both variables), but rather whether there are differences in the ranks between data pairs or not.


## Calculating correlations with R

The following three functions can be used to calculate correlation coefficients: `cor()`, `cor.test()`, and `rcorr()`. The first two functions are part of R, while the function `rcorr()` is available in the `Hmisc` package:

```{r}
#| message: false
library(Hmisc)
```

These three functions have different features, i.e., which function to use depends on the required properties, which are summarized in the following table ("CI" stands for confidence interval):

|            | Pearson | Spearman | Kendall | *p*-values | CI | pairwise |
|:-----------|:-------:|:--------:|:-------:|:----------:|:--:|:--------:|
| `cor`      | x       | x        | x       |            |    | x        |
| `cor.test` | x       | x        | x       | x          | x  |          |
| `rcorr`    | x       | x        |         | x          |    | x        |


### The `cor()` function

We call the `cor()` function as follows:

```r
cor(x, y, method="pearson")
```

Here, we pass two vectors and specify which correlation should be calculated (by default, the Pearson correlation is calculated). If `x` is a data frame with at least two columns, `y` can be omitted – then the function calculates pairwise correlations between all columns of the data frame.


### The `cor.test()` function

Calling the `cor.test()` function is very similar:

```r
cor.test(x, y, alternative="t", method="pearson", conf.level=0.95)
```

Here, we can specify the form of the alternative hypothesis (`"two-sided"`, `"greater"`, `"less"`) and the confidence level. This function can only handle two variables.


### The `rcorr()` function

We call the `rcorr()` function as follows:

```r
rcorr(x, y, type="pearson")
```

Here, `x` and `y` are vectors. As with the `cor()` function, it is possible to specify only the `x` argument if it is a matrix with at least two columns.


## Example

The three functions can best be illustrated using an example. We load a (fictional) data set [`exam.dat`](exam.dat) on exam anxiety:

```{r}
#| message: false
library(readr)
(exam = read_tsv("exam.dat"))
```

It is helpful to first visualize the data. A scatter plot (including the regression line) is suitable for visualizing the relationship between two variables. We can create three separate plots for the three pairwise combinations:

```{r}
plot(exam$Revise, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Exam ~ exam$Revise), col="blue")
plot(exam$Anxiety, exam$Exam, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Exam ~ exam$Anxiety), col="blue")
plot(exam$Revise, exam$Anxiety, pch=16, col=rgb(0, 0, 0, 0.5))
abline(lm(exam$Anxiety ~ exam$Revise), col="blue")
```

:::{.callout-tip}
If there are more than two variables (as in this example), the `pairs()` function can be used to display all pairwise scatter plots simultaneously:

```{r}
pairs(exam[, 2:4])
```

If you also want to include regression lines, you can pass an (anonymous) function as an argument to the `panel` argument:

```{r}
pairs(
    exam[2:4],
    panel=function(x, y) {
        points(x, y, pch=16, col=rgb(0, 0, 0, 0.5))
        abline(lm(y ~ x), col="blue")
    }
)
```
:::


### Pearson correlation

Now we calculate the Pearson correlations between the three variables `Exam`, `Anxiety`, and `Revise`:

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")])
```

The output is a correlation matrix. We can obtain the individual coefficients for all variable pairs directly from this matrix. The diagonal contains the correlations of the variables with themselves and therefore consists of values that are exactly 1. It does not matter whether we read the correlations in the triangle *below* the diagonal or *above* the diagonal, as the correlation matrix is symmetric (the correlation between "Exam" and "Revise" is the same as between "Revise" and "Exam" – the correlation does not measure causal relationships).

If we also need *p*-values, we have to use the `rcorr()` function. However, this function does not expect the data as a data frame but as a matrix. Therefore, we must convert the data into a matrix when calling the function:

```{r}
rcorr(as.matrix(exam[, c("Exam", "Anxiety", "Revise")]))
```

In addition to the correlation matrix, the function also provides the *p*-values. In this example, all correlations are significant, as the *p*-values are very small (rounded to zero).

If we also need confidence intervals, we must use the `cor.test()` function. However, this function only supports two variables, so we must call the function multiple times to obtain all pairwise correlations. For example, to calculate the correlation between "Anxiety" and "Exam":

```{r}
cor.test(exam$Anxiety, exam$Exam)
```


### Coefficient of determination $R^2$

If we square the correlation coefficient $r$, we obtain the coefficient of determination $R^2$. It indicates how much variance in one variable can be explained by the other (again, this does not imply causation). It is common to denote the correlation with the lowercase $r$, but the coefficient of determination uses an uppercase $R^2$.

$$R^2 = r^2$$

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")])^2
```


### Spearman rank correlation coefficient

We can also determine the Spearman correlation:

```{r}
cor(exam[, c("Exam", "Anxiety", "Revise")], method="spearman")
rcorr(as.matrix(exam[, c("Exam", "Anxiety", "Revise")]), type="spearman")
cor.test(exam$Revise, exam$Exam, method="spearman")
```


### Kendall rank correlation coefficient

The function `cor.test()` emits a warning in the previous example, because the data contain equal ranks. In such cases, the Kendall correlation coefficient is the better choice:

```{r}
cor.test(exam$Revise, exam$Exam, method="kendall")
```


## Exercises

### Exercise 1

In this chapter, we calculated the significance of a correlation $r = 0.25$ with a sample size of $N = 40$. This correlation was not significant with $p = 0.12$. Now, consider the following questions:

1. What is the smallest sample size $N$ that you would need to choose to obtain a significant result with $\alpha = 0.05$?
2. What is the smallest sample size $N$ that would make even a very small correlation of $r = 0.05$ significant?

You can solve this example by trying different sample sizes (increasing them step by step) and calculating the corresponding *p*-values.


### Exercise 2

We manually calculated the correlation between $x = (8, 1, -4, 5, 6, 10, 9)^T$ and $y = (-2, -5, -6, 0, 3, 7, 10)^T$ in this chapter. Now calculate this correlation using R! In addition, provide the 99% confidence interval and the *p*-value! Is this correlation significant at a significance level of $\alpha = 0.01$?


### Exercise 3

Using the example data set `mtcars`, analyze the relationship between the variables `mpg`, `disp`, and `hp`. Visualize the relationship between the variable pairs and then calculate Pearson and Spearman correlations!


### Exercise 4

The file [`pm10.csv`](pm10.csv) contains the monthly PM10 particulate matter concentration values from two measurement stations in Graz from February 2006 to May 2016. Perform the following analyses:

- Create a line plot showing the PM10 concentration over time for both measurement stations.
- Create a plot showing the relationship between the two measurement stations.

Finally, calculate the Pearson correlation between both measurement stations, including the confidence interval and *p*-value. How much variance of one variable can be explained by the other variable?

:::{.callout-note}
After importing the data, convert the `Date` column to a date type. To display both time series simultaneously, you can first create a line plot of one variable and then add the second time series with a different color using the `lines()` function.

If you use the `cor()` function to calculate the correlation, you can exclude missing values in either of the two variables using the argument `use="complete.obs"`.
:::


### Exercise 5

First, calculate the correlation between bill length (`bill_length_mm`) and bill depth (`bill_depth_mm`) in the `penguins` data set (from the `palmerpenguins` package). Second, calculate this correlation for each species separately! What do you notice?
