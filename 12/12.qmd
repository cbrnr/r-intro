---
title: "12 – Mittelwertvergleich"
subtitle: "Statistische Datenanalyse mit R"
author: "Clemens Brunner"
date: 2023-01-26
format: html
engine: knitr
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
lang: de
author-title: "Autor"
published-title: "Veröffentlicht"
---

## Versuchsdesigns

Nicht nur die Beziehungen zwischen zwei (oder mehreren) Variablen sind in Studien interessant – wie wir gesehen haben, kann man solche (lineare) Beziehungen mit Korrelationen oder Regressionsmodellen beschreiben. Oft ist man aber auch an mittleren Unterschieden zwischen zwei (oder mehreren) Gruppen interessiert.

Im Gegensatz zu *korrelativen* Studiendesigns kann man mit *experimentellen* Studiendesigns auch auf Kausalität rückschließen. In experimentellen Designs ist man daher oft an Mittelwertsunterschieden zwischen zwei (oder mehreren) Gruppen interessiert, welche auf eine konkrete Behandlung zurückzuführen sind.

:::{.callout-tip}
Korrelative Studien nennt man auch [Beobachtungsstudien](https://de.wikipedia.org/wiki/Beobachtungsstudie) – hier erhebt man Daten, ohne Manipulationen entsprechend der Forschungshypothese vorzunehmen. Experimentelle Studien nennt man auch [Interventionsstudien](https://de.wikipedia.org/wiki/Interventionsstudie) – hier manipuliert man gezielt eine oder mehrere Variablen, die für die Forschungshypothese relevant sind (z.B. eine Gruppe erhält ein Medikament, eine andere ein Placebo).
:::

Prinzipiell unterscheidet man bei experimentellen Designs zwischen *abhängigen* und *unabhängigen* Designs. In unabhängigen Designs sind die Individuen in den einzelnen Gruppen voneinander unabhängig (z.B. verschiedene Personen). In abhängigen Designs sind die Individuen zwar innerhalb der Gruppen unabhängig, aber zwischen den Gruppen besteht eine Abhängigkeit (z.B. befinden sich dieselben Personen in mehreren Gruppen, man spricht auch von einem Messwiederholungs-Design). Abhängige Designs berücksichtigen individuelle Unterschiede, die für die Behandlung nicht relevant sind, und können daher kleinere Unterschiede besser detektieren als unabhängige Designs. Dies lässt sich an einem Beispiel veranschaulichen.


## Beispiel

In einem (fiktiven) Versuch wurde die Angst vor Spinnen auf einer Skala von 0 bis 100 gemessen (größere Werte stehen für höhere Angst). Es wurden 24 Personen in zwei Gruppen untersucht (also 12 Personen pro Gruppe). Einer Gruppe wurden Fotos von Spinnen gezeigt, der anderen Gruppe wurden echte Spinnen gezeigt. Wir beginnen mit dem Importieren der Datei [`spider.dat`](spider.dat):

```{r}
#| message: false
library(readr)
(spider = read_tsv("spider.dat"))
```

Die mittleren Angstwerte für die beiden Gruppen betragen:

```{r}
by(spider$Anxiety, spider$Group, mean)
```

Im Durchschnitt ist die Angst also in der Gruppe, die echte Spinnen sieht, größer.


### Unabhängige Stichproben

Die Daten in `spider` liegen im Long-Format vor, d.h. dieses Format würde gut für ein unabhängiges Design passen, da sich so jede Person in einer eigenen Zeile befindet.

Stellen wir nun die Mittelwerte beider Gruppen in einer Grafik gegenüber. Die schwarzen Punkte zeigen die Gruppenmittelwerte, die Fehlerbalken stellen die 95%-Konfidenzintervalle dar, und die roten Punkte sind die Angstwerte der einzelnen Personen:

```{r}
#| message: false
#| echo: false
library(dplyr)
spider_stats = spider %>%
    group_by(Group) %>%
    summarize(N=length(Anxiety),
              mean=mean(Anxiety),
              sd=sd(Anxiety),
              se=sd/sqrt(N),
              ci=se*qt(0.975, N-1))

library(ggplot2)
set.seed(1)
theme_set(theme_minimal())
ggplot(spider_stats, aes(Group, mean)) +
    geom_jitter(
        data=spider,
        mapping=aes(x=Group, y=Anxiety),
        alpha=0.5,
        height=0.5,
        width=0.05,
        color="red"
    ) +
    geom_point(size=4) +
    geom_errorbar(aes(ymin=mean-ci, ymax=mean+ci), width=0.1) +
    xlab("") +
    ylab("Anxiety") +
    ggtitle("Unabhängiges Design")
```

Man erkennt, dass die Konfidenzintervalle stark überlappen, d.h. der Unterschied zwischen den beiden Mittelwerten ist wahrscheinlich nicht signifikant.


### Abhängige Stichproben

Bei abhängigen Gruppen (also beispielsweise aus einem Design mit Messwiederholung, in welchem jede der 12 Personen in beiden Gruppen war) bringt man die Daten zuerst am besten ins Wide-Format:

```{r}
#| message: false
library(tidyr)
spider_w = pivot_wider(
    cbind(id=rep(1:12, 2), spider),
    names_from=Group,
    values_from=Anxiety
)
spider_w$id = NULL
names(spider_w) = c("picture", "real")
spider_w
```

Anschließend berechnen wir angepasste Werte, welche die individuellen Unterschiede der Personen berücksichtigen. Dazu fügen wir zunächst eine Spalte mit der mittleren Angst jeder Person hinzu.

```{r}
spider_w$mean = rowMeans(spider_w)
spider_w
```

Nun berechnen wir die Differenz der Personenmittelwerte zum Gesamtmittelwert aller Daten – dies ist der Korrekturfaktor für Designs mit Messwiederholung.

```{r}
spider_w$adj = mean(c(spider_w$picture, spider_w$real)) - spider_w$mean
spider_w
```

Damit können wir die einzelnen Angstwerte pro Person korrigieren.

```{r}
spider_w$picture_adj = spider_w$picture + spider_w$adj
spider_w$real_adj = spider_w$real + spider_w$adj
```

Dadurch haben wir erreicht, dass die korrigierten Werte nun für alle Personen denselben Mittelwert ergeben, d.h. personenspezifische Unterschiede werden berücksichtigt und nur der Unterschied zwischen den Gruppen wird untersucht:

```{r}
rowMeans(spider_w[, c("picture_adj", "real_adj")])
```

Die Daten sehen nun so aus:
```{r}
spider_w
```

Die angepassten Werte können wir wieder in einer Grafik darstellen:

```{r echo=FALSE}
spider_w_stats = data.frame(Group=c("Picture", "Real Spider"),
                             N=c(length(spider_w$picture_adj), length(spider_w$real_adj)),
                             mean=c(mean(spider_w$picture_adj), mean(spider_w$real_adj)),
                             sd=c(sd(spider_w$picture_adj), sd(spider_w$real_adj)))
spider_w_stats = spider_w_stats %>%
    mutate(se=sd/sqrt(N), ci=qt(0.975, N-1)*se)
set.seed(1)
ggplot(spider_w_stats, aes(Group, mean)) +
    geom_jitter(
        data=spider,
        mapping=aes(x=Group, y=Anxiety),
        alpha=0.5,
        height=0.5,
        width=0.05,
        color="red"
    ) +
    geom_point(size=4) +
    geom_errorbar(aes(ymin=mean-ci, ymax=mean+ci), width=0.1) +
    xlab("") +
    ylab("Anxiety") +
    ggtitle("Abhängiges Design")
```

Aus der Grafik ist ersichtlich, dass die Gruppenmittelwerte (schwarze Punkte) gleich wie im unabhängigen Design sind. Die Konfidenzintervalle sind jedoch aufgrund des abhängigen Designs wesentlich kleiner geworden. In diesem Beispiel überlappen sie jetzt nicht mehr, was auf einen signifikanten Unterschied zwischen den Gruppenmittelwerten schließen lässt.


## Der _t_-Test

Der _t_-Test wird verwendet, um die Mittelwerte zweier Gruppen miteinander zu vergleichen. Hier gibt es zwei Varianten, nämlich einen _t_-Test für abhängige Gruppen und einen für unabhängige Gruppen. Ersteren nennt man auch gepaarten oder abhängigen _t_-Test, letzteren nennt man unabhängigen _t_-Test.

Die _t_-Statistik ist wie viele Statistiken aufgebaut: sie setzt die Varianz, die vom Modell erklärt werden kann in Beziehung zur Varianz, die *nicht* vom Modell erklärt werden kann (also kurz Effekt geteilt durch Fehler). Im Falle des _t_-Tests ist das Modell der Unterschied der beiden Mittelwerte minus der erwarteten Differenz, und der Fehler wird durch den Standardfehler der Mittelwertsdifferenz geschätzt:

$$t = \frac{\text{Beobachtete Differenz} - \text{Erwartete Differenz}}{\text{Standardfehler der Differenz}}$$

Die Differenz bezieht sich immer auf die Differenz zwischen den Mittelwerten. Die erwartete Differenz (unter der Annahme der Nullhypothese) ist in den meisten Fällen gleich Null. Die Vorgehensweise beim Testen ist also wie folgt:

* Zwei Stichproben werden erhoben und deren Mittelwerte berechnet. Die beiden Mittelwerte können sich wenig oder stark voneinander unterscheiden.
* Wenn beide Stichproben aus derselben Population stammen, sollten die Mittelwerte ungefähr gleich sein. Dies ist die Annahme der Nullhypothese (es gibt keinen Unterschied). Große Differenzen können in seltenen Fällen aber zufällig auftreten.
* Wir vergleichen den beobachteten Unterschied mit dem erwarteten Unterschied, und wir verwenden den Standardfehler als Maß für die Variabilität der Stichprobenmittelwerte.
* Wenn der beobachtete Unterschied größer als der erwartete ist, kann das zwei Gründe haben:
    1. Es gibt keinen Unterschied und der beobachtete Unterschied ist zufällig aufgetreten, d.h. zumindest eine Stichprobe ist nicht repräsentativ für ihre Population.
    2. Die beiden Stichproben kommen aus unterschiedlichen Populationen, d.h. beide Stichproben sind repräsentativ für ihre Population. Das bedeutet, dass es also tatsächlich einen Unterschied gibt!

Je größer der Unterschied bzw. die _t_-Statistik, desto mehr spricht für den zweiten Fall (d.h. für einen tatsächlichen Effekt).


## Der _t_-Test als lineares Modell

Wir haben den _t_-Test bereits bei der linearen Regression kennengelernt. Hier wird er verwendet, um zu überprüfen, ob Regressionskoeffizienten signifikant unterschiedlich von 0 sind. Wie wir außerdem soeben erfahren haben, wird der _t_-Test auch angewendet, um Mittelwerte zu vergleichen. Tatsächlich ist es so, dass der _t_-Test als lineares Modell interpretiert werden kann. Er kann also wieder mit folgender allgemeinen Form beschrieben werden:

$$y_i = \hat{y}_i + \varepsilon_i$$

Ein _t_-Test kann in diesem Kontext mit abhängigen und unabhängigen Variablen formuliert werden. Die abhängige Variable ist die Variable, deren Gruppenmittelwerte man vergleichen möchte (z.B. Angst vor Spinnen). Die unabhängige Variable codiert die Gruppen. Man verwendet dann für den _t_-Test das bekannte Modell:

$$\hat{y}_i = b_0 + b_1 \cdot x_i$$

Am besten illustriert man die Funktionsweise mit einem Beispiel. Dazu verwenden wir wieder die Daten über die Angst vor Spinnen. Erstellen wir nun ein lineares Regressionsmodel, welches `Anxiety` durch `Group` vorhersagt:

```{r}
model = lm(Anxiety ~ Group, data=spider)
summary(model)
```

:::{.callout-note}
Hier ist anzumerken, dass die kategorische Variable `Group` von R automatisch mit 0 und 1 codiert wird (und zwar alphabetisch, d.h. die Stufe `Picture` entspricht 0 und die Stufe `Real Spider` entspricht 1). Man spricht hier von Dummy-Coding.
:::

Wenn man sich nun die Regressionskoeffizienten ansieht, bemerkt man, dass der Intercept $b_0 = 40$ dem Mittelwert der Gruppe 0 (`Picture`) entspricht. Die Steigung $b_1 = 7$ entspricht dem Unterschied der Mittelwerte zwischen den beiden Gruppen ($47 - 40 = 7$). Der _t_-Test für diesen Koeffizienten testet, ob die Steigung signifikant von Null verschieden ist. Er testet somit also auch, ob der Unterschied zwischen den Mittelwerten von 7 signifikant von Null verschieden ist. Man sieht, dass dieser Test nicht signifikant ist ($p=0.107$), d.h. man kann daraus schließen, dass sich die Mittelwerte nicht signifikant voneinander unterscheiden.

Grafisch kann man die Situation wie folgt darstellen:
```{r}
library(ggplot2)
ggplot(spider, aes(Group, Anxiety)) +
    geom_point(alpha=0.5) +
    geom_smooth(method="lm", aes(group=1), formula="y~x") +
    xlab("")
```

Die Regressionsgerade verbindet beide Gruppenmittelwerte und die Steigung beträgt 7.

Wir können dies durch Einsetzen der Gruppenmittelwerte in die Gleichung des linearen Modells überprüfen. Wir beginnen mit der Gruppe 0, also `Picture`. Wir wissen, dass der Mittelwert dieser Gruppe gleich 40 ist.

```{r}
mean(spider$Anxiety[spider$Group=="Picture"])
```

Wir setzen in folgende Gleichung ein:
$$\hat{y}_i = b_0 + b_1 \cdot x_i$$
Für $\hat{y}_i$ verwenden wir den Gruppenmittelwert der `Picture`-Gruppe, und das zugehörige $x_i$ ist also $x_{\text{Picture}}$ (codiert mit 0).
$$\bar{y}_{\text{Picture}} = b_0 + b_1 \cdot x_{\text{Picture}}$$
$$40 = b_0 + b_1 \cdot 0$$
$$b_0 = 40$$

Man sieht also, dass der Intercept $b_0$ dem Gruppenmittelwert der ersten Gruppe (mit Codierung 0) entspricht. Setzen wir anschließend die Werte für die Gruppe `Real Spider` ein. Hier beträgt der Gruppenmittelwert 47:

```{r}
mean(spider$Anxiety[spider$Group=="Real Spider"])
```

$$\bar{y}_{\text{Real Spider}} = b_0 + b_1 \cdot x_{\text{Real Spider}}$$
$$47 = 40 + b_1 \cdot 1$$
$$b_1 = 7$$

Die Steigung der Geraden entspricht also genau dem Unterschied der beiden Mittelwerte.


## Annahmen

Da der _t_-Test eine lineare Regression ist, setzt er auch dieselben Annahmen wie diese voraus:

* Die Schätzfunktion ist normalverteilt (beim abhängigen _t_-Test ist die Schätzfunktion der Differenzen gemeint).
* Die Daten sind intervallskaliert.
* Beim unabhängigen _t_-Test müssen die Daten in den Gruppen voneinander unabhängig sein.
* Beim unabhängigen _t_-Test muss Varianzhomogenität gegeben sein. Praktisch ist diese Voraussetzung aber nicht notwendig, da standardmäßig ein verbessertes Verfahren angewendet wird, welches Verletzungen dieser Annahme automatisch korrigiert ([Welch _t_-Test](https://de.wikipedia.org/wiki/Zweistichproben-t-Test#Welch-Test)).


## Unabhängiger _t_-Test in R

Die _t_-Statistik berechnet sich also durch das Verhältnis erklärter Varianz (Spalte `Estimate`) zu nicht erklärter Varianz (Spalte `Std. Error`). Beim unabhängigen _t_-Test vergleicht man so die Mittelwerte beider Bedingungen:

$$t = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\text{Standardfehler}}$$

Die Nullhypothese besagt dass $\mu_1 = \mu_2$, daher vereinfacht sich die Gleichung zu:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\text{Standardfehler}}$$

Der Standardfehler der Differenz beider Gruppen ist bei gleicher Gruppengröße wie folgt definiert:

$$\text{SE} = \sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}$$

Daher lautet die Gleichung für den _t_-Test:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}}$$

Wenn die beiden Gruppen unterschiedlich viele Personen enthalten, muss man den Standardfehler über die gepoolte Varianz berechnen:

$$s_p^2 = \frac{(N_1 - 1) s_1^2 + (N_2 - 1) s_2^2}{N_1 + N_2 - 2}$$

Daraus ergibt sich dann für den _t_-Test mit $N_1 + N_2 - 2$ Freiheitsgraden:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_p^2}{N_1} + \frac{s_p^2}{N_2}}}$$

In R kann man _t_-Tests mit der Funktion `t.test()` durchführen. Der Aufruf ist auf zwei verschiedene Arten möglich. Eine Möglichkeit ist, die Funktion wie ein lineares Modell aufzurufen:

```{r}
model = t.test(Anxiety ~ Group, data=spider)
model
```

Das erste Argument ist also eine Formel, deren linke Seite die Datenpunkte angibt (die Spalte `Anxiety` in diesem Beispiel). Die rechte Seite der Formel gibt die Gruppierungsspalte an (also hier `Group`). Damit man die Spaltennamen direkt verwenden kann, spezifiziert man noch `data=spider`, damit klar ist dass diese Spalten im Data Frame `spider` zu finden sind.

Ausgegeben wird der Wert der _t_-Statistik, die Freiheitsgrade (standardmäßig korrigiert nach Welch, was die Voraussetzung der Varianzhomogenität überflüssig macht), sowie der _p_-Wert. Weiters gibt es noch das 95%-Konfidenzintervall für die _t_-Statistik sowie die Gruppenmittelwerte.

Die zweite Möglichkeit die Funktion aufzurufen verwendet zwei Argumente; hier übergibt man also die Daten der beiden Gruppen in Vektoren als jeweils separate Argumente:

```{r}
(model = t.test(spider_w$picture, spider_w$real))
```

Das Ergebnis ist aber immer dasselbe, egal wie die Funktion aufgerufen wird. Der Unterschied zwischen den Gruppenmittelwerten ist nicht signifikant.

Die Effektgröße kann man aus dem Wert von $t$ in eine Korrelation $r$ umrechnen:

$$r = \sqrt{\frac{t^2}{t^2 + \text{df}}}$$

Im Beispiel ist die Effektgröße $r$ also:

```{r}
t = model$statistic[[1]]
df = model$parameter[[1]]
r = sqrt(t^2 / (t^2 + df))
round(r, 3)
```


## Abhängiger _t_-Test in R

Der abhängige (oder gepaarte) _t_-Test funktioniert ebenso, sieht sich aber die Mittelwerte der einzelnen Differenzen an anstelle der Differenz der Mittelwerte:

$$t = \frac{\bar{D} - \mu_D}{s_D / \sqrt{N}}$$

Unter der Nullhypothese ist $\mu_D = 0$:
$$t = \frac{\bar{D}}{s_D / \sqrt{N}}$$

Für den abhängigen _t_-Test verwendet man wieder die Funktion `t.test()` und setzt das Argument `paired=TRUE`. Wieder gibt es zwei Möglichkeiten des Aufrufs, je nachdem, in welchem Format die Daten vorliegen.

```{r}
(model = t.test(Anxiety ~ Group, data=spider, paired=TRUE))
```

```{r}
(model = t.test(spider_w$picture, spider_w$real, paired=TRUE))
```

Das Ergebnis ist diesmal signifikant mit $p=0.03098$. Dies entspricht unseren Überlegungen mit den unterschiedlich großen Konfidenzintervallen von abhängigen und unabhängigen Versuchsdesigns. Abhängige Designs können also kleinere Unterschiede detektieren.

Die Effektgröße kann man wieder aus $t$ mit der Formel oben in $r$ umwandeln:

```{r}
t = model$statistic[[1]]
df = model$parameter[[1]]
r = sqrt(t^2 / (t^2 + df))
round(r, 3)
```

Es handelt sich also in diesem Fall um einen großen Effekt.


## Ausblick

Zusammenfassend haben wir also gesehen, dass der Mittelwertsvergleich über den *t*-Test auf ein lineares Modell zurückgeführt werden kann. Möchte man mehr als zwei Mittelwerte vergleichen, kann man das lineare Modell mit mehreren Prädiktoren verwenden (Dummy-Coding). In der klassischen Varianzanalyse betrachtet man die *F*-Statistik, welche aber auch Teil der linearen Regression ist (sie misst die Güte des Modells) und dementsprechend auch in der Ausgabe aufscheint. Man kann also auch eine ANOVA als Spezialfall eines linearen Modells sehen. Um die vertrauten ANOVA-Tabellen denoch auch in R zu erhalten, kann man beispielsweise das Paket `ez` benutzen.

Bei abhängigen Messungen kann man keine linearen Modelle rechnen, da eine der wichtigsten Voraussetzungen die Unabhängigkeit der Messpunkte ist. Sogenannte lineare gemischte Modelle (engl. *linear mixed models*) können aber mit diesen Abhängigkeiten umgehen und werden immer häufiger statt den klassischen Messwiederholungs-ANOVAs eingesetzt. Gemischte Modelle sind Verallgemeinerungen von linearen Modellen, oder umgekehrt sind lineare Modelle Spezialfälle von gemischten Modellen. Das Paket `lme4` hat sich in R als Standard zur Berechnung dieser Modelle durchgesetzt (bzw. die darauf aufbauenden Pakete `lmerTest` und `afex`).


## Übungen

### Übung 1

Das Paket `dplyr` beinhaltet den Datensatz `starwars` (aktivieren Sie das Paket um auf diesen Datensatz zugreifen zu können). Verwenden Sie diese Daten um herauszufinden, ob die weiblichen Charaktere signifikant kleiner sind als die männlichen (Spalte `height`). Verwenden Sie die Spalte `gender` um zwischen den Geschlechtern `"feminine"` und `"masculine"` zu unterscheiden. Verwenden Sie für Ihre Analyse ein Signifikanzniveau von 5% und geben Sie die Mittelwerte beider Gruppen an.

Führen Sie auch einen Vergleich des Gewichtes (Spalte `mass`) durch (überprüfen Sie die Hypothese dass männliche Charaktere mehr wiegen als weibliche). Geben Sie auch hier die Mittelwerte beider Gruppen an.

*Hinweis:* Verwenden Sie das Argument `alternative` von `t.test()`, um einen *einseitigen* Test durchzuführen.


### Übung 2

Vergleichen Sie im `penguins`-Datensatz (aus dem Paket `palmerpenguins`), ob sich die Merkmale `bill_length_mm` bzw. `bill_depth_mm` zwischen den Spezies unterscheiden (führen Sie paarweise Vergleiche für jedes Merkmal durch). Berichten Sie die relevanten Statistiken, Signifikanzen und Effektgrößen (Korrelationskoeffizient *r*) für jeden *t*-Test.

Die Funktion `pairwise.t.test()` rechnet mehrere paarweise Vergleiche. Verwenden Sie diese Funktion (zusätzlich zu den einzeln gerechneten *t*-Tests von zuvor), um die Vergleiche durchzuführen. Mit dem Argument `p.adjust.method` können Sie außerdem die *p*-Werte korrigieren, weil mehrere Tests die Wahrscheinlichkeit eines falsch positiven Ergebnisses (der *t*-Test sagt "es gibt einen Unterschied" obwohl tatsächlich kein Unterschied besteht) erhöhen.


### Übung 3

Laden Sie den Datensatz `sleep` (standardmäßig bei R dabei). Unterscheiden sich die beiden Gruppen (`group`) in der Anzahl an zusätzlichen Schlafstunden (`extra`)? Wenn ja, wie groß ist dieser Unterschied im Mittel? Geben Sie um den mittleren Unterschied auch ein 95%-Konfidenzintervall an.

*Hinweis:* Entnehmen Sie dem Hilfetext, ob Sie einen abhängigen oder unabhängigen *t*-Test rechnen müssen.
