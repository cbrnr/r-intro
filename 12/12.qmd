---
title: "12 – Comparing Means"
subtitle: "Statistical Data Analysis with R"
author: "Clemens Brunner"
date: 2025-06-02
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
---

## Experimental designs

Besides looking at how variables relate – for example, with correlations or regression – we often want to know if two (or more) groups differ in their average scores. In correlational (or [observational](https://en.wikipedia.org/wiki/Observational_study)) studies, we simply collect data *without* manipulating anything. Experimental (or [intervention](https://en.wikipedia.org/wiki/Interventional_study)) studies deliberately *manipulate* one or more variables (for example, one group receives a drug, another a placebo). This allows us to attribute any differences in the outcome to the manipulation, letting us draw causal conclusions that correlational studies cannot offer.

Within experimental studies, we distinguish between *independent* and *dependent* (repeated-measures) designs. In an independent design, each group is made up of different participants, so observations are unrelated across groups. In a dependent design, the same participants appear in multiple conditions, creating a dependency between groups. Because each person serves as their own control, dependent designs automatically factor out individual differences that are irrelevant to the treatment, giving the analysis more power to detect small effects than an independent design. This advantage becomes clear in the following example.


## Example

Imagine a study that measured fear of spiders on a scale from 0 to 100 (higher scores indicate greater fear). Twenty-four volunteers participated in the study and were split into two groups (12 participants per group). One group viewed photos of spiders, whereas the other group was confronted with real spiders. We start by importing the data from the file [`spider.dat`](spider.dat):

```{r}
#| message: false
library(readr)
(spider = read_tsv("spider.dat"))
```

The mean anxiety values for the two groups are:

```{r}
by(spider$Anxiety, spider$Group, mean)
```

The mean anxiety for the real spider group is 7 points higher than for the picture group.


### Independent samples

The `spider` data frame contains data from one person per row (which is a useful format for independent designs). The following plot shows the anxiety values for each person in both groups (black dots represent the group means, red dots represent individual anxiety values, and the error bars represent 95% confidence intervals):


```{r}
#| message: false
#| echo: false
library(dplyr)
spider_stats = spider %>%
    group_by(Group) %>%
    summarize(N=length(Anxiety),
              mean=mean(Anxiety),
              sd=sd(Anxiety),
              se=sd/sqrt(N),
              ci=se*qt(0.975, N-1))

library(ggplot2)
set.seed(1)
theme_set(theme_minimal())
ggplot(spider_stats, aes(Group, mean)) +
    geom_jitter(
        data=spider,
        mapping=aes(x=Group, y=Anxiety),
        alpha=0.5,
        height=0.5,
        width=0.05,
        color="red"
    ) +
    geom_point(size=4) +
    geom_errorbar(aes(ymin=mean-ci, ymax=mean+ci), width=0.1) +
    xlab("") +
    ylab("Anxiety") +
    ggtitle("Independent design")
```

The plot clearly shows that the confidence intervals overlap significantly, i.e., the difference between the two means is probably not significant.


### Dependent samples

Now imagine we've switched to a dependent design: the same 12 participants took part in both conditions. To reflect this, we reshape the data into a wide format, putting each person into a single row with two columns (one for each condition):

```{r}
#| message: false
library(tidyr)
spider_w = pivot_wider(
    cbind(id=rep(1:12, 2), spider),
    names_from=Group,
    values_from=Anxiety
)
spider_w$id = NULL
spider_w
```

Next, we calculate adjusted values that take individual fear levels into account. First, we add a column with the mean anxiety of each person:

```{r}
spider_w$mean = rowMeans(spider_w)
spider_w
```

Now we calculate the difference between the individual mean values and the overall mean value of all data points – this is the correction factor for repeated-measures designs.

```{r}
spider_w$adj = mean(c(spider_w$picture, spider_w$real)) - spider_w$mean
spider_w
```

This way, we can correct the individual anxiety values per person.

```{r}
spider_w$picture_adj = spider_w$picture + spider_w$adj
spider_w$real_adj = spider_w$real + spider_w$adj
```

The corrected values are now centered around the same mean value for all persons, i.e., individual differences are taken into account, and only the difference between the groups remains:

```{r}
rowMeans(spider_w[, c("picture_adj", "real_adj")])
```

The data now look like this:

```{r}
spider_w
```

We can visualize the adjusted values in a plot:

```{r echo=FALSE}
spider_w_stats = data.frame(Group=c("picture", "real"),
                             N=c(length(spider_w$picture_adj), length(spider_w$real_adj)),
                             mean=c(mean(spider_w$picture_adj), mean(spider_w$real_adj)),
                             sd=c(sd(spider_w$picture_adj), sd(spider_w$real_adj)))
spider_w_stats = spider_w_stats %>%
    mutate(se=sd/sqrt(N), ci=qt(0.975, N-1)*se)
set.seed(1)
ggplot(spider_w_stats, aes(Group, mean)) +
    geom_jitter(
        data=spider,
        mapping=aes(x=Group, y=Anxiety),
        alpha=0.5,
        height=0.5,
        width=0.05,
        color="red"
    ) +
    geom_point(size=4) +
    geom_errorbar(aes(ymin=mean-ci, ymax=mean+ci), width=0.1) +
    xlab("") +
    ylab("Anxiety") +
    ggtitle("Dependent design")
```

The plot shows that the group means (black dots) are identical to the independent design. The raw data (red dots) have not changed either. However, the confidence intervals are much smaller, because we removed individual differences. In this example, they no longer overlap, indicating a significant difference between the group means.


## The *t*-test

After visualizing group differences, let's now discuss the *t*-test, which is used to compare the means of two groups. There are two variants of the *t*-test, namely a *dependent* (or *paired*) and an *independent* version.

The *t*-statistic relates the variance explained by the model to the variance *not* explained by the model (or, in other words, *effect* divided by *error*). In the case of the *t*-test, the model is the difference between the two means minus the expected difference, and the error is estimated by the standard error of the mean difference:

$$t = \frac{\text{Observed difference} - \text{Expected difference}}{\text{Standard error of the difference}}$$

Here, the difference always refers to the difference between the group means. The expected difference (under the null hypothesis) is usually zero (we expect no group differences). The procedure for testing is as follows:

* We collect two samples and calculate their means.
* If both samples come from the same population, the means should be approximately equal. This is the null hypothesis (there is no difference). Larger differences can occur randomly (but rarely).
* We compare the observed difference with the expected difference, and we use the standard error as a measure of the variability of the sample means.
* If the observed difference is greater than the expected difference, this can have two reasons:
    1. There is no difference, and the observed difference occurred by chance (i.e., one sample is not representative of its population).
    2. The two samples come from different populations, i.e., both samples are representative of their population. This means that there is indeed a significant difference between the two groups!

The larger the difference (or rather, the *t*-statistic), the more evidence there is for the second case (i.e., for an actual effect).


## The *t*-test as a linear model

We first encountered the *t*-test in the context of linear regression, where we used it to check whether regression coefficients differ significantly from zero. The very same test also lets us compare group means, because a difference between means can be framed as a simple linear model:

$$y_i = \hat{y}_i + \varepsilon_i$$

The *dependent* variable $y$ is the variable whose group means we want to compare (e.g., fear of spiders). The *independent* variable $x$ encodes the groups. We then fit a linear model $\hat{y}$:

$$\hat{y}_i = b_0 + b_1 \cdot x_i$$

Let's illustrate this with an example (using the spider fear data). We create a linear regression model that predicts `Anxiety` from `Group`:

```{r}
model = lm(Anxiety ~ Group, data=spider)
summary(model)
```

:::{.callout-note}
R automatically encodes the categorical variable `Group` as 0 and 1 (alphabetically, i.e., the level `picture` corresponds to 0 and the level `real` corresponds to 1). This is called *dummy coding*.
:::

Looking at the regression coefficients, we observe that the intercept $b_0 = 40$ corresponds to the mean of group 0 (`picture`). The slope $b_1 = 7$ corresponds to the difference between the group means ($47 - 40 = 7$). The *t*-test for this coefficient tests whether the slope is significantly different from zero. However, it also tests whether the difference between the group means is significantly different from zero. We see that this test is not significant with $p=0.107$, i.e., we can conclude that the means do not differ significantly. This result confirms the insight from our graphical representation (overlapping confidence intervals) from the independent samples design.

Graphically, we can represent the situation as follows:

```{r}
#| echo: false
library(ggplot2)
ggplot(spider, aes(Group, Anxiety)) +
    geom_jitter(width=0.05, height=0, alpha=0.5) +
    geom_smooth(method="lm", aes(group=1), formula="y~x") +
    xlab("")
```

The regression line connects both group means, and its slope is 7.

We can verify this by substituting the group means into the equation of the linear model. We start with group 0, i.e., `picture`. We know that the mean of this group is 40.

```{r}
mean(spider$Anxiety[spider$Group=="picture"])
```

We substitute the group mean into the equation:

$$\hat{y}_i = b_0 + b_1 \cdot x_i$$

We use the group mean of the `picture` group for $\hat{y}_i$, and the corresponding $x_i$ is thus $x_{\text{picture}}$ (encoded as 0).

$$\bar{y}_{\text{picture}} = b_0 + b_1 \cdot x_{\text{picture}}$$
$$40 = b_0 + b_1 \cdot 0$$
$$b_0 = 40$$

Apparently, the intercept $b_0$ corresponds to the group mean of the first group (encoded as 0). Next, we substitute the values for the `real` group. Here, the group mean is 47:

```{r}
mean(spider$Anxiety[spider$Group=="real"])
```

$$\bar{y}_{\text{real}} = b_0 + b_1 \cdot x_{\text{real}}$$
$$47 = 40 + b_1 \cdot 1$$
$$b_1 = 7$$

Therefore, the slope $b_1$ corresponds to the difference between the two means.


## Assumptions

Since the *t*-test is based on linear regression, it also has the same assumptions as linear regression:

* The residuals are normally distributed (for the dependent *t*-test, this means the residuals of the differences).
* The data are interval-scaled.
* For the independent *t*-test, the data in the groups must be independent of each other.
* For the independent *t*-test, homogeneity of variance must be given. In practice, this assumption is not necessary, as an improved method is applied by default, which automatically corrects violations of this assumption ([Welch *t*-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)).


## Independent *t*-test in R

The *t*-statistic is calculated as the ratio of explained variance (column `Estimate`) to unexplained variance (column `Std. Error`). In the independent *t*-test, we compare the means of both conditions:

$$t = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\text{Standard error}}$$

The null hypothesis states that $\mu_1 = \mu_2$, so the equation simplifies to:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\text{Standard error}}$$

The standard error of the difference between both groups is defined as follows (when both groups have the same size):

$$\text{SE} = \sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}$$

Therefore, the formula for the *t*-test is:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}}$$

If the two groups are of different sizes, the standard error must be calculated using the pooled variance:

$$s_p^2 = \frac{(N_1 - 1) s_1^2 + (N_2 - 1) s_2^2}{N_1 + N_2 - 2}$$

The formula for the *t*-test with $N_1 + N_2 - 2$ degrees of freedom is then:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_p^2}{N_1} + \frac{s_p^2}{N_2}}}$$

Instead of fitting the model with `lm()`, we can use `t.test()`, whose output is often more familiar. Behind the scenes, it fits the same linear model to obtain the *t*-statistic, so the results are identical.

```{r}
(model = t.test(Anxiety ~ Group, data=spider))
```

The first argument is a formula with the outcome on the left side and the grouping factor on the right side. To use column names directly, we need to specify `data=spider` to indicate that these columns are contained in the `spider` data frame.

The output shows the value of the *t*-statistic, the degrees of freedom (usually corrected according to Welch, which makes the assumption of homogeneity of variance unnecessary), and the *p*-value. Furthermore, the 95% confidence interval for the *t*-statistic and the group means are also included.

Alternatively, instead of the formula interface, we can call `t.test()` with two separate vectors, one for each group's anxiety scores:

```{r}
(model = t.test(spider_w$picture, spider_w$real))
```

We can also calculate an effect size by converting the value of $t$ into a correlation $r$:

$$r = \sqrt{\frac{t^2}{t^2 + \text{df}}}$$

In this example, the effect size $r$ is:

```{r}
t = model$statistic[[1]]
df = model$parameter[[1]]
r = sqrt(t^2 / (t^2 + df))
round(r, 3)
```


## Dependent *t*-test in R

The dependent (or paired) *t*-test works similarly, but uses the means of the individual *differences* instead of the difference of the means:

$$t = \frac{\bar{D} - \mu_D}{s_D / \sqrt{N}}$$

Under the null hypothesis, $\mu_D = 0$:

$$t = \frac{\bar{D}}{s_D / \sqrt{N}}$$

In R, we can again use the `t.test()` function, but this time we set the argument `paired=TRUE`. In this case, we can only use the variant with two arguments (i.e., the data should be in wide format):

```{r}
(model = t.test(spider_w$picture, spider_w$real, paired=TRUE))
```

This time, the result is significant with $p=0.03098$. This corresponds to our considerations with the different confidence intervals of dependent and independent designs. Dependent designs can detect smaller differences!

Again, we can calculate the effect size by converting the $t$-value into a correlation:

```{r}
t = model$statistic[[1]]
df = model$parameter[[1]]
r = sqrt(t^2 / (t^2 + df))
round(r, 3)
```


## Outlook

In summary, an independent *t*-test is simply a linear regression model, containing a single predictor that separates two groups. Extending the idea, comparisons among more than two groups can be handled by adding dummy-coded predictors to the linear model. In classical analysis of variance, we focus on the *F*-statistic, which is merely the goodness-of-fit measure already reported in linear regression. Thus, ANOVA is just another special case of the general linear model. If you want the familiar ANOVA table in R, the `afex` package will print it for you.

When measurements are dependent because the same participants (or items) appear in several conditions, the assumption of independent errors is violated and ordinary linear models are no longer adequate. Linear mixed-effects models solve this by adding random effects for participants (and/or items), and they are increasingly preferred over the classical repeated-measures ANOVA. In R, these models are fitted with `lme4`, while companion packages such as `lmerTest` and `afex` supply *p*-values and ANOVA-style summaries.


## Exercises

### Exercise 1

The `dplyr` package contains the `starwars` dataset (load the package to access this dataset). Use this data frame to find out if female characters are significantly smaller than male characters (column `height`). Use the `gender` column to distinguish between `"feminine"` and `"masculine"` characters. Use a significance level of 5% and report the means of both groups.

In addition, compare the weight (column `mass`) of the characters (check the hypothesis that male characters weigh more than female characters). Report the means of both groups here as well.

:::{.callout-note}
Use the `alternative` argument of `t.test()` to perform a *one-sided* test.
:::


### Exercise 2

Compare the features `bill_length_mm` and `bill_depth_mm` in the `penguins` dataset (from the `palmerpenguins` package) to see if they differ between species (perform pairwise comparisons for each feature). Report the relevant statistics, significances, and effect sizes for each *t*-test.

The `pairwise.t.test()` function calculates multiple pairwise comparisons. Use this function (in addition to the individually calculated *t*-tests from before) to perform the comparisons. You can also correct the *p*-values with the `p.adjust.method` argument, as multiple tests increase the probability of a false positive result (the *t*-test says "there is a difference" even though there is actually no difference).


### Exercise 3

Load the `sleep` dataset (included by default in R). Do the two groups (`group`) differ in the number of additional hours of sleep (`extra`)? If so, what is the difference on average? Provide a 95% confidence interval.

:::{.callout-note}
Refer to the documentation to determine whether you need to calculate a dependent or independent *t*-test.
:::
