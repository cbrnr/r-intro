---
title: "6 – Lösungen"
subtitle: "Statistische Datenanalyse mit R"
author: "Clemens Brunner"
date: 2024-04-22
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
lang: de
author-title: "Autor"
published-title: "Veröffentlicht"
---

## Übung 1

```{r}
#| message: false
library(readr)
library(psych)
library(pastecs)
library(car)

df = read_delim("household_power_consumption.zip", delim=";", na=c("", "?"))

sapply(df[, 3:6], mean, na.rm=TRUE)
sapply(df[, 3:6], median, na.rm=TRUE)
sapply(df[, 3:6], min, na.rm=TRUE)
sapply(df[, 3:6], max, na.rm=TRUE)

summary(df[, 3:6])
describe(df[, 3:6])
round(stat.desc(df[, 3:6]), 1)
```

Aus den Ausgaben der zusammenfassenden Statistiken kann abgelesen werden, dass die mittlere Spannung 240.84 und der Median der globale Wirkleistung 0.602 beträgt. Die Funktion `summary()` zeigt außerdem an, dass es pro Spalte 25979 fehlende Werte gibt (unter `NA's`).


## Übung 2

```{r}
library(palmerpenguins)
dim(penguins)  # 344 Zeilen, 8 Spalten
summary(penguins)
by(penguins[, 3:5], penguins$species, colMeans, na.rm=TRUE)
```

Die Faktorspalten `species` und `island` haben jeweils drei Stufen, die Spalte `sex` hat zwei Stufen (wobei hier auch fehlende Werte vorkommen). Laut `summary(penguins)` gibt es in den Spalten 3–6 jeweils 2 fehlende Werte und in der 7. Spalte 11 fehlende Werte.


## Übung 3

```{r}
str(mtcars)
describe(mtcars)  # min, max, mean, median
shapiro.test(mtcars$mpg)
```

Die Nullhypothese der Normalverteilung kann *nicht* verworfen werden ($p = 0.123$).

```{r}
#| warning: false
leveneTest(mtcars$mpg, mtcars$cyl)
```

Die Nullhypothese der Varianzhomogenität kann verworfen werden ($p = 0.009$).


## Übung 4

```{r}
#| message: false
library(readr)

df = read_tsv("lecturer.dat")
```

Versuchen wir zunächst, die Mittelwerte der numerischen Spalten mit der Funktion `mean()` zu berechnen:

```{r}
by(df[, -(1:3)], df$job, mean)
```

Dies führt zu Warnungen sowie `NA` für die Ergebnisse beider Gruppen. Der Grund ist, dass die gruppierten Daten nach wie vor als Data Frame mit vier numerischen Spalten vorliegen. Die Funktion `mean()` funktioniert aber nur mit einem Vektor (bzw. einer einzigen Spalte eines Data Frames). Daher müssen wir eine andere Aggregierungsfunktion verwenden, die mit Data Frames umgehen kann. Eine Möglichkeit ist die Funktion `colMeans()`:

```{r}
by(df[, -(1:3)], df$job, colMeans)
```


## Übung 5

```{r}
#| error: true
set.seed(4)  # stellt sicher, dass die Zufallszahlen reproduzierbar sind
x = rnorm(5001)
shapiro.test(x)
```

Die Funktion `shapiro.test()` funktioniert nur bis zu einer Stichprobengröße von 5000, darüber hinaus wird ein Fehler ausgegeben. Für dieses Verhalten gibt es zwei Gründe:

1. Es ist generell nicht sinnvoll, eine sehr große Stichprobe auf Normalverteilung zu testen, da die Teststatistik selbst bei geringsten Abweichungen von der Normalverteilung sehr wahrscheinlich signifikant wird.
2. Theoretisch könnte der Shapiro-Wilk-Test auch für Stichproben größer als 5000 gerechnet werden, dies wurde in der ursprünglichen Publikation des Tests sowie dessen Implementierung in der Programmiersprache FORTRAN jedoch nicht berücksichtigt. Die Funktion `shapiro.wilk()` in R basiert auf dieser FORTRAN-Implementierung und ist daher ebenfalls auf Stichproben bis zu einer Größe von 5000 beschränkt.

Eine detaillierte Erklärung von [Ben Bolker](https://math.mcmaster.ca/~bolker/) ist [hier](https://stats.stackexchange.com/a/506528/53514) zu finden.
