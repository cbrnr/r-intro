---
title: "6 – Deskriptive Statistiken"
subtitle: "Statistische Datenanalyse mit R"
author: "Clemens Brunner"
date: 2022-11-10
format: html
engine: knitr
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
lang: de
author-title: "Autor"
published-title: "Veröffentlicht"
---

## Allgemeines

Nachdem man eine Datei korrekt importiert hat (d.h. wenn die Daten in einem Data Frame bzw. Tibble vorhanden sind und alle Spalten den passenden Datentyp haben), kann man mit der statistischen Analyse beginnen. Der erste Schritt dabei ist meist, sich mit deskriptiven Statistiken eine Beschreibung der vorhandenen Daten zu erzeugen.

Als Beispiel dazu importieren wir die Datei [`lecturer.dat`](lecturer.dat) aus der letzten Übung:

```{r}
#| message: false
library(readr)
(df = read_tsv("lecturer.dat"))
```

Danach führen wir wieder die Konvertierungen für die Spalten `birth_date` und `job` durch:

```{r}
df$birth_date = as.Date(df$birth_date, format="%m/%d/%Y")
df$job = factor(df$job, levels=c(1, 2), labels=c("Lecturer", "Student"))
df
```

Die `name`-Spalte brauchen wir für unsere nachfolgenden Betrachtungen nicht mehr, deswegen entfernen wir sie:

```{r}
df$name = NULL
```

Es gibt in R eine Reihe an Funktionen, welche zusammenfassende Statistiken einer Variable (bzw. eines Vektors) berechnen. Nützliche Funktionen sind z.B. `mean()`, `sd()`, `var()`, `min()`, `max()`, `median()`, `range()` und `quantile()`. Den Mittelwert eines Vektors (und damit auch einer Spalte von `df`) kann man also wie folgt berechnen:

```{r}
mean(df$friends)
```

Vor allem bei Faktoren ist es interessant zu wissen, wie viele Stufen es im Datensatz gibt. Auch bei numerischen oder Text-Vektoren mit mehrfach vorkommenden Werten ist es manchmal praktisch, die unterschiedlichen Werte herauszufinden. Dafür gibt es die Funktion `unique()`, welche die eindeutigen Elemente eines Vektors zurückgibt:

```{r}
unique(df$friends)
unique(df$job)
```

All diese Berechnungen müsste man nun für jede interessierende Spalte wiederholen (weil diese nur mit Vektoren funktionieren), was relativ mühsam wäre. Deswegen gibt es die Funktion `sapply()`, welche eine Funktion auf jede Spalte eines Data Frames einzeln anwendet. Möchte man also den Mittelwert für jede numerische Spalte von `df` berechnen, kann man dies so tun:

```{r}
sapply(df[, -c(1, 2)], mean)
```

:::{.callout-note}
Für die Berechnung der Spaltenmittelwerte kann man auch die uns bereits bekannte Funktion `colMeans()` verwenden.
:::

Beachten Sie, dass `df[, -c(1, 2)]` alle Spalten aus `df` ausgenommen der ersten und zweiten bedeutet. So kann man jede beliebige Funktion auf einzelne Spalten anwenden, z.B. die Standardabweichung:

```{r}
sapply(df[, -c(1, 2)], sd)
```

Es gibt aber auch spezielle Funktionen, welche direkt *mehrere* statistische Kenngrößen für alle Spalten eines Data Frames berechnen. Im Folgenden gehen wir näher auf vier dieser Funktionen ein, nämlich `summary()`, `describe()` und `stat.desc()`. Nur `summary()` ist standardmäßig bei R dabei, die anderen zwei Funktionen erfordern die Installation von zusätzlichen Paketen.


### Die Funktion `summary()`

Die Funktion `summary()` liefert eine geeignete Zusammenfassung für jede Spalte eines Data Frames (Tibbles). Numerische Spalten sowie Datumsspalten werden mit sechs Werten beschrieben: Minimum, 1. Quartil, 2. Quartil (Median), 3. Quartil, Maximum sowie Mittelwert. Für Faktoren werden die Stufen sowie die Anzahl an Fällen pro Stufe aufgelistet.

```{r}
summary(df)
```

Auch bei anderen Objekten als Data Frames liefert `summary()` oft eine sinnvolle kurze Beschreibung.


### Die Funktion `describe()`

Eine weitere Möglichkeit, noch mehr statistische Kenngrößen für numerische Spalten auszugeben, bietet die Funktion `describe()` aus dem `psych`-Paket. Nicht-numerische Spalten werden hier nicht vernünftig zusammengefasst, deshalb sollte man der Funktion nur numerische Spalten übergeben.

```{r}
library(psych)
describe(df[, 3:6])
```

:::{.callout-note}
Eigentlich kann man Funktionen aus Paketen auch ohne Aktivierung verwenden. Dazu muss man den Paketnamen gefolgt von `::` voranstellen. Im vorigen Beispiel könnte man also auf `library(psych)` verzichten und die Funktion mit `psych::describe()` trotzdem verwenden.
:::

Man kann diese Funktion auch auf einzelne Gruppen separat anwenden. Im Beispiel könnte man dies getrennt für jede Stufe von `df$job` tun. Dazu verwendet man die verwandte Funktion `describeBy()`:

```{r}
describeBy(df[, c("friends", "alcohol", "income", "neurotic")], df$job)
```

Das erste Argument ist das zu beschreibende Data Frame, und das zweite Argument ist die Spalte, nach der gruppiert werden soll (üblicherweise ein Faktor).


### Die Funktion `stat.desc()`

Das Paket `pastecs` beinhaltet die Funktion `stat.desc()` zur Beschreibung von Daten. Mit der Funktion `round()` sollte man hier außerdem einstellen, wie viele Kommastellen ausgegeben werden sollen, da die Ausgabe der Funktion sonst relativ unübersichtlich ist. Wenn das Argument `norm=TRUE` gesetzt wird, werden für alle Spalten Tests auf Normalverteilung durchgeführt.

```{r}
library(pastecs)
round(
    stat.desc(df[, c("friends", "alcohol", "income", "neurotic")], norm=TRUE),
    digits=2
)
```


### Gruppieren mit `by()`

Für die Funktion `stat.desc()` gibt es keine direkte Variante für gruppierte Daten (so wie `describeBy()` für `describe()`). Es gibt aber in R die generische Funktion `by()`, welche beliebige Funktionen auf gruppierte Daten anwendet. Das erste Argument ist hier wie üblich der Datensatz, das zweite Argument ist die Gruppierungsspalte, und das dritte Argument ist die Funktion, die auf die gruppierten Daten angewendet werden soll.

```{r}
by(df[, c("friends", "alcohol", "income", "neurotic")], df$job, describe)
```

Möchte man der Funktion im dritten Argument selbst Argumente übergeben (z.B. `norm=TRUE` für die Funktion `stat.desc()`), kann man dies mit weiteren Argumenten ganz am Ende tun:

```{r}
by(
    df[, c("friends", "alcohol", "income", "neurotic")],
    df$job,
    stat.desc,
    norm=TRUE
)
by(df[, -c(1, 2)], df$job, summary)
by(df$friends, df$job, mean)
```


## Test auf Normalverteilung

Die Funktion `stat.desc()` liefert bereits das Ergebnis des [Shapiro-Wilk-Tests](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) auf Normalverteilung (die Einträge `normtest.W` und `normtest.p` enthalten den Wert der Teststatistik bzw. die Signifikanz). Wenn `normtest.p` signifikant ist (z.B. kleiner als 0.05), dann kann man die Nullhypothese der Normalverteilung verwerfen. Man kann den Shapiro-Wilk-Test auch direkt mit der Funktion `shapiro.test()` aufrufen:

```{r}
shapiro.test(df$income)
```

Mit der `by()`-Funktion kann man den Test auch getrennt auf verschiedene Gruppen anwenden.

```{r}
by(df$income, df$job, shapiro.test)
```

Der [Kolmogorov-Smirnov-Test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) kann gegebene Daten auf *beliebige* Verteilungen testen, d.h. natürlich auch auf Normalverteilung. Im Falle der Normalverteilung ist aber der Shapiro-Wilk-Test vorzuziehen, da dieser speziell auf die Normalverteilung zugeschnitten ist und daher mehr statistische Power besitzt.

```{r}
ks.test(df$income, "pnorm", mean(df$income), sd(df$income))
```

Da die Stichprobengröße in unserem Beispiel nur sehr klein ist, lassen sich aber ohnehin keine vernünftigen Aussagen über die Verteilung der Daten treffen.


## Test auf Varianzhomogenität

Der Levene-Test prüft auf Gleichheit der Varianzen (Homoskedastizität) von zwei oder mehr Gruppen. Die Nullhypothese ist, dass die Varianzen in allen Gruppen gleich sind. In R führt man den Test mit der Funktion `leveneTest()` aus dem Paket `car` durch. Dazu sehen wir uns die Beispieldaten `Moore` an, welche mit dem Paket `car` automatisch geladen werden.

```{r message=FALSE}
library(car)
?Moore
head(Moore, 4)
tail(Moore, 4)
summary(Moore)
```

Der Levene-Test für die Spalte `conformity` gruppiert nach der Spalte `fcategory` wird wie folgt aufgerufen:

```{r}
leveneTest(Moore$conformity, Moore$fcategory)
```

In diesem Beispiel kann die Nullhypothese der Varianzgleichheit der Variable `conformity` über die Gruppen in `fcategory` also nicht verworfen werden, da der *p*-Wert (hier als `Pr(>F)` bezeichnet) mit 0.9551 extrem groß ist.


## Übungen

### Übung 1

Berechnen Sie statistische Kenngrößen wie Mittelwert, Median, Minimum und Maximum für die vier numerischen Variablen `Global_active_power`, `Global_reactive_power`, `Voltage` und `Global_intensity` aus den Daten die Sie in der vorigen Einheit bereits importiert haben ([Individual Household Electric Power Consumption](http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)).

-   Berechnen Sie die Kenngrößen mit der Funktion `sapply()`.
-   Berechnen Sie die Kenngrößen mit der Funktion `summary()`.
-   Berechnen Sie die Kenngrößen mit der Funktion `describe()` aus dem Paket `psych`.
-   Wenden Sie die Funktion `stat.desc()` aus dem Paket `pastecs` auf die Daten an (runden Sie die Ergebnisse auf *eine* Nachkommastelle).
-   Wie groß ist die gemessene mittlere Spannung `Voltage`?
-   Wie groß ist der Median der globalen Wirkleistung `Global_active_power`?
-   Wie viele fehlende Werte gibt es insgesamt bzw. pro Spalte?


### Übung 2

Das Paket `palmerpenguins` beinhaltet im Tibble `penguins` Messdaten über physische Eigenschaften von drei Pinguinarten (Adélie, Chinstrap und Gentoo). Aktivieren Sie das Paket und beantworten Sie mit geeigneten R-Befehlen folgende Fragen:

-   Aus wie vielen Zeilen und Spalten besteht der Datensatz `penguins`?
-   Die Spalten `species`, `island` und `sex` sind Faktoren – wie viele Stufen gibt es jeweils?
-   Berechnen Sie deskriptive Statistiken getrennt für die drei Pinguinspezies! Wie lauten die Mittelwerte der Spalten `bill_length_mm`, `bill_depth_mm` und `flipper_length_mm` für die drei Spezies?
-   Gibt es fehlende Werte in den Daten?


### Übung 3

Im Datensatz `mtcars` befinden sich diverse Kenngrößen von 32 Autos.

-   Berechnen Sie das Minimum und Maximum sowie den Mittelwert und den Median von allen Spalten.
-   Überprüfen Sie, ob die Daten in der Spalte `mpg` normalverteilt sind.
-   Überprüfen Sie, ob Varianzhomogenität für die Daten in der Spalte `mpg` gegeben ist für die drei Gruppen, die durch die Spalte `cyl` definiert sind.
