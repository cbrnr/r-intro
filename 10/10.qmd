---
title: "10 – Lineare Regression"
subtitle: "Statistische Datenanalyse mit R"
author: "Clemens Brunner"
date: 2024-06-03
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
lang: de
author-title: "Autor"
published-title: "Veröffentlicht"
---

## Einfache lineare Regression

Wir haben gesehen, dass man über die Korrelation die Beziehung zwischen zwei Variablen beschreiben kann. Man kann nun einen Schritt weiter gehen und versuchen, eine Variable durch die andere "vorherzusagen". Eine weit verbreitete Methode dafür ist die lineare Regression, welche eine abhängige Variable durch *eine* unabhängige Variable (einfache Regression) bzw. durch *mehrere* unabhängige Variablen (multiple Regression) zu erklären versucht.

Ein allgemeines statistisches Modell kann man generell wie folgt aufstellen:

$$\mathrm{outcome}_i = \mathrm{model}_i + \mathrm{error}_i$$

Man beschreibt also die gemessenen Daten durch ein Modell, welches im Allgemeinen aber immer Fehler machen wird (d.h. es beschreibt die gemessenen Daten nicht perfekt). Um diese Formel kürzer anschreiben zu können, setzt man $\mathrm{outcome}_i = y_i$, $\mathrm{model}_i = \hat{y}_i$ und $\mathrm{error}_i = \varepsilon_i$:

$$y_i = \hat{y}_i + \varepsilon_i$$

Im Fall der linearen Regression ist das Modell $\hat{y}_i$ linear, also eine Gerade. Die Gleichung kann daher wie folgt angeschrieben werden:

$$y_i = \underbrace{\left(b_0 + b_1 x_i\right)}_{\hat{y}_i} + \varepsilon_i$$

:::{.callout-note}
Eine Geradengleichung kann in der Form $y = k \cdot x + d$ angeschrieben werden. Die zwei Parameter $k$ und $d$ sind wie folgt definiert:

- Der Schnittpunkt mit der y-Achse $d$ entspricht dem Wert von $y$ an der Stelle $x=0$.
- Die Steigung $k$ ist das Verhältnis der Änderung von $y$ (wird als $\Delta y$ geschrieben) zur Änderung von $x$ (wird als $\Delta x$ geschrieben):

   $$k = \frac{\Delta y}{\Delta x}$$
:::

Die Variablen $b_0$ und $b_1$ beschreiben den Schnittpunkt mit der y-Achse (Intercept) bzw. die Steigung der Geraden und werden als *Regressionskoeffizienten* bezeichnet. Der Term $\varepsilon$ beschreibt den Fehler zwischen den vom Modell vorhergesagten Werten und den tatsächlich gemessenen Werten. Der Term $x$ wird als unabhängige Variable, Prädiktor oder Treatment bezeichnet. Der Term $y$ wird als abhängige Variable oder Outcome bezeichnet. Das tiefgestellte $i$ steht für den $i$-ten Datenpunkt, d.h. $y_i$ ist der zugehörige Wert für den $i$-ten Datenpunkt $x_i$. Das Modell macht für diesen Datenpunkt den Fehler $\varepsilon_i$. Den vom Modell vorhergesagten Wert bezeichnet man als $\hat{y}_i$.

:::{.callout-tip}
Zur Veranschaulichung sind in der folgenden Abbildung drei Geraden mit gleichen Intercepts aber unterschiedlichen Steigungen (links) sowie unterschiedlichen Intercepts aber gleichen Steigungen (rechts) abgebildet.

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 5
library(ggplot2)
library(patchwork)

theme_set(theme_minimal())

p1 = ggplot(data.frame(x=0, y=0), aes(x, y)) +
    scale_x_continuous(limits=c(0, 100)) +
    scale_y_continuous(limits=c(0, 100)) +
    labs(x="x", y="y") +
    geom_abline(slope=(1/1), intercept=50) +
    geom_abline(slope=(-1/3), intercept=50) +
    geom_abline(slope=(-2/3), intercept=50) +
    theme(aspect.ratio=1)
p2 = ggplot(data.frame(x=0, y=0), aes(x, y)) +
    scale_x_continuous(limits=c(0, 100)) +
    scale_y_continuous(limits=c(0, 100)) +
    labs(x="x", y="y") +
    geom_abline(slope=(1/2), intercept=50) +
    geom_abline(slope=(1/2), intercept=20) +
    geom_abline(slope=(1/2), intercept=70) +
    theme(aspect.ratio=1)
p1 + p1
```

In der linken Abbildung haben alle drei Geraden denselben Intercept $b_0 = 50$ (sie schneiden die y-Achse an der Stelle $y = 50$). Die Steigung der ersten Geraden beträgt $b_1 = 1$. Dies ist aus der Grafik ersichtlich, denn wenn man $x$ beispielsweise um 25 erhöht, dann erhöht sich $y$ ebenfalls um 25. Die Steigung beträgt also $b_1 = \frac{\Delta y}{\Delta x} = \frac{25}{25} = 1$. Die Steigung der zweiten Geraden beträgt $b_1 = -\frac{1}{3}$, denn wenn man $x$ um 75 erhöht, dann *erniedrigt* sich $y$ um 25. Die Steigung beträgt also $b_1 = \frac{-25}{75} = -\frac{1}{3}$. Die Steigung der dritten Geraden ist entsprechend $b_1 = \frac{-50}{75} = -\frac{2}{3}$.

In der rechten Abbildung haben alle drei Geraden die Steigung $b_1 = \frac{1}{2}$. Die erste Gerade schneidet die y-Achse bei $y = 70$, die zweite bei $y = 50$ und die dritte bei $y = 20$, was den drei Intercepts entspricht.
:::

Das Modell (also die Regressionsgerade) soll die Daten möglichst gut beschreiben, was meist durch die [Methode der kleinsten Fehlerquadrate (Least Squares)](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) erreicht wird. Diese Methode findet jene Gerade, welche die quadrierten Unterschiede zwischen dem Modell (der Geraden) und den einzelnen Datenpunkten minimiert. Die Unterschiede (Fehler) werden hier als *Residuen* bezeichnet. In der folgenden Grafik sind die Daten als schwarze Punkte, das Modell als schwarze Gerade und die Residuen als rote vertikale Linien dargestellt. Beachten Sie, dass es sowohl positive als auch negative Residuen gibt (in der Grafik sind beispielhaft zwei Werte zu sehen). Damit sich die positiven und negativen Terme nicht aufheben, werden die einzelnen Residuen zuerst quadriert und erst dann summiert – diese Quadratsumme wird minimiert (es wird also die Gerade gesucht, welche die kleinste Quadratsumme ergibt). Die von dieser Methode gefundene Gerade hat also die kleinste Fehlerquadratsumme unter allen möglichen Geraden.

```{r}
#| echo: false
set.seed(2)
x = rnorm(15, mean=12, sd=7)
y = 5 * x + rnorm(15, sd=20)
df = data.frame(x=x, y=y)
m = lm(y ~ x)
yhat = m$fitted.values
ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=yhat, color="error")) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    geom_point(size=2) +
    theme(legend.position="none") +
    scale_x_continuous(breaks=0:30, minor_breaks=NULL) +
    scale_y_continuous(breaks=seq(-20, 170, 20)) +
    annotate("text", x=27, y=155, label="28.5", color="red") +
    annotate("text", x=6.8, y=5, label="\u201345.8", color="red")
```


## Quadratsummen

Die Fehlerquadratsumme ist ein Maß für die Güte des Modells (Model Fit). Für die gefundene Gerade ist diese Fehlerquadratsumme zwar immer minimal (unter allen möglichen Geraden), es ist aber trotzdem nicht klar, wie gut sich die Daten überhaupt mit einer Gerade beschreiben lassen. Deswegen vergleicht man das lineare Modell mit dem einfachsten Modell, welches die Prädiktorvariable vollkommen ignoriert – dem Mittelwert über alle Datenpunkte $y_i$ (das entspricht einer waagrechten Geraden). Bei diesem Modell ist der Wert der Prädiktorvariable $x_i$ also vollkommen egal, da komplett unabhängig davon immer derselbe Wert für die abhängige Variable $y_i$ vorhergesagt wird. Das lineare Modell ist dann ein guter Fit, wenn es signifikant besser als dieses einfachste Modell ist.


Die Summe der quadratischen Abweichungen vom einfachsten Modell (Mittelwert) wird auch als SST bezeichnet (totale Quadratsumme). Die Summe der quadratischen Abweichungen vom linearen Modell wird als SSR (Residuenquadratsumme) bezeichnet, denn das lineare Modell macht im Allgemeinen Fehler (geht nicht perfekt durch alle Datenpunkte). Schließlich gibt es noch die quadratischen Abweichungen des linearen Modells vom einfachsten Modell, welche man als SSM (Modellquadratsumme) bezeichnet.

```{r}
#| echo: false
#| fig-width: 15
set.seed(2)
x = rnorm(15, mean=12, sd=7)
y = 5 * x + rnorm(15, sd=20)
df = data.frame(x=x, y=y)
m = lm(y ~ x)
yhat = m$fitted.values
p_sst = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=mean(y), color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    theme(legend.position="none", plot.title=element_text(hjust=0.5)) +
    ggtitle("SST")

p_ssm = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=mean(y), yend=yhat, color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    theme(legend.position="none", plot.title=element_text(hjust=0.5), axis.title.y=element_blank(), axis.text.y=element_blank()) +
    ggtitle("SSM")

p_ssr = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=yhat, color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    theme(legend.position="none", plot.title=element_text(hjust=0.5), axis.title.y=element_blank(), axis.text.y=element_blank()) +
    ggtitle("SSR")

p_sst + p_ssm + p_ssr
```

Die vorangegangenen drei Abbildungen veranschaulichen diese Quadratsummen. SST ist hier nichts anderes als die (nicht normierte) *Varianz* der Daten. SSM sollte möglichst groß sein, denn dann ist das lineare Modell wesentlich besser als der globale Mittelwert. SSR sollte möglichst klein sein bei einem guten Modell – an SSR sieht man die übrig gebliebene Varianz, die das Modell nicht erklären kann.

In mathematischer Notation sieht dies wie folgt aus:

$$\mathrm{SST} = \sum_{i=1}^N (y_i - \bar{y})^2$$

$$\mathrm{SSR} = \sum_{i=1}^N (y_i - \hat{y}_i)^2$$

$$\mathrm{SSM} = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2$$

Hier sind $y_i$ die einzelnen Messwerte, $\bar{y}$ ist der Mittelwert und $\hat{y}_i$ sind die einzelnen vom Modell vorhergesagten Werte (also die auf der Geraden liegenden Werte). $N$ ist die Anzahl der Datenpunkte. In Formeln kann man $\bar{y}$ und $\hat{y}_i$ so schreiben:

$$\bar{y} = \frac{1}{N} \sum_{i=1}^N y_i$$

$$\hat{y}_i = b_0 + b_1 x_i$$

Es gilt:

$$\mathrm{SST} = \mathrm{SSM} + \mathrm{SSR}$$


## Modellgüte

### Bestimmtheitsmaß $R^2$

Ein Maß für die Modellgüte ist das Verhältnis von SSM zu SST, welches als $R^2$ bezeichnet wird. Es ist der Anteil an Varianz der Daten, die vom Modell erklärt werden kann:

$$R^2 = \frac{\mathrm{SSM}}{\mathrm{SST}}$$

Dies ist derselbe Wert, den wir auch schon bei der Korrelation kennengelernt haben. Um die Pearson-Korrelation zwischen den beiden Variablen zu bekommen, muss man also nur die Wurzel aus $R^2$ ziehen. Diese Beziehung gilt in dieser Form allerdings nur für die einfache lineare Regression, also nur bei einer einzigen unabhängigen Variable.


### $F$-Wert

Eine weitere wichtige Anwendung der Quadratsummen ist die Berechnung des $F$-Wertes. Dies ist das Verhältnis der systematischen Varianz (also die vom Modell erklärten Varianz) zur unsystematischen Varianz (also die Varianz, die nicht vom Modell erklärt werden kann). Für den $F$-Wert verwendet man aber nicht direkt SSM und SSR, sondern die *mittleren* Quadratsummen. Hier dividiert man die Quadratsummen durch die jeweiligen Freiheitsgrade und erhält so MSM und MSR. Die Anzahl der Freiheitsgrade von MSM entspricht der Anzahl der geschätzten Modellparameter $p$ minus 1. Die Anzahl der Freiheitsgrade von MSR entspricht der Anzahl der Messwerte $N$ minus der Anzahl der geschätzten Modellparameter $p$.

$$\mathrm{MSM} = \frac{\mathrm{SSM}}{\mathrm{dfM}} = \frac{\mathrm{SSM}}{p - 1}$$

$$\mathrm{MSR} = \frac{\mathrm{SSR}}{\mathrm{dfR}} = \frac{\mathrm{SSR}}{N - p}$$

Für die einfache Regression gibt es genau zwei Modellparameter $b_0$ und $b_1$, daher ist $p = 2$. Für die Freiheitsgrade der Modellquadratsumme gilt daher:

$$\mathrm{dfM} = p - 1 = 2 - 1 = 1$$

Die Freiheitsgrade der Residuenquadratsumme sind in diesem Fall dann

$$\mathrm{dfR} = N - p = N - 2.$$

Der $F$-Wert kann dann wie folgt berechnet werden:
$$F=\frac{\mathrm{MSM}}{\mathrm{MSR}}$$

Dieser Wert ist ebenso wie $R^2$ ein Maß für die Güte des Modells. Er gibt an, wie viel Varianz das Modell erklärt gegenüber wie viel Varianz das Modell nicht erklärt. D.h. ein Wert von 1 entspricht dem Fall, dass die erklärte Varianz gleich groß ist wie die nicht erklärte – also ein schlechtes Modell. Für ein gutes Modell sollte daher $F \gg 1$ gelten.


## Koeffizienten

In der einfachen linearen Regression entspricht der Regressionskoeffizient $b_1$ der Steigung der Geraden. Er entspricht der Änderung der abhängigen Variable (AV) relativ zu einer Änderung der unabhängigen Variable (UV) um eine Einheit. Ein schlechtes Modell (wie der Mittelwert) sagt immer denselben Wert für die AV vorher, unabhängig vom Wert der UV. Die Steigung $b_1$ ist für so ein Modell also Null. Wenn aber die UV den Wert der AV vorhersagen kann, muss die Steigung signifikant von Null verschieden sein. Diese Hypothese kann man mit dem sogenannten $t$-Test überprüfen. Ein $t$-Test kann also in der linearen Regression eingesetzt werden, um zu beurteilen, ob eine UV ein signifikanter Prädiktor für die AV ist.

Die $t$-Statistik vergleicht das Modell mit seinem Fehler; konkret möchte man hier wissen, ob der beobachtete Wert des Regressionskoeffizienten groß gegenüber seinem Standardfehler ist:

$$t = \frac{b}{\mathrm{SE}_b}$$

Die Freiheitsgrade dieser Statistik sind $N-p$, also im Fall der einfachen linearen Regression $N-2$.


## Beispiel

Wie man eine Regressionsanalyse in R durchführt lässt sich am besten anhand eines Beispiels zeigen. Laden wir einen Datensatz [`sales1.dat`](sales1.dat), welcher Daten über Musikalbenverkäufe (Spalte `sales`) und die Höhe des Werbebudgets (Spalte `adverts`) enthält:

```{r}
#| message: false
library(readr)
(album = read_tsv("sales1.dat"))
```

Es ist anzunehmen, dass ein höheres Werbebudget zu höheren Verkaufszahlen führt. Neben der Berechnung diverser deskriptiver Statistiken (wird hier nicht durchgeführt) ist es wichtig, die Daten vor einer Regressionsanalyse grafisch darzustellen. Hier bietet sich ein Scatterplot mit überlagerter Regressionsgerade an (das Argument von `abline()`, nämlich `lm(sales ~ adverts, data=album)`, ist das Regressionsmodell und wird gleich erklärt):

```{r}
plot(
    x=album$adverts,
    y=album$sales,
    pch=21,
    bg=rgb(0, 0, 0, 0.5),
    xlab="Adverts (1000 EUR)",
    ylab="Sales (1000)"
)
abline(lm(sales ~ adverts, data=album), col="blue", lwd=2)
```

Es ist klar, dass eine positive Beziehung zwischen den beiden Variablen besteht (je größer das Werbebudget desto mehr Albenverkäufe). Außerdem ist die Steigung der Regressionsgeraden stark verschieden von Null, d.h. es ist zu erwarten, dass das Regressionsmodell signifikant ist.

In R kann man mit der Funktion `lm()` (steht für "linear model") eine Regressionsanalyse durchführen:

```{r}
model = lm(sales ~ adverts, data=album)
```

Das erste Argument `sales ~ adverts` ist eine Formel (welche durch eine Tilde `~` gekennzeichnet ist). Diese Formel kann man als "`sales` wird vorhergesagt durch `adverts`" lesen. Im allgemeinen nimmt die Formel die Form `AV ~ UV` an. Mit dem Argument `data=album` teilt man der Funktion mit, dass sich die Namen in der Formel auf Spaltennamen des Data Frames `album` beziehen.

Das Ergebnis der Regressionsanalyse weisen wir hier der Variablen `model` zu. Eine kompakte Darstellung des Ergebnisses kann man sich ausgeben lassen, indem man sich eine Zusammenfassung des Modells mittels `summary()` ansieht:

```{r}
summary(model)
```

Beginnen wir mit der vorletzten Zeile:

```
Multiple R-squared:  0.3346,    Adjusted R-squared:  0.3313
```

Hier sehen wir $R^2$, also das Verhältnis von SSM zu SST. Daraus können wir schließen, dass die Werbeausgaben ca. 33.5% der Varianz der Albenverkäufe erklären können. Dies bedeutet natürlich, dass ca. 66.5% der Varianz unerklärt ist, d.h. es muss andere relevante Faktoren dafür geben, die wir nicht im Modell berücksichtigt haben.

Im Fall der einfachen linearen Regression können wir auch sofort die Pearson-Korrelation zwischen den beiden Variablen berechnen, indem wir die Wurzel aus $R^2$ ziehen:

```{r}
sqrt(summary(model)$r.squared)
```

Diesen Wert könnte man jetzt vergleichen mit jenem aus der direkten Berechnung der Korrelation (z.B. mit `cor()` oder `cor.test()`) – das Ergebnis ist identisch:
```{r}
cor(album$adverts, album$sales)
```

Die letzte Zeile der Modellzusammenfassung zeigt die $F$-Statistik und deren Signifikanz:

```
F-statistic: 99.59 on 1 and 198 DF,  p-value: < 2.2e-16
```

Die $F$-Statistik ist das Verhältnis von MSM zu MSR. Der Wert liegt bei 99.59, was bei Freiheitsgraden 1 und 198 einer Signifikanz von $p<0.001$ entspricht. Dies bedeutet, dass die Wahrscheinlichkeit kleiner als 0.1% ist, diesen $F$-Wert (oder einen noch größeren) unter der Nullhypothese (das Modell unterscheidet sich nicht vom einfachen Mittelwertsmodell) zu erhalten. Wir können also daraus schließen, dass das lineare Modell signifikant besser als das einfachste Modell ist.

Der $F$-Wert bedeutet also, dass das Modell insgesamt ein guter Fit der Daten ist (verglichen mit dem globalen Mittelwert). Es wird aber eigentlich keine Aussage über die individuellen Prädiktoren getroffen (wobei man im Fall der einfachen Regression natürlich darauf schließen kann, dass die einzige Variable dann ebenfalls ein guter Prädiktor ist). In der Zusammenfassung sind die Regressionskoeffizienten wie folgt ersichtlich:

```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 1.341e+02  7.537e+00  17.799   <2e-16 ***
adverts     9.612e-02  9.632e-03   9.979   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

In der Zeile `(Intercept)` listet die Spalte `Estimate` den Wert für $b_0$, also die Albenverkäufe wenn das Werbebudget gleich 0 ist (Schnittpunkt mit der y-Achse). Dieser Wert beträgt `1.341e+02`, also 134100 Verkäufe (weil die Maßeinheit in 1000 Einheiten ist).

Der Wert für $b_1$, also die Steigung der Geraden, ist in der Zeile `adverts` mit `9.612e-02`, also 0.09612 abzulesen. Das bedeutet, wenn die Prädiktorvariable `adverts` sich um eine Einheit ändert, dann ändert sich die Ergebnisvariable `sales` um 0.09612 Einheiten. Das heißt wenn man das Werbebudget um 1000€ erhöht, verkauft man 96 Alben mehr.

Weiters sieht man die Standardfehler der Koeffizienten sowie deren $t$-Werte. Die letzte Spalte `Pr(>|t|)` gibt die $p$-Werte an inklusive Codes für signifikante Ergebnisse. Beide Koeffizienten sind signifikant mit $p<0.001$, wobei uns hier eigentlich nur die Steigung der Geraden interessiert und es für uns nicht wichtig ist, ob der Intercept signifikant von Null verschieden ist.


## Werte vorhersagen

Das Modell `model` kann man nun verwenden, um neue Werte für `sales` in Abhängigkeit von `adverts` vorherzusagen. Hierfür muss man einfach die berechneten Werte von $b_0$ und $b_1$ in das lineare Modell einsetzen. Die exakten Koeffizienten erhält man mit `model$coefficients` oder `coefficients(model)`.

$$\hat{y} = b_0 + b_1 x = 134.1 + 0.09612 \cdot x$$

Nun kann man berechnen, wie hoch die Verkäufe $y$ wären, wenn man ein Werbebudget von $x=100$ hätte:

$$\hat{y} = 134.1 + 0.09612 \cdot 100 = 143.75$$

D.h. man würde 143750 Alben verkaufen bei einem Werbebudget von 100000 EUR.

Einfacher und allgemeiner kann man aber die Funktion `predict()` verwenden. Als Argumente gibt man das Modell und die neuen Daten an (welche als Data Frame übergeben werden müssen):

```{r}
predict(model, data.frame(adverts=100))
```

So kann man auch gleich Vorhersagen für mehrere Werte gleichzeitig berechnen:

```{r}
predict(model, data.frame(adverts=c(0, 10, 100, 2000)))
```


## Multiple lineare Regression

Die multiple lineare Regression ist eine Erweiterung der einfachen linearen Regression auf Situationen mit mehreren Prädiktoren. Das grundlegende Konzept bleibt aber unverändert, wir verwenden nach wie vor folgende allgemeine Modellgleichung:

$$y_i = \hat{y}_i + \varepsilon_i$$

Das Modell mit $n$ Prädiktoren $x_{1}$, $x_{2}$, $\ldots$, $x_{n}$ wird nun wie folgt formuliert:

$$y_i = (b_0 + b_1 x_{1i} + b_2 x_{2i} + \ldots + b_n x_{ni}) + \varepsilon_i$$

Jeder Prädiktor erhält also ein eigenes Gewicht bzw. einen eigenen Regressionskoeffizienten. Die Koeffizienten werden mittels Least Squares wieder so berechnet, dass die entstehende "Gerade" (technisch spricht man hier von einer [Hyperebene](https://de.wikipedia.org/wiki/Hyperebene)) den mittleren quadratischen Fehler minimiert.

Die Quadratsummen SST, SSM und SSR werden analog wie bei der einfachen Regression berechnet. Wieder kann man $R^2$ berechnen, welches den Anteil der Varianz in der abhängigen Variable angibt, welche durch das Modell erklärt wird. Je höher dieser Wert ist, desto besser kann das Modell die Daten beschreiben. Im Gegensatz zur einfachen Regression kann man aus $R^2$ nicht die Pearson-Korrelation zwischen den einzelnen Variablen berechnen, sondern die Korrelation zwischen den *vorhergesagten* Werten und den tatsächlich *beobachteten* Werten.


## Überprüfen der Datenpunkte

Ein gegebenes Modell sollte man immer darauf überprüfen, wie gut es die gemessenen Daten tatsächlich beschreibt. Lineare Modelle können sehr sensitiv auf einzelne Datenpunkte reagieren, welche nicht zum generellen Trend der Daten passen. Einen tatsächlichen Einfluss auf das Modell üben solche Ausreißer aber nur aus, wenn sie weit weg vom Mittelwert der Prädiktoren liegen – man bezeichnet diesen potentiellen Einfluss jedes Datenpunkts als *Leverage*.

Kritisch sind also jene Punkte, welche einen potentiellen hohen Einfluss haben (hohe Leverage) und gleichzeitig nicht zum generellen Trend der Daten passen, also Ausreißer sind. Die folgende Grafik veranschaulicht die drei möglichen Situationen. Dabei sind die vier Ausgangsdatenpunkte schwarz dargestellt und die zugehörige Regressionsgerade ist schwarz strichliert. Der zusätzliche fünfte Datenpunkt ist rot dargestellt, und die Regressionsgerade durch alle fünf Datenpunkte ist ebenfalls rot. Durch einen einzigen zusätzlichen Datenpunkt ändert sich also das ursprüngliche Modell mehr oder weniger stark (von schwarz strichliert auf rot durchgezogen).

```{r}
#| echo: false
x = c(5:8)
y = x + rnorm(length(x), 0, 0.1)
l = lm(y ~ x)

# low leverage outlier (= uninfluential)
x1 = 6.5
y1 = 3
l1 = lm(c(y, y1) ~ c(x, x1))

# high leverage non-outlier (= uninfluential)
x2 = 12
y2 = 11.8
l2 = lm(c(y, y2) ~ c(x, x2))

# high leverage outlier (= influential)
x3 = 14
y3 = 8
l3 = lm(c(y, y3) ~ c(x, x3))

par(mfrow=c(1, 3), mar=c(1, 1, 1, 1))
plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x1, y1, pch=19, col="red")
abline(l1, col="red")
title("low leverage outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x2, y2, pch=19, col="red")
abline(l2, col="red")
title("high leverage non-outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x3, y3, pch=19, col="red")
abline(l3, col="red")
title("high leverage outlier")
```

Je weniger Datenpunkte vorhanden sind, desto größer ist der Einfluss von Ausreißern auf das lineare Modell. Die folgende Grafik veranschaulicht die Auswirkung eines Ausreißers für drei verschiedene Stichprobengrößen. Man erkennt, dass ein Ausreißer das Modell nur wenig verändert, wenn sehr viele Datenpunkte vorhanden sind. Wenn es aber nur wenige Datenpunkte gibt, kann ein einziger Ausreißer das Modell stark verändern.

```{r}
#| echo: false
outlier = function(n=100, alpha=0.5)
{
    x = runif(n, 5, 15)
    y = x + rnorm(length(x), 0, 0.2)
    l = lm(y ~ x)
    plot(x, y, pch=19, col=rgb(0, 0, 0, alpha), xlim=c(5, 20), ylim=c(5, 20), axes=FALSE, xlab="", ylab="")
    abline(l, lty=2)

    x_o = 20
    y_o = 5
    points(x_o, y_o, pch=19, col="red")
    l_o = lm(c(y, y_o) ~ c(x, x_o))
    abline(l_o, col="red")

    mtext(bquote(italic(n) == .(n)), side=1, family="serif")
}

par(mfrow=c(1, 3), mar=c(1, 3, 1, 3))
outlier(1000, alpha=0.15)
outlier(100)
outlier(10, alpha=0.8)
```


## Modellannahmen

Um mit einem linearen Regressionsmodell Vorhersagen auf ungesehene Daten machen zu können, müssen folgende Annahmen erfüllt sein:

* Die abhängige Variable muss intervallskaliert sein.
* Die unabhängigen Variablen (Prädiktoren) müssen intervallskaliert (oder nominalskaliert in zwei Kategorien) sein.
* Die Prädiktoren müssen Varianzen ungleich 0 haben.
* Es darf keine Multikollinearität bestehen, d.h. zwei oder mehrere Prädiktoren dürfen nicht linear voneinander abhängig sein. Dies kann z.B. mit der VIF-Statistik (Variance Inflation Factor) überprüft werden.
* Homoskedastizität, d.h. die Varianz der Residuen muss konstant über die Werte der Prädiktoren sein (Varianzhomogenität).
* Die Residuen müssen normalverteilt sein.
    
  :::{.callout-important}
  Diese Voraussetzung der Normalverteilung gilt für die *Residuen* und *nicht* für die Prädiktoren!
  :::

* Die Residuen müssen unabhängig voneinander sein (kann z.B. mit dem Durbin-Watson-Test überprüft werden).
* Die Beziehung zwischen unabhängigen Variablen und abhängiger Variable muss linear sein.


## Beispiel

Im folgenden Beispiel sehen wir uns wieder die Anzahl der Verkäufe von Musikalben in Abhängigkeit der Höhe des Werbebudgets an. Zusätzlich gibt es jetzt aber zwei weitere Prädiktoren, nämlich die Anzahl an Airplay-Stunden im größten nationalen Radiosender und die Attraktivität der Bandmitglieder. Wir beginnen mit dem Laden der Daten [`sales2.dat`](sales2.dat):

```{r}
#| message: false
library(readr)
album2 = read_tsv("sales2.dat")
```

Berechnen wir nun ein lineares Regressionsmodell. Als Vergleichsmodell führen wir zuerst eine einfache Regression mit dem einzigen Prädiktor Werbebudget durch:

```{r}
model1 = lm(sales ~ adverts, data=album2)
```

Zusätzliche Faktoren kann man nun in einem zweiten Modell einfach durch den `+`-Operator hinzufügen:

```{r}
model2 = lm(sales ~ adverts + airplay + attract, data=album2)
```

Anschließend können wir uns die zusammengefassten Ergebnisse der beiden Modelle anzeigen lassen:

```{r}
summary(model1)
summary(model2)
```

Da das erste Modell identisch mit jenem aus der vorigen Einheit ist, kennen wir die Ergebnisse bereits. Wenden wir uns daher dem zweiten Modell zu. $R^2$ ist hier 0.6647, das heißt das Modell kann nun 66% der Varianz erklären. Im Vergleich zum ersten Modell mit nur einem Prädiktor ist das eine Steigerung um 33%, d.h. die beiden Prädiktoren Airplay und Attraktivität können zusätzliche Varianz im Ausmaß von 33% erklären.

Die Regressionskoeffizienten werden ebenfalls in der Ausgabe dargestellt. Wir können daher das lineare Modell wie folgt schreiben:

$$y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 = -26.613 + 0.085 \cdot x_1 + 3.367 \cdot x_2 + 11.086 \cdot x_3$$

Hier stehen $b_0$ für den Intercept (d.h. jener Wert von $y$, wenn alle Prädiktoren 0 sind), $b_1$ für das Werbebudget `adverts`, $b_2$ für die Airplay-Stunden `airplay` und $b_3$ für die Attraktivität `attract`.

Die Regressionskoeffizienten geben Auskunft, um wie viel sich die abhängige Variable ändert, wenn man einen Prädiktor um eine Einheit erhöht und dabei alle anderen Prädiktoren konstant hält. Dies bedeutet im Beispiel:

* Wenn `adverts` um eine Einheit erhöht wird, dann erhöht sich `sales` um 0.085 Einheiten. D.h. wenn man 1000 EUR mehr für Werbung ausgibt, verkauft man um 85 Alben mehr.
* Wenn `airplay` um eine Einheit erhöht wird, dann erhöht sich `sales` um 3.37 Einheiten. D.h. wenn man das Album um eine Stunde mehr im Radio spielt, verkauft man um 3367 Alben mehr.
* Wenn `attract` um eine Einheit erhöht wird, dann erhöht sich `sales` um 11.086 Einheiten (d.h. 11086 zusätzliche Alben).

Für jeden Regressionskoeffizienten wird ein $t$-Test gerechnet, welcher angibt, ob sich der Koeffizient signifikant von 0 unterscheidet (d.h. ob er signifikant zum Modell beiträgt). Die Größe der $t$-Statistik lässt auf den Einfluss der Koeffizienten schließen, d.h. `adverts` und `airplay` haben einen ähnlich großen Einfluss auf das Modell, wo hingegen `attract` einen geringeren Einfluss hat.

Oft ist es hilfreich, nicht nur die Regressionskoeffizienten zu analysieren, sondern auch die standardisierten Regressionskoeffizienten. Diese kann man berechnen, in dem man zuerst alle Variablen standardisiert und danach das lineare Modell berechnet. Standardisierte Variablen haben einen Mittelwert von 0 und eine Standardabweichung von 1. Man könnte die Standardisierung daher relativ einfach selbst vornehmen, in dem man für jede Variable zuerst deren Mittelwert subtrahiert und danach durch deren Standardabweichung dividiert.

:::{.callout-tip}
Die Funktion `scale()` kann verwendet werden, um die Spalten eine Data Frames zu standardisieren. Diese liefert aber immer eine Matrix zurück, d.h. wenn man ein Data Frame bzw. Tibble standardisieren möchte, muss man danach noch `as.data.frame()` bzw. `tibble::as_tibble()` anwenden.
:::

Alternativ kann man dazu auch *nachträglich* die Funktion `lm.beta()` aus dem `lm.beta`-Paket benutzen:

```{r}
library(lm.beta)
lm.beta(model2)
```

Die standardisierten Regressionskoeffizienten werden üblicherweise mit $\beta_i$ bezeichnet. Da alle Variablen nun in Standardabweichungen gemessen werden, kann man diese direkt miteinander vergleichen. Man sieht im Beispiel also:

* Wenn `adverts` um eine Standardabweichung erhöht wird (485655 EUR), dann erhöht sich `sales` um 0.511 Standardabweichungen (41240 Alben).
* Wenn `airplay` um eine Standardabweichung erhöht wird (12.270), dann erhöht sich `sales` um 0.512 Standardabweichungen (41320 Alben).
* Wenn `attract` um eine Standardabweichung erhöht wird (1.395), dann erhöht sich `sales` um 0.192 Standardabweichungen (15490 Alben).

Konfidenzintervalle für die (nicht standardisierten) Regressionskoeffizienten erhält man mit der Funktion `confint()` (standardmäßig werden 95%-Intervalle berechnet):

```{r}
confint(model2)
```

Zwei (oder mehrere) Modelle können mit der $F$-Statistik verglichen werden. Der $F$-Wert, der bei der zusammenfassenden Beschreibung eines Modells angezeigt wird, vergleicht das Modell standardmäßig mit dem einfachsten Mittelwertmodell. Möchte man das Modell mit einem anderen Modell vergleichen, ist zu beachten, dass `model2` eine Erweiterung von `model1` sein muss, d.h. `model2` muss alle Terme von `model1` beinhalten plus eventuelle zusätzliche Faktoren. In R gibt man hier folgenden Befehl ein:

```{r}
anova(model1, model2)
```

Der $F$-Wert beträgt also 96.447 und ist signifikant, d.h. das zweite Modell ist signifikant besser als das erste.


### Datenpunkte mit großem Einfluss

Um die diversen Ausreißerstatistiken für jeden einzelnen Wert übersichtlich beurteilen zu können, kann man die Werte mit folgenden Funktionen berechnen:

* `resid()`: Residuen
* `rstandard()`: Standardisierte Residuen
* `rstudent()`: Studentisierte Residuen (berechnet mit Leave-One-Out)
* `hatvalues()`: Leverage
* `dfbeta()`: Unterschied der Regressionskoeffizienten mittels Leave-One-Out
* `cooks.distance()`: Cook's Distanz
* `dffits()`: Unterschied im vorhergesagtem Wert mittels Leave-One-Out

Sehr praktisch ist die Funktion `influence.measures()`, welche mehrere Ausreißerstatistiken für jeden Datenpunkt übersichtlich aufbereitet ausgibt.

```r
influence.measures(model2)
```


### Modellannahmen

Mulitkollinearität kann mit der VIF-Statistik beurteilt werden; in R kann man dazu die Funktion `vif()` aus dem `car`-Paket verwenden.

```{r}
#| message: false
library(car)
vif(model2)
```

Der größte VIF-Wert sollte nicht größer als 10 sein. Der durchschnittliche VIF sollte nicht wesentlich größer als 1 sein, was man wie folgt überprüfen kann:

```{r}
mean(vif(model2))
```

Wenn man den `plot()`-Befehl auf das Modell anwendet, werden vier diagnostische Plots erstellt.

```{r}
#| fig-width: 12
#| fig-height: 8
par(mfrow=c(2, 2), cex=0.75)
plot(model2)
```

Im Plot links oben sind die vorhergesagten Werte gegen die Residuen aufgetragen. Hier kann man die Linearitätsannahme (die rote Linie sollte immer ungefähr gleich Null sein) sowie die Homoskedastizitätsannahme (die Streuung der Datenpunkte sollte sich entlang der x-Achse nicht ändern) überprüfen. Der Plot links unten ist ähnlich, nur ist hier statt den (absoluten) Residuen die Wurzel aus dem Betrag der standardisierten Residuen aufgetragen. Auch hier lässt sich beurteilen, ob die Annahme der Varianzhomogenität erfüllt ist oder nicht. Im Plot rechts oben lässt sich die Normalverteilungsannahme der Residuen mit einem QQ-Plot überprüfen. Im Plot rechts unten sind Punkte mit großem Einfluss dargestellt (gemessen an der Leverage); Cook's Distanz ist ebenfalls im Plot ersichtlich.

Die Annahme über die Unabhängigkeit der Residuen kann mit dem Durbin-Watson-Test `dwt()` überprüft werden.

```{r}
dwt(model2)
```

In diesem Beispiel kann man davon ausgehen, dass die Residuen unabhängig sind, da wegen $p\approx 0.7$ die Nullhypothese nicht verworfen werden kann.


## Übungen

### Übung 1

Die Datei [`cars.csv`](cars.csv) enthält mehrere Messwerte für den Bremsweg (`dist`) von Autos die bei einer bestimmten Geschwindigkeit (`speed`) bremsen. Wir wollen untersuchen, ob es einen (linearen) Zusammenhang zwischen der Geschwindigkeit und dem Bremsweg gibt. Stellen Sie zunächst beide Variablen in einem Scatterplot dar (`speed` auf der *x*-Achse und `dist` auf der *y*-Achse).


### Übung 2

Führen Sie mit den Daten aus Übung 1 eine lineare Regressionsanalyse durch und stellen Sie die Ergebnisse zusammengefasst dar. Erwähnen Sie die wichtigsten Eckpunkte Ihres Regressionsmodells.


### Übung 3

Berechnen Sie die Pearson-Korrelation zwischen den beiden Variablen (ohne Verwendung von $R^2$ aus dem Modell) und überprüfen Sie, ob dieser Wert mit $R^2$ aus dem Regressionsmodell übereinstimmt.


### Übung 4

Wie lautet die Gleichung der Regressionsgeraden? Welche Bremswege sagt das Modell für Geschwindigkeiten von 5 bzw. 65 voraus? Verwenden Sie für die Vorhersage sowohl die Geradengleichung als auch die Funktion `predict()`.


### Übung 5

Laden Sie die Daten aus der Datei `sales2.dat` wie in den Unterlagen gezeigt. Standardisieren Sie danach alle Variablen und berechnen Sie dann ein lineares Regressionsmodell. Vergleichen Sie die Regressionskoeffizienten mit den Ergebnissen der Funktion `lm.beta()`, welche auf ein Modell mit nicht standardisierten Daten angewendet werden kann.


### Übung 6

Laden Sie den Datensatz [`aggression.dat`](aggression.dat), welcher (komplett erfundene) Daten über Aggressionen unter Kindern enthält. Es wurden 666 Kinder untersucht und folgende Variablen erhoben:

- Erziehungsstil (hoher Wert entspricht schlechtem Stil)
- Computerspielen (hoher Wert entspricht viel Computerspielen)
- Fernsehen (hoher Wert entspricht viel Fernsehen)
- Ernährung (hoher Wert entspricht gesunder Ernährung)
- Aggressionen der Geschwister (hoher Wert entspricht hoher Aggression)

Von früheren Studien weiß man, dass Erziehungsstil sowie Aggressionen der Geschwister signifikante Prädiktoren für das Aggressionslevel eines Kindes sind.

Stellen Sie zwei lineare Regressionsmodelle auf. Das erste soll nur die beiden Faktoren beinhalten, welche erwiesenermaßen einen Einfluss auf die Aggression haben. Das zweite Modell soll alle Faktoren beinhalten. Beantworten Sie anschließend folgende Punkte:

1. Bestimmen Sie für beide Modelle das Bestimmtheitsmaß $R^2$ und geben Sie die Tabelle der Regressionskoeffizienten aus.
2. Interpretieren Sie für beide Modelle getrennt die einzelnen Koeffizienten hinsichtlich Relevanz (hier sind standardisierte Koeffizienten hilfreich) und Signifikanz.
3. Vergleichen Sie beide Modelle miteinander. Ist das zweite Modell eine signifikante Verbesserung zum ersten?


### Übung 7

Überprüfen Sie für das zweite Modell (mit allen Prädiktoren) aus der vorigen Übung folgende Voraussetzungen:

- Sind die unabhängigen Variablen kollinear (VIF)?
- Sind die Residuen unabhängig (Durbin-Watson-Test)?
- Sind die Residuen normalverteilt (QQ-Plot)?
- Sind die Abhängigkeiten linear und ist die Varianz homogen (Plot Residuen vs. vorhergesagte Werte)?
- Gibt es Datenpunkte mit großem Einfluss auf das Modell (Plot Residuen vs. Leverage)?

*Hinweis:* Sehen Sie sich die Hilfe zur Funktion `plot.lm()` an (damit können Sie ein lineares Modell plotten und die Grafiken aus den letzten drei Fragen erstellen; mit dem Argument `which` können Sie sich die gewünschte Grafik herauspicken).
