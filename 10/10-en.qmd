---
title: "10 – Linear Regression"
subtitle: "Statistical Data Analysis with R"
author: "Clemens Brunner"
date: 2024-12-12
format:
  html:
    page-layout: full
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
highlight-style: github
title-block-banner: true
theme:
  light: flatly
  dark: darkly
---

## Simple linear regression

We have seen that correlation can describe the relationship between two variables. We can now take a step further and try to "predict" one variable from the other. A widely used method for this task is regression, which tries to explain a dependent variable by *one* independent variable (simple regression) or by *multiple* independent variables (multiple regression).

Let's start with the most general form of a statistical model:

$$\mathrm{outcome}_i = \mathrm{model}_i + \mathrm{error}_i$$

This equation uses a model to describe the measured data (the outcome), but the model is not perfect and generally does not describe the data exactly (it makes errors). We can write this equation more concisely by setting $y_i = \mathrm{outcome}_i$, $\hat{y}_i = \mathrm{model}_i$, and $\varepsilon_i = \mathrm{error}_i$:

$$y_i = \hat{y}_i + \varepsilon_i$$

In linear regression, the model $\hat{y}_i$ is a *straight line*, which is the simplest possible model. Despite its simplicity, this linear approach is incredibly powerful and widely applicable. The model equation can therefore be written as follows:

$$y_i = \underbrace{\left(b_0 + b_1 x_i\right)}_{\hat{y}_i} + \varepsilon_i$$

::: {.callout-note}
A straight line can be written as a linear equation $y = k \cdot x + d$. The following figure illustrates how to visually identify these two parameters for any given line.

1. The intercept $d$ is the intersection of the line with the y-axis. For this example, the line intersects the y-axis at $d=3$.
2. The slope $k$ describes the rate of change in the y-direction relative to changes in the x-direction. To calculate the slope, we observe how much the line rises or falls for a given horizontal movement, or in other words, $k = \frac{\Delta y}{\Delta x}$. In this case, the slope is $k = \frac{\Delta y}{\Delta x} = \frac{-0.5}{1} = - 0.5$, meaning that for every unit increase in $x$ (corresponding to $\Delta x = 1$), the value of $y$ decreases by 0.5 units (corresponding to $\Delta y = -0.5$):

![](line.png){width=50%}

Therefore, this line can be written as $y = -\frac{1}{2} \cdot x + 3$.
:::

The model coefficients $b_0$ and $b_1$ correspond to the intercept and the slope of the line, respectively. In the context of regression, these variables are called *regression coefficients*. The term $\varepsilon$ describes the error between the values predicted by the model and the actual measured values. The term $x$ is called the independent variable, predictor, or treatment. The term $y$ is called the dependent variable or outcome. The subscript $i$ denotes the $i$-th data point, i.e., $y_i$ is the value associated with the $i$-th data point $x_i$. The model makes an error $\varepsilon_i$ for this data point. The value predicted by the model is denoted as $\hat{y}_i$.

::: {.callout-tip}
To illustrate different intercepts and slopes, consider the following two figures. The left figure shows three lines with the same intercept but different slopes, while the right figure shows three lines with different intercepts but the same slope.

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 5
library(ggplot2)
library(patchwork)

theme_set(theme_minimal())

p1 = ggplot(data.frame(x=0, y=0), aes(x, y)) +
    scale_x_continuous(limits=c(0, 100)) +
    scale_y_continuous(limits=c(0, 100)) +
    labs(x="x", y="y") +
    geom_abline(slope=(1/1), intercept=50) +
    geom_abline(slope=(-1/3), intercept=50) +
    geom_abline(slope=(-2/3), intercept=50) +
    theme(aspect.ratio=1)
p2 = ggplot(data.frame(x=0, y=0), aes(x, y)) +
    scale_x_continuous(limits=c(0, 100)) +
    scale_y_continuous(limits=c(0, 100)) +
    labs(x="x", y="y") +
    geom_abline(slope=(1/2), intercept=50) +
    geom_abline(slope=(1/2), intercept=20) +
    geom_abline(slope=(1/2), intercept=70) +
    theme(aspect.ratio=1)
p1 + p2
```

In the left figure, all three lines have the same intercept $b_0 = 50$ (they intersect the y-axis at $y = 50$). By visual inspection, the slopes of the lines are $b_1 = 1$, $b_1 = -\frac{1}{3}$, and $b_1 = -\frac{2}{3}$, respectively. In the right figure, all three lines have the same slope $b_1 = \frac{1}{2}$. The first line intersects the y-axis at $y = 70$, the second at $y = 50$, and the third at $y = 20$, corresponding to the three intercepts.
:::

We are looking for a model (i.e., a straight line) that best describes the data, which is usually achieved by the [method of least squares](https://en.wikipedia.org/wiki/Least_squares). This method finds the line that minimizes the squared differences between the model (the line) and the individual data points. These differences (errors) are called *residuals*. In the following figure, the data points are shown as black dots, the model as a black line, and the residuals as red vertical lines. Note that there are both positive and negative residuals (two examples are shown in the figure). To prevent positive and negative terms from canceling each other out, the individual residuals are first squared and then summed. The method of least squares finds the line that minimizes this sum of squared errors among all possible lines.

```{r}
#| echo: false
set.seed(2)
x = rnorm(15, mean=12, sd=7)
y = 5 * x + rnorm(15, sd=20)
df = data.frame(x=x, y=y)
m = lm(y ~ x)
yhat = m$fitted.values
ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=yhat, color="error")) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    geom_point(size=2) +
    theme(legend.position="none") +
    scale_x_continuous(breaks=0:30, minor_breaks=NULL) +
    scale_y_continuous(breaks=seq(-20, 170, 20)) +
    annotate("text", x=27, y=155, label="28.5", color="red") +
    annotate("text", x=6.8, y=5, label="\u201345.8", color="red")
```


## Sum of squares

The sum of squared residuals measures how well a model fits the data. While least squares *always* finds the line that minimizes this sum, it does not guarantee that the model describes the data well. To assess this, we compare the linear model to a baseline model that ignores the predictor variable entirely – using the mean of all $y_i$ values as its prediction (a horizontal line). In this baseline model, the predictor $x_i$ has no effect; the predicted $y_i$ is always the same. The linear model is considered a good fit if it significantly outperforms this simple baseline model.

The sum of squared deviations of the data points from the baseline (mean) is called the *total sum of squares* (SST). The sum of squared deviations of the data points from the linear model are known as the *residual sum of squares* (SSR). Lastly, the sum of squared deviations of the linear model's predictions from the baseline model are referred to as the *model sum of squares* (SSM).

```{r}
#| echo: false
#| fig-width: 15
set.seed(2)
x = rnorm(15, mean=12, sd=7)
y = 5 * x + rnorm(15, sd=20)
df = data.frame(x=x, y=y)
m = lm(y ~ x)
yhat = m$fitted.values
p_sst = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=mean(y), color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    theme(legend.position="none", plot.title=element_text(hjust=0.5)) +
    ggtitle("SST")

p_ssm = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=mean(y), yend=yhat, color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    theme(legend.position="none", plot.title=element_text(hjust=0.5), axis.title.y=element_blank(), axis.text.y=element_blank()) +
    ggtitle("SSM")

p_ssr = ggplot(df, aes(x, y)) +
    geom_segment(aes(x=x, xend=x, y=y, yend=yhat, color="error")) +
    geom_point() +
    geom_hline(yintercept=mean(y), linewidth=0.75) +
    geom_smooth(method=lm, formula=y ~ x, se=FALSE, color="black") +
    theme(legend.position="none", plot.title=element_text(hjust=0.5), axis.title.y=element_blank(), axis.text.y=element_blank()) +
    ggtitle("SSR")

p_sst + p_ssm + p_ssr
```

The previous figure illustrates these sums of squares. The left panel shows the total sum of squares (SST), representing the *variance* of the data. The middle panel displays the model sum of squares (SSM), which should be as large as possible to indicate that the linear model significantly outperforms the baseline. The right panel shows the residual sum of squares (SSR), which should be as small as possible for a good model, as it represents the remaining variance that the model fails to explain.

Mathematically, these sums are defined as follows:

$$\mathrm{SST} = \sum_{i=1}^N (y_i - \bar{y})^2$$

$$\mathrm{SSR} = \sum_{i=1}^N (y_i - \hat{y}_i)^2$$

$$\mathrm{SSM} = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2$$

Here, $y_i$ are the individual data points, $\bar{y}$ is the mean, and $\hat{y}_i$ are the individual predicted values ($N$ represents the number of data points). Moreover, $\bar{y}$ and $\hat{y}_i$ can be written as:

$$\bar{y} = \frac{1}{N} \sum_{i=1}^N y_i$$

$$\hat{y}_i = b_0 + b_1 x_i$$

The following relationship holds:

$$\mathrm{SST} = \mathrm{SSM} + \mathrm{SSR}$$


## Model fit

### Coefficient of determination $R^2$

The ratio of SSM to SST is a popular measure of model fit, known as $R^2$, the coefficient of determination. It represents the proportion of variance in the data that the model can explain:

$$R^2 = \frac{\mathrm{SSM}}{\mathrm{SST}}$$

This value ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates that the model explains all variance. This is the same value we encountered when discussing correlation. To obtain the Pearson correlation $r$ between the two variables $x$ and $y$, we simply take the square root of $R^2$. However, this relationship only holds for simple linear regression, i.e., for a single independent variable.


### $F$ value

We can use the sum of squares to calculate another important model statistic, the so-called $F$ value. This value is the ratio of the systematic variance (i.e., the variance explained by the model) to the unsystematic variance (i.e., the variance that cannot be explained by the model). However, we do not use SSM and SSR directly for the $F$ value, but rather the *mean* sum of squares. Here, we divide the sum of squares by the respective degrees of freedom to obtain MSM and MSR. The degrees of freedom of MSM correspond to the number of estimated model parameters $p$ minus 1. The degrees of freedom of MSR correspond to the number of observations $N$ minus the number of estimated model parameters $p$.

$$\mathrm{MSM} = \frac{\mathrm{SSM}}{\mathrm{dfM}} = \frac{\mathrm{SSM}}{p - 1}$$

$$\mathrm{MSR} = \frac{\mathrm{SSR}}{\mathrm{dfR}} = \frac{\mathrm{SSR}}{N - p}$$

For simple regression, there are exactly two model parameters $b_0$ and $b_1$, so $p = 2$. The degrees of freedom for the model sum of squares are therefore:

$$\mathrm{dfM} = p - 1 = 2 - 1 = 1$$

The degrees of freedom for the residual sum of squares are in this case:

$$\mathrm{dfR} = N - p = N - 2.$$

We can then calculate $F$ as follows:

$$F=\frac{\mathrm{MSM}}{\mathrm{MSR}}$$

::: {.callout-tip}
Since $F$ depends on two degrees of freedom, we typically make this explicit as $F(\mathrm{dfM}, \mathrm{dfR})$.
:::

Like $R^2$, the $F$ value is a measure of model fit. It indicates how much variance the model explains compared to how much variance the model cannot explain. A value of $F=1$ corresponds to the case where the explained variance is equal to the unexplained variance, which corresponds to a bad model. For a good model, we should have $F \gg 1$.


## Coefficients

In simple linear regression, the regression coefficient $b_1$ corresponds to the slope of the regression line. It represents the change in the dependent variable (DV) relative to a unit change in the independent variable (IV). A poor model (like the global mean) always predicts the same value for the DV, regardless of the IV. The slope $b_1$ is therefore zero for such a model. However, if the IV can predict the value of the DV, the slope must be significantly different from zero. This hypothesis can be tested using the so-called $t$-test. In linear regression, a $t$-test can be used to assess whether an IV is a significant predictor of the DV.

The $t$-statistic compares the model to its error; specifically, it tests whether the observed value of the regression coefficient is large relative to its standard error:

$$t = \frac{b}{\mathrm{SE}_b}$$

The degrees of freedom for this statistic are $N - p$, which corresponds to $N - 2$ in the case of simple linear regression.


## Example (simple linear regression)

Let's work through an example to demonstrate how to perform a regression analysis in R. We will load a dataset [`sales1.dat`](sales1.dat) containing data on music album sales (column `sales`) and the amount of advertising budget (column `adverts`) per album:

```{r}
#| message: false
library(readr)
(album = read_tsv("sales1.dat"))
```

We hypothesize that a higher advertising budget leads to higher sales. Besides calculating various descriptive statistics (not done here), it is essential to visualize the data before performing a regression analysis. We will use scatter plot with an overlaid regression line (we'll explain the argument of `abline()`, namely `lm(sales ~ adverts, data=album)`, shortly):

```{r}
plot(
    x=album$adverts,
    y=album$sales,
    pch=21,
    bg=rgb(0, 0, 0, 0.5),
    xlab="Adverts (1000 EUR)",
    ylab="Sales (1000)"
)
abline(lm(sales ~ adverts, data=album), col="blue", lwd=2)
```

Clearly, there is a positive relationship between the two variables (the higher the advertising budget, the more album sales). Moreover, the slope of the regression line is significantly different from zero, indicating that the regression model is likely to be a good fit.

We can perform a regression analysis in R using the `lm()` function (which stands for "linear model"):

```{r}
model = lm(sales ~ adverts, data=album)
```

The first argument `sales ~ adverts` is a formula (indicated by a tilde `~`). This formula can be read as "`sales` is predicted by `adverts`" (in general, the formula takes the form `DV ~ IV`). With the argument `data=album`, we tell the function that the names in the formula refer to columns of the data frame `album`.

We store the result of the regression analysis in the variable `model`. We can obtain a summary of the regression analysis by calling `summary()` on this variable:

```{r}
summary(model)
```

Let's interpret the output, starting with the penultimate line:

```
Multiple R-squared:  0.3346,    Adjusted R-squared:  0.3313
```

This line shows $R^2$, the ratio of SSM to SST (there are actually two different estimates for $R^2$, but we will not discuss the details here, also because they are almost identical anyway). From this value, we can infer that advertising expenses can explain approximately 33.5% of the variance in album sales. This means that approximately 66.5% of the variance remains unexplained, indicating that there must be other relevant factors that we have not considered in the model.

In the case of simple linear regression, we can also immediately calculate the Pearson correlation between advertising budget and album sales by taking the square root of $R^2$:

```{r}
sqrt(summary(model)$r.squared)
```

Let's compare this value with the result computed by the `cor()` function:

```{r}
cor(album$adverts, album$sales)
```

The last line of the model summary shows the $F$ statistic and its significance (the $p$-value):

```
F-statistic: 99.59 on 1 and 198 DF,  p-value: < 2.2e-16
```

The $F$ statistic is the ratio of MSM to MSR. Its value is 99.59, which corresponds to a significance of $p < 0.001$ with 1 and 198 degrees of freedom, respectively. This means that the probability of obtaining this $F$ value (or a larger one) under the null hypothesis (the model does not differ from the baseline mean model) is less than 0.1%. We can therefore conclude that the linear model is significantly better than the baseline model.

The $F$ statistic indicates that the model is a good fit for the data (compared to the global mean). However, it does not actually make any statements about the individual predictors (although in the case of simple regression, there is only one variable, so one can infer that this variable is a good predictor). The regression coefficients are summarized as follows:

```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 1.341e+02  7.537e+00  17.799   <2e-16 ***
adverts     9.612e-02  9.632e-03   9.979   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

The row `(Intercept)` shows the value for $b_0$, which corresponds to album sales when the advertising budget is zero. This value is `1.341e+02`, or `134.1`, which translates to 134,100 sales, as the variable is measured in thousands of units.

The value for $b_1$ (the slope of the regression line) is listed in the row `adverts` as `9.612e-02`, or `0.09612`. This means that for every one-unit increase in the predictor variable `adverts`, the outcome variable `sales` increases by 0.09612 units. In practical terms, increasing the advertising budget by €1,000 results in 96 additional album sales.

Furthermore, the table shows the standard errors of the coefficients, the $t$-values, and the $p$-values. Both coefficients are significant with $p < 0.001$. However, we are usually only interested in the slope, as it is not important whether the intercept differs significantly from zero.


## Predicting values

We can now use our model to predict new values for `sales` based on `adverts`. To do this, we simply insert the calculated values of $b_0$ and $b_1$ into the linear model. The exact coefficients can be obtained with `model$coefficients` or `coefficients(model)`:

$$\hat{y} = b_0 + b_1 x = 134.1 + 0.09612 \cdot x$$

Now we can calculate the sales $y$ if we had an advertising budget of $x = 100$:

$$\hat{y} = 134.1 + 0.09612 \cdot 100 = 143.75$$

This means that with an advertising budget of €100,000, we would sell 143,750 albums.

However, it is easier and more convenient to use the `predict()` function. As arguments, we pass the model and the new data (which must be passed as a data frame):

```{r}
predict(model, data.frame(adverts=100))
```

This way, we can also make predictions for multiple values at once:

```{r}
predict(model, data.frame(adverts=c(0, 10, 100, 2000)))
```


## Multiple linear regression

Multiple linear regression is an extension of simple linear regression to situations with multiple predictors. The basic concept remains the same; we still use the general model equation:

$$y_i = \hat{y}_i + \varepsilon_i$$

The model with $n$ predictors $x_{1}$, $x_{2}$, $\ldots$, $x_{n}$ is now written as follows:

$$y_i = (b_0 + b_1 x_{1i} + b_2 x_{2i} + \ldots + b_n x_{ni}) + \varepsilon_i$$

Each predictor has its own weight or regression coefficient. Again, we use least squares to calculate the coefficients in such a way that the resulting "line" (technically, a [hyperplane](https://en.wikipedia.org/wiki/Hyperplane)) minimizes the mean squared error.

We compute the sum of squares SST, SSM, and SSR in the same way as in simple regression. We can also calculate $R^2$, which indicates the proportion of variance in the dependent variable that is explained by the model. The higher this value, the better the model can describe the data. However, unlike simple regression, we cannot calculate the Pearson correlation between the individual variables from $R^2$. Instead, by taking the square root, we can calculate the correlation between the *predicted* values and the *observed* values.


## Outliers and leverage

We should always check how well a given model describes the measured data. Linear models can be very sensitive to individual data points that do not fit the general trend of the data. However, such outliers only have an actual impact on the model if they are far from the mean of the predictors. This potential influence of each data point is called *leverage*.

The most critical data points are those that have high leverage and do not fit the general trend of the data. The following figure illustrates three possible situations. The four original data points are shown in black, and the corresponding regression line is shown as a black dashed line. The additional fifth data point is shown in red, and the regression line through all five data points is also shown in red. Adding a single data point therefore changes the original model more or less (from black dashed to red solid).

```{r}
#| echo: false
x = c(5:8)
y = x + rnorm(length(x), 0, 0.1)
l = lm(y ~ x)

# low leverage outlier (= uninfluential)
x1 = 6.5
y1 = 3
l1 = lm(c(y, y1) ~ c(x, x1))

# high leverage non-outlier (= uninfluential)
x2 = 12
y2 = 11.8
l2 = lm(c(y, y2) ~ c(x, x2))

# high leverage outlier (= influential)
x3 = 14
y3 = 8
l3 = lm(c(y, y3) ~ c(x, x3))

par(mfrow=c(1, 3), mar=c(1, 1, 1, 1))
plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x1, y1, pch=19, col="red")
abline(l1, col="red")
title("low leverage outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x2, y2, pch=19, col="red")
abline(l2, col="red")
title("high leverage non-outlier")

plot(x, y, pch=19, col=rgb(0, 0, 0, 1), xlim=c(2, 14), ylim=c(2, 14), xaxt="n", yaxt="n", ann=FALSE, bty="n")
abline(l, lty=2)
points(x3, y3, pch=19, col="red")
abline(l3, col="red")
title("high leverage outlier")
```

The fewer data points are available, the greater the influence of outliers on the linear model. The following figure illustrates the effect of a single outlier for three different sample sizes. If there are many data points, the outlier does not really influence the model significantly (left panel). However, the smaller the sample size, the greater the influence of the outlier (middle and right panels).

```{r}
#| echo: false
outlier = function(n=100, alpha=0.5)
{
    x = runif(n, 5, 15)
    y = x + rnorm(length(x), 0, 0.2)
    l = lm(y ~ x)
    plot(x, y, pch=19, col=rgb(0, 0, 0, alpha), xlim=c(5, 20), ylim=c(5, 20), axes=FALSE, xlab="", ylab="")
    abline(l, lty=2)

    x_o = 20
    y_o = 5
    points(x_o, y_o, pch=19, col="red")
    l_o = lm(c(y, y_o) ~ c(x, x_o))
    abline(l_o, col="red")

    mtext(bquote(italic(n) == .(n)), side=1, family="serif")
}

par(mfrow=c(1, 3), mar=c(1, 3, 1, 3))
outlier(1000, alpha=0.15)
outlier(100)
outlier(10, alpha=0.8)
```


## Model assumptions

In order to use a linear model to make predictions on unseen data, the following assumptions must be met:

* The dependent variable must be continuous (interval scale).
* The independent variables (predictors) must be continuous (or on a nominal scale with two categories).
* The predictors must have non-zero variances.
* Predictors must not be linearly dependent (multicollinearity). This can be checked, for example, with the VIF statistic (variance inflation factor).
* Homoscedasticity, i.e., the variance of the residuals must be constant across the values of the predictors (homogeneity of variance).
* The residuals must be normally distributed.

  :::{.callout-important}
  This assumption of normality applies to the *residuals* and *not* to the predictors!
  :::

* The residuals must be independent of each other (this can be checked, for example, with the Durbin-Watson test).
* The relationship between independent variables and the dependent variable must be linear.


## Example (multiple linear regression)

In the following example, we will again look at the number of music album sales in relation to the advertising budget. However, we will now include two additional predictors: the number of airplay hours on the largest national radio stations and the attractiveness of the band members. We start by loading the data [`sales2.dat`](sales2.dat):

```{r}
#| message: false
library(readr)
album2 = read_tsv("sales2.dat")
```

Let's calculate a linear regression model. As a comparison model, we first perform a simple regression with only the advertising budget as a predictor:

```{r}
model1 = lm(sales ~ adverts, data=album2)
```

We can now add additional factors to a second model using the `+` operator:

```{r}
model2 = lm(sales ~ adverts + airplay + attract, data=album2)
```

We can the summarize the results of both models:

```{r}
summary(model1)
summary(model2)
```

Since the first model is identical to the one from the previous example, we already know the results. Let's therefore discuss the second model. $R^2$ is 0.6647, meaning the model can now explain 66% of the variance. Compared to the first model with only one predictor, this is an increase of 33%, i.e., the two predictors airplay and attractiveness explain an additional 33% of the variance.

The output of the `summary()` function also shows the regression coefficients. We can write this multiple linear regression model as follows:

$$y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 = -26.613 + 0.085 \cdot x_1 + 3.367 \cdot x_2 + 11.086 \cdot x_3$$

Here, $b_0$ is the intercept (i.e., the value of $y$ when all predictors are 0), $b_1$ is the advertising budget `adverts`, $b_2$ is the airplay hours `airplay`, and $b_3$ is the attractiveness `attract`.

The regression coefficients reflect how much the dependent variable changes when a predictor is increased by one unit while keeping all other predictors constant. In our example, this means:

* If we increase `adverts` by one unit, `sales` increases by 0.085 units. This means that for every €1,000 spent on advertising, we sell 85 more albums.
* If we increase `airplay` by one unit, `sales` increases by 3.367 units. This means that for every additional hour of airplay, we sell 3,367 more albums.
* If we increase `attract` by one unit, `sales` increases by 11.086 units. This means that for every additional attractiveness point, we sell 11,086 more albums.

The $t$-values and $p$-values indicate whether the regression coefficients are significantly different from zero. The size of the $t$-value indicates the influence (importance) of the coefficients, i.e., `adverts` and `airplay` have a similar influence on the model, while `attract` has a smaller influence.

Often, it is helpful to inspect the standardized regression coefficients, because otherwise the coefficients depend on the scale of the predictors. These can be calculated by first standardizing all variables and then calculating the linear model (standardized variables have a mean of 0 and a standard deviation of 1).

:::{.callout-tip}
The `scale()` function can be used to standardize the columns of a data frame. However, this function always returns a matrix, so if you want to standardize a data frame or tibble, you need to apply `as.data.frame()` or `tibble::as_tibble()` afterwards.
:::

Alternatively, you can also use the `lm.beta()` function from the `lm.beta` package after the regular (unstandardized) model has been calculated:

```{r}
library(lm.beta)
lm.beta(model2)
```

The standardized regression coefficients are usually denoted by $\beta_i$. Since all variables are now measured in standard deviations, they can be directly compared. In the example, we can make the following statements:

* If we increase `adverts` by one standard deviation (€485,655), `sales` increases by 0.511 standard deviations (41,240 albums).
* If we increase `airplay` by one standard deviation (12.27 hours), `sales` increases by 0.512 standard deviations (41,320 albums).
* If we increase `attract` by one standard deviation (1.40 points), `sales` increases by 0.192 standard deviations (15,490 albums).

Just like we already saw with the $t$-values, the standardized regression coefficients show that `adverts` and `airplay` have a similar influence on the model, while `attract` has a smaller influence.

We can calculate confidence intervals for the (non-standardized) regression coefficients with the `confint()` function (by default, the function computes 95% confidence intervals):

```{r}
confint(model2)
```

As we have already seen, we can compare two (or more) models using the $F$ statistic. The $F$ value displayed when summarizing a model compares the model to the simplest baseline model. If you want to compare the model to a different model, `model2` must be an extension of `model1`, i.e., `model2` must include all terms of `model1` plus any additional factors. In R, you can compare two models using the following command:

```{r}
anova(model1, model2)
```

The $F$ value is 96.447, and because the $p$-value is less than 0.001, this indicates that the second model is significantly better than the first.


### Influential data points

We can use the following functions to calculate various outlier statistics for each individual data point:

* `resid()`: Residuals
* `rstandard()`: Standardized residuals
* `rstudent()`: Studentized residuals (using leave-one-out)
* `hatvalues()`: Leverage
* `dfbeta()`: Difference between regression coefficients using leave-one-out
* `cooks.distance()`: Cook's distance
* `dffits()`: Difference between predicted values using leave-one-out

::: {.callout-note}
Leave-one-out means that the respective data point is excluded from the calculation of the statistic. We can then compare the statistic for each data point for a model with and without the respective data point, which allows us to determine the influence of each data point on the model.
:::

A particularly useful function is `influence.measures()`, which provides an overview of several outlier statistics for each data point:

```r
influence.measures(model2)
```


### Model diagnostics

We can use the VIF statistic to check for multicollinearity using the `vif()` function from the `car` package:

```{r}
#| message: false
library(car)
vif(model2)
```

The largest VIF value should not exceed 10. The average VIF value should not be significantly greater than 1, which can be checked as follows:

```{r}
mean(vif(model2))
```

If we plot a regression model using the `plot()` function, we get four diagnostic plots:

```{r}
#| fig-width: 12
#| fig-height: 8
par(mfrow=c(2, 2), cex=0.75)
plot(model2)
```

The plot in the upper left shows the predicted values versus the residuals. Here, we can check the linearity assumption (the red line should be a straight line around zero) and the homoscedasticity assumption (the spread of the data points should not change along the x-axis). The plot in the lower left is similar, but instead of the (absolute) residuals, the square root of the standardized residuals is shown. This plot allows us to assess whether the assumption of homogeneity of variance is met. The plot in the upper right shows a QQ plot to check the normality assumption of the residuals. If this assumption is met, data point should lie on the straight diagonal line. The plot in the lower right shows influential points (measured by leverage); Cook's distance is also visible in the plot.

We can also check the residuals for independence with the Durbin-Watson test using the `dwt()` function from the `car` package:

```{r}
dwt(model2)
```

In this example, we can assume that the residuals are independent, as the $p$-value is approximately 0.7, which means that the null hypothesis (residuals are independent) cannot be rejected.


## Exercises

### Exercise 1

The file [`cars.csv`](cars.csv) contains several measurements of braking distances (`dist`) of cars braking at a certain speed (`speed`). We would like to investigate whether there is a (linear) relationship between speed and braking distance. First, plot both variables in a scatter plot (`speed` on the *x*-axis and `dist` on the *y*-axis). What do you observe? Is there a linear relationship between the two variables?


### Exercise 2

Perform a linear regression analysis and summarize the results!


### Exercise 3

Compute the Pearson correlation between the two variables (without using $R^2$ from the model) and check whether this value matches $R^2$ from the regression model.


### Exercise 4

Write down the model equation. Which braking distances does the model predict for a car driving 5 mph and 65 mph, respectively? Use both the model equation and the `predict()` function to estimate the braking distances.


### Exercise 5

Import the data from the file `sales2.dat` as shown in the course materials. Standardize all variables and then calculate a linear regression model. Compare the regression coefficients with the results of the `lm.beta()` function, which can be applied to a model with non-standardized data.


### Exercise 6

Import the dataset [`aggression.dat`](aggression.dat), which contains data on aggression among 666 children. It contains the following variables:

- Parenting style (high value corresponds to bad style)
- Playing computer games (high value corresponds to playing a lot of computer games)
- Watching TV (high value corresponds to watching a lot of TV)
- Nutrition (high value corresponds to healthy nutrition)
- Aggression of siblings (high value corresponds to high aggression)

From previous studies, we know that parenting style and aggression of siblings are significant predictors of a child's aggression.

Fit two linear regression models. The first should only include the two factors that have previously been shown to influence aggression. The second model should include all factors. Then answer the following questions:

1. Calculate the coefficient of determination $R^2$ for both models and provide the table of regression coefficients.
2. Interpret the individual coefficients in terms of relevance (standardized coefficients are helpful) and significance for both models separately.
3. Compare both models. Is the second model a significant improvement over the first?


### Exercise 7

Check the following assumptions for the second model from the previous exercise:

- Are the predictors linearly dependent (VIF)?
- Are the residuals independent (Durbin-Watson test)?
- Are the residuals normally distributed (QQ plot)?
- Are the dependencies linear and is the variance homogeneous (plot residuals vs. predicted values)?
- Are there any data points with a large influence on the model (plot residuals vs. leverage)?

::: {.callout-tip}
Read the documentation of the `plot.lm()` function, which creates diagnostic plots for linear models. The `which` argument allows you to select the desired diagnostic plot.
:::
